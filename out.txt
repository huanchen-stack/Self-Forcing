PyTorch intra-op threads = 1
PyTorch inter-op threads = 6
Free VRAM 38.965087890625 GB
‚ö†Ô∏è Low VRAM mode activated - models will be swapped dynamically to save memory
‚úÖ VAE decoder initialized with default VAE
is causal...
CausalWanModel(
  (patch_embedding): Conv3d(16, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))
  (text_embedding): Sequential(
    (0): Linear(in_features=4096, out_features=1536, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (time_embedding): Sequential(
    (0): Linear(in_features=256, out_features=1536, bias=True)
    (1): SiLU()
    (2): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (time_projection): Sequential(
    (0): SiLU()
    (1): Linear(in_features=1536, out_features=9216, bias=True)
  )
  (blocks): ModuleList(
    (0-29): 30 x CausalWanAttentionBlock(
      (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (self_attn): CausalWanSelfAttention(
        (q): Linear(in_features=1536, out_features=1536, bias=True)
        (k): Linear(in_features=1536, out_features=1536, bias=True)
        (v): Linear(in_features=1536, out_features=1536, bias=True)
        (o): Linear(in_features=1536, out_features=1536, bias=True)
        (norm_q): WanRMSNorm()
        (norm_k): WanRMSNorm()
      )
      (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)
      (cross_attn): WanT2VCrossAttention(
        (q): Linear(in_features=1536, out_features=1536, bias=True)
        (k): Linear(in_features=1536, out_features=1536, bias=True)
        (v): Linear(in_features=1536, out_features=1536, bias=True)
        (o): Linear(in_features=1536, out_features=1536, bias=True)
        (norm_q): WanRMSNorm()
        (norm_k): WanRMSNorm()
      )
      (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (ffn): Sequential(
        (0): Linear(in_features=1536, out_features=8960, bias=True)
        (1): GELU(approximate='tanh')
        (2): Linear(in_features=8960, out_features=1536, bias=True)
      )
    )
  )
  (head): CausalHead(
    (norm): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    (head): Linear(in_features=1536, out_features=64, bias=True)
  )
)
KV inference with 3 frames per block
üöÄ Starting demo on http://0.0.0.0:5001
 * Serving Flask app 'demo'
 * Debug mode: off
Client connected
Client disconnected
Client connected
üì° Frame sender thread started
Prompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.
Conditional dict keys: ['prompt_embeds']
	 prompt_embeds shape: torch.Size([1, 512, 4096]), dtype: torch.bfloat16
Moving DynamicSwap_WanTextEncoder to cuda:0 with preserved memory: 41.077611446380615 GB
Seed: 3761933378
rnd: <torch._C.Generator object at 0x73a3446b0c90> (based on gpu cuda:0)
 - Using local attention size x frame seq length = 21 x 1560 to set KV cache size to 32760
Frames status: total 42, per block [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
In loop: idx 0, current_num_frames 3, current_start_frame 0
üîÑ Processing block 1/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.159743994474411 ms
---------------- attention block ----------------
		qkv norm: 2.332672119140625 ms
		rope: 2.070528030395508 ms
		kv cache update (if evict): 0.21606400609016418 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 26.845184326171875 ms
		out projection: 0.35839998722076416 ms
	self-attn: 41.72083282470703 ms
	cross-attn: 2.0531198978424072 ms
	ffn: 3.631103992462158 ms
forward previous block to here: 47.845375061035156 ms
---------------- attention block ----------------
		qkv norm: 1.0004479885101318 ms
		rope: 1.1519999504089355 ms
		kv cache update (if evict): 0.2027519941329956 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.644544005393982 ms
		out projection: 0.2979840040206909 ms
	self-attn: 5.271552085876465 ms
	cross-attn: 1.7418240308761597 ms
	ffn: 1.8708479404449463 ms
forward previous block to here: 9.294848442077637 ms
---------------- attention block ----------------
		qkv norm: 0.9983999729156494 ms
		rope: 1.0977280139923096 ms
		kv cache update (if evict): 0.20377600193023682 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.5595519542694092 ms
		out projection: 0.3041279911994934 ms
	self-attn: 5.1609601974487305 ms
	cross-attn: 1.4192639589309692 ms
	ffn: 1.738752007484436 ms
forward previous block to here: 8.685567855834961 ms
---------------- attention block ----------------
		qkv norm: 0.9891840219497681 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.481727957725525 ms
		out projection: 0.2877439856529236 ms
	self-attn: 4.765696048736572 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.7182719707489014 ms
forward previous block to here: 8.192000389099121 ms
---------------- attention block ----------------
		qkv norm: 0.9943040013313293 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4704639911651611 ms
		out projection: 0.27033600211143494 ms
	self-attn: 4.769792079925537 ms
	cross-attn: 1.3649920225143433 ms
	ffn: 1.716223955154419 ms
forward previous block to here: 8.174592018127441 ms
---------------- attention block ----------------
		qkv norm: 0.9881600141525269 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4489599466323853 ms
		out projection: 0.27033600211143494 ms
	self-attn: 4.758528232574463 ms
	cross-attn: 1.368064045906067 ms
	ffn: 1.7203199863433838 ms
forward previous block to here: 8.228863716125488 ms
---------------- attention block ----------------
		qkv norm: 0.9994239807128906 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4479360580444336 ms
		out projection: 0.2693119943141937 ms
	self-attn: 4.7472639083862305 ms
	cross-attn: 1.3608959913253784 ms
	ffn: 1.7326079607009888 ms
forward previous block to here: 8.20736026763916 ms
---------------- attention block ----------------
		qkv norm: 0.9902080297470093 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.462272047996521 ms
		out projection: 0.27136000990867615 ms
	self-attn: 4.7124481201171875 ms
	cross-attn: 1.3701119422912598 ms
	ffn: 1.7172479629516602 ms
forward previous block to here: 8.139776229858398 ms
---------------- attention block ----------------
		qkv norm: 0.9891840219497681 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4612480401992798 ms
		out projection: 0.2672640085220337 ms
	self-attn: 4.7267842292785645 ms
	cross-attn: 1.3660160303115845 ms
	ffn: 1.7182719707489014 ms
forward previous block to here: 8.147968292236328 ms
---------------- attention block ----------------
		qkv norm: 0.9922559857368469 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4735360145568848 ms
		out projection: 0.2662400007247925 ms
	self-attn: 4.727807998657227 ms
	cross-attn: 1.351680040359497 ms
	ffn: 1.714176058769226 ms
forward previous block to here: 8.122367858886719 ms
---------------- attention block ----------------
		qkv norm: 0.9902080297470093 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4684159755706787 ms
		out projection: 0.2693119943141937 ms
	self-attn: 4.725759983062744 ms
	cross-attn: 1.369088053703308 ms
	ffn: 1.785856008529663 ms
forward previous block to here: 8.248319625854492 ms
---------------- attention block ----------------
		qkv norm: 0.9891840219497681 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.457152009010315 ms
		out projection: 0.2693119943141937 ms
	self-attn: 4.7267842292785645 ms
	cross-attn: 1.3926399946212769 ms
	ffn: 1.7182719707489014 ms
forward previous block to here: 8.178688049316406 ms
---------------- attention block ----------------
		qkv norm: 0.9861119985580444 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4632960557937622 ms
		out projection: 0.2672640085220337 ms
	self-attn: 4.725759983062744 ms
	cross-attn: 1.3567999601364136 ms
	ffn: 1.743872046470642 ms
forward previous block to here: 8.163328170776367 ms
---------------- attention block ----------------
		qkv norm: 0.9871360063552856 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4581760168075562 ms
		out projection: 0.2662400007247925 ms
	self-attn: 4.675583839416504 ms
	cross-attn: 1.3578239679336548 ms
	ffn: 1.714176058769226 ms
forward previous block to here: 8.093695640563965 ms
---------------- attention block ----------------
		qkv norm: 0.9871360063552856 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4428160190582275 ms
		out projection: 0.26521599292755127 ms
	self-attn: 4.711423873901367 ms
	cross-attn: 1.3557759523391724 ms
	ffn: 1.7172479629516602 ms
forward previous block to here: 8.10700798034668 ms
---------------- attention block ----------------
		qkv norm: 0.9943040013313293 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4489599466323853 ms
		out projection: 0.2764799892902374 ms
	self-attn: 4.716544151306152 ms
	cross-attn: 1.3844480514526367 ms
	ffn: 1.7305599451065063 ms
forward previous block to here: 8.165375709533691 ms
---------------- attention block ----------------
		qkv norm: 0.9881600141525269 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4673919677734375 ms
		out projection: 0.2672640085220337 ms
	self-attn: 4.745215892791748 ms
	cross-attn: 1.3875199556350708 ms
	ffn: 1.7151999473571777 ms
forward previous block to here: 8.173567771911621 ms
---------------- attention block ----------------
		qkv norm: 0.9881600141525269 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4612480401992798 ms
		out projection: 0.26419198513031006 ms
	self-attn: 4.736000061035156 ms
	cross-attn: 1.4192639589309692 ms
	ffn: 1.7151999473571777 ms
forward previous block to here: 8.196096420288086 ms
---------------- attention block ----------------
		qkv norm: 0.9871360063552856 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4520319700241089 ms
		out projection: 0.2662400007247925 ms
	self-attn: 4.677631855010986 ms
	cross-attn: 1.3864959478378296 ms
	ffn: 1.7151999473571777 ms
forward previous block to here: 8.120320320129395 ms
---------------- attention block ----------------
		qkv norm: 0.9871360063552856 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4540799856185913 ms
		out projection: 0.2662400007247925 ms
	self-attn: 4.7226881980896 ms
	cross-attn: 1.3752319812774658 ms
	ffn: 1.7131520509719849 ms
forward previous block to here: 8.138751983642578 ms
---------------- attention block ----------------
		qkv norm: 1.0035200119018555 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.4899200201034546 ms
		out projection: 0.2764799892902374 ms
	self-attn: 4.808703899383545 ms
	cross-attn: 1.4305280447006226 ms
	ffn: 1.7377279996871948 ms
forward previous block to here: 8.338432312011719 ms
---------------- attention block ----------------
		qkv norm: 0.9943040013313293 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3137919902801514 ms
		out projection: 0.2662400007247925 ms
	self-attn: 4.626431941986084 ms
	cross-attn: 1.427456021308899 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 7.841792106628418 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.082368016242981 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3424639701843262 ms
		out projection: 0.2467840015888214 ms
	self-attn: 4.528128147125244 ms
	cross-attn: 1.405951976776123 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 7.716864109039307 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2584960460662842 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.341760158538818 ms
	cross-attn: 1.4069759845733643 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 7.53766393661499 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2892160415649414 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.365312099456787 ms
	cross-attn: 1.4335999488830566 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 7.627776145935059 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0792959928512573 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2953599691390991 ms
		out projection: 0.24371199309825897 ms
	self-attn: 4.4410881996154785 ms
	cross-attn: 1.4315520524978638 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 7.674880027770996 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2564480304718018 ms
		out projection: 0.2549760043621063 ms
	self-attn: 4.361216068267822 ms
	cross-attn: 1.3824000358581543 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 7.527423858642578 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2451839447021484 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.312064170837402 ms
	cross-attn: 1.3373440504074097 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 7.417856216430664 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3035520315170288 ms
		out projection: 0.2385919988155365 ms
	self-attn: 4.336639881134033 ms
	cross-attn: 1.3373440504074097 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 7.443456172943115 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2421120405197144 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.279295921325684 ms
	cross-attn: 1.3322240114212036 ms
	ffn: 1.4346239566802979 ms
forward last block: 7.420928001403809 ms
##### model time: 778.8697509765625 #####
	 Transformer denoising step 1/4 completed in 811.07 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10956799983978271 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.393664002418518 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.60697603225708 ms
	cross-attn: 1.1509759426116943 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 7.540736198425293 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.257472038269043 ms
		out projection: 0.24780799448490143 ms
	self-attn: 4.346879959106445 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 7.192575931549072 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2666879892349243 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.356095790863037 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.160831928253174 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.235967993736267 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.265984058380127 ms
	cross-attn: 1.1192320585250854 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 7.161856174468994 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2492799758911133 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.256768226623535 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 7.101439952850342 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.274880051612854 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.357120037078857 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 7.196671962738037 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2349439859390259 ms
		out projection: 0.24473600089550018 ms
	self-attn: 4.261888027191162 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.081984043121338 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.229119777679443 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.03385591506958 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.222976207733154 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.0502400398254395 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.227776050567627 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.192255973815918 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 6.977536201477051 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.193280220031738 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 6.990848064422607 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.222656011581421 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.20147180557251 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.972415924072266 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.215807914733887 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 7.0348801612854 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.175871849060059 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 6.982656002044678 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2349439859390259 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.205567836761475 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 7.016448020935059 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.221951961517334 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.004159927368164 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.230144023895264 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 7.061503887176514 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2031999826431274 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.208640098571777 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.99289608001709 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2390400171279907 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.235263824462891 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.004159927368164 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.219903945922852 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.972415924072266 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.1991039514541626 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.176896095275879 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 6.998015880584717 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.178944110870361 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 6.981632232666016 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.225024223327637 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.010303974151611 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.24268800020217896 ms
	self-attn: 4.203519821166992 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.969344139099121 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.228096008300781 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.03385591506958 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.222656011581421 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.253695964813232 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.026688098907471 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.1943039894104 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.9713921546936035 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.187136173248291 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.953983783721924 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2288000583648682 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.237311840057373 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 6.996992111206055 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.179967880249023 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.415168046951294 ms
forward last block: 6.987775802612305 ms
##### model time: 215.50694274902344 #####
	 Transformer denoising step 2/4 completed in 216.05 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2666879892349243 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.267007827758789 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.017471790313721 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.195328235626221 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 6.944767951965332 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.168704032897949 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 7.026688098907471 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.206592082977295 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 6.985727787017822 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.196352005004883 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4039039611816406 ms
forward previous block to here: 6.9416961669921875 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.206272006034851 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.176896095275879 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.405951976776123 ms
forward previous block to here: 6.923264026641846 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2031999826431274 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.1902079582214355 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 6.967296123504639 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.184063911437988 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 6.953983783721924 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.198400020599365 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 6.975488185882568 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.220928192138672 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4079999923706055 ms
forward previous block to here: 6.966271877288818 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.189184188842773 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.9918718338012695 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.2342400550842285 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 7.020544052124023 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.222656011581421 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.261888027191162 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.038976192474365 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2687360048294067 ms
		out projection: 0.26214399933815 ms
	self-attn: 4.375552177429199 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.129087924957275 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2431360483169556 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.223999977111816 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 6.989823818206787 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.16153621673584 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.949888229370117 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.164608001708984 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.936575889587402 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.157440185546875 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4079999923706055 ms
forward previous block to here: 6.900735855102539 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.200448036193848 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.964223861694336 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.158463954925537 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.940671920776367 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.164608001708984 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4079999923706055 ms
forward previous block to here: 6.9324798583984375 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.2385919988155365 ms
	self-attn: 4.176896095275879 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.928383827209473 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2216320037841797 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.238336086273193 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.020544052124023 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.1748480796813965 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.949888229370117 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.227776050567627 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.184063911437988 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.944767951965332 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.178944110870361 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 6.9468159675598145 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.189184188842773 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 6.947840213775635 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.222656011581421 ms
		out projection: 0.22835199534893036 ms
	self-attn: 4.162559986114502 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.947840213775635 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.169727802276611 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4069759845733643 ms
forward previous block to here: 6.961152076721191 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2267520427703857 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.188159942626953 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.410048007965088 ms
forward last block: 6.9468159675598145 ms
##### model time: 212.99813842773438 #####
	 Transformer denoising step 3/4 completed in 213.49 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.222656011581421 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.184063911437988 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 6.98470401763916 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.186111927032471 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.957056045532227 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.1656317710876465 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 6.910975933074951 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2539519965648651 ms
	self-attn: 4.205567836761475 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.999040126800537 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.163584232330322 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 6.967296123504639 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2257280349731445 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.1656317710876465 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.9273600578308105 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.159488201141357 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 6.939648151397705 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2349439859390259 ms
		out projection: 0.2539519965648651 ms
	self-attn: 4.203519821166992 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.978559970855713 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.156415939331055 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 6.951935768127441 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2216320037841797 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.182015895843506 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.9713921546936035 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.162559986114502 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 6.917119979858398 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.2549760043621063 ms
	self-attn: 4.1943039894104 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.953983783721924 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2288000583648682 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.173823833465576 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.967296123504639 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.172800064086914 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 6.949888229370117 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.16153621673584 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.9120001792907715 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.227776050567627 ms
		out projection: 0.24985599517822266 ms
	self-attn: 4.217855930328369 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 6.993919849395752 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.155392169952393 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 6.952960014343262 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.1748480796813965 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.938623905181885 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2380160093307495 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.199423789978027 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 6.981632232666016 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.25702399015426636 ms
	self-attn: 4.2188801765441895 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.9918718338012695 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.210368037223816 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.1748480796813965 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.007232189178467 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.22835199534893036 ms
	self-attn: 4.211711883544922 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 6.975488185882568 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2380160093307495 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.2096638679504395 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.975488185882568 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.187136173248291 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 6.944767951965332 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.206272006034851 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.199423789978027 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.98367977142334 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.22835199534893036 ms
	self-attn: 4.187136173248291 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.939648151397705 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.264639973640442 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.2393598556518555 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.0359039306640625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.20249605178833 ms
	cross-attn: 1.1038719415664673 ms
	ffn: 1.5380480289459229 ms
forward previous block to here: 7.267327785491943 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3189120292663574 ms
		out projection: 0.27750399708747864 ms
	self-attn: 4.632575988769531 ms
	cross-attn: 1.2328959703445435 ms
	ffn: 1.4981119632720947 ms
forward previous block to here: 7.724031925201416 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3312000036239624 ms
		out projection: 0.2549760043621063 ms
	self-attn: 4.60595178604126 ms
	cross-attn: 1.2339199781417847 ms
	ffn: 1.4632960557937622 ms
forward last block: 7.672832012176514 ms
##### model time: 214.4286651611328 #####
	 Transformer final denoising step 4/4 completed in 215.04 ms
‚ö° Block 1 denoising completed in 1458.82 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11161600053310394 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3148159980773926 ms
		out projection: 0.25600001215934753 ms
	self-attn: 4.588543891906738 ms
	cross-attn: 1.2533760070800781 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 7.693312168121338 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0833920240402222 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3127679824829102 ms
		out projection: 0.26009601354599 ms
	self-attn: 4.634624004364014 ms
	cross-attn: 1.2799999713897705 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 7.7711358070373535 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2974079847335815 ms
		out projection: 0.26521599292755127 ms
	self-attn: 4.552703857421875 ms
	cross-attn: 1.2533760070800781 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 7.624703884124756 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.299456000328064 ms
		out projection: 0.2549760043621063 ms
	self-attn: 4.513792037963867 ms
	cross-attn: 1.2318719625473022 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 7.593984127044678 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.316864013671875 ms
		out projection: 0.2519040107727051 ms
	self-attn: 4.60697603225708 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.465983867645264 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2451839447021484 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.227071762084961 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.041024208068848 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.2393598556518555 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.028736114501953 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.219903945922852 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.020544052124023 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.246528148651123 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.051263809204102 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2288000583648682 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.245503902435303 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.056384086608887 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.206592082977295 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.970367908477783 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.197375774383545 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.000063896179199 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.20070399343967438 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2462079524993896 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.376575946807861 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.178239822387695 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2349439859390259 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.207615852355957 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 6.978559970855713 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2267520427703857 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.193280220031738 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.967296123504639 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2328959703445435 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.25267219543457 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.024640083312988 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.189184188842773 ms
	cross-attn: 1.1038719415664673 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 7.049215793609619 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.207615852355957 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.027711868286133 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2441600561141968 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.257791996002197 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.057407855987549 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.217855930328369 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.007232189178467 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.217855930328369 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.019519805908203 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.241407871246338 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 7.045119762420654 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.227071762084961 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.062528133392334 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2267520427703857 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.30182409286499 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.126016139984131 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.227776050567627 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.275199890136719 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.075839996337891 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.227776050567627 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.220928192138672 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.04307222366333 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.191232204437256 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.010303974151611 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2421120405197144 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.214784145355225 ms
	cross-attn: 1.1653120517730713 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.145472049713135 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2298239469528198 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.267007827758789 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.070720195770264 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2257280349731445 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.257791996002197 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4264320135116577 ms
forward last block: 7.0830078125 ms
##### model time: 218.2256622314453 #####
KV cache update for next block completed in 218.95 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 1 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 9, 3, 480, 832])
üé® Block 1 VAE decoding completed in 660.80 ms
pixels shape after possible cropping: torch.Size([1, 9, 3, 480, 832]), block_frames: 9
üì° Queueing 9 frames from block 1 for sending...
‚úÖ Block 1 completed in 2387.58 ms (9 frames queued in 0.049 ms)
In loop: idx 1, current_num_frames 3, current_start_frame 3
üîÑ Processing block 2/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11059200018644333 ms
---------------- attention block ----------------
		qkv norm: 9.21292781829834 ms
		rope: 5.498879909515381 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.1678080558776855 ms
		out projection: 0.26521599292755127 ms
	self-attn: 19.205120086669922 ms
	cross-attn: 1.240064024925232 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 22.2607364654541 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.2170879989862442 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.045952081680298 ms
		out projection: 0.25088000297546387 ms
	self-attn: 5.97708797454834 ms
	cross-attn: 9.71776008605957 ms
	ffn: 5.833727836608887 ms
forward previous block to here: 21.891071319580078 ms
---------------- attention block ----------------
		qkv norm: 2.934783935546875 ms
		rope: 1.1008000373840332 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0971519947052 ms
		out projection: 0.26214399933815 ms
	self-attn: 13.315072059631348 ms
	cross-attn: 6.025216102600098 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 22.969343185424805 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0751999616622925 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0500481128692627 ms
		out projection: 0.2744320034980774 ms
	self-attn: 5.2254719734191895 ms
	cross-attn: 12.508159637451172 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 19.517440795898438 ms
---------------- attention block ----------------
		qkv norm: 0.9902080297470093 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.21196800470352173 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.10534405708313 ms
		out projection: 0.3184640109539032 ms
	self-attn: 15.022080421447754 ms
	cross-attn: 5.174272060394287 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 23.65132713317871 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 13.917183876037598 ms
		out projection: 0.32256001234054565 ms
	self-attn: 17.323007583618164 ms
	cross-attn: 3.6526079177856445 ms
	ffn: 1.6076799631118774 ms
forward previous block to here: 23.005184173583984 ms
---------------- attention block ----------------
		qkv norm: 0.881663978099823 ms
		rope: 5.509119987487793 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.1237759590148926 ms
		out projection: 0.2744320034980774 ms
	self-attn: 19.62393569946289 ms
	cross-attn: 1.2584960460662842 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 29.97964859008789 ms
---------------- attention block ----------------
		qkv norm: 4.6530561447143555 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 4.356095790863037 ms
		out projection: 0.2734079957008362 ms
	self-attn: 11.480064392089844 ms
	cross-attn: 1.1612160205841064 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 14.429183959960938 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.2805759906768799 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.228224039077759 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.577727794647217 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 8.451071739196777 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0244479179382324 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.114880084991455 ms
	cross-attn: 1.1212799549102783 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 8.0066556930542 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.01964807510376 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.870463848114014 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.01043176651001 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 7.8438401222229 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.007040023803711 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.017600059509277 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.846911907196045 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.990975856781006 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.803904056549072 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.968448162078857 ms
	cross-attn: 1.4602240324020386 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.182784080505371 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.039103984832764 ms
	cross-attn: 1.1110399961471558 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.896063804626465 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.0135040283203125 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 7.866367816925049 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.105663776397705 ms
	cross-attn: 1.3967360258102417 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.260607719421387 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.992000102996826 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.786496162414551 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.989952087402344 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.814144134521484 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0049920082092285 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.980735778808594 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.791615962982178 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.963327884674072 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.781375885009766 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.007040023803711 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.980735778808594 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.774208068847656 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0008959770202637 ms
		out projection: 0.24985599517822266 ms
	self-attn: 4.996096134185791 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 7.792640209197998 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.004288196563721 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.770112037658691 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0101120471954346 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.992000102996826 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.7649922370910645 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.012160062789917 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.012479782104492 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.816192150115967 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.96127986907959 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.723008155822754 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.971519947052002 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.770112037658691 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.982783794403076 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4131200313568115 ms
forward last block: 7.734272003173828 ms
##### model time: 355.1088562011719 #####
	 Transformer denoising step 1/4 completed in 355.61 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.984831809997559 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.753727912902832 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.0083842277526855 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.7854719161987305 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.974592208862305 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.766016006469727 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.787519931793213 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.067456007003784 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.119999885559082 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.95136022567749 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0008959770202637 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.0032639503479 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.835648059844971 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.007040023803711 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.9991679191589355 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.816192150115967 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.014527797698975 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.812096118927002 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.9694719314575195 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.767039775848389 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.975615978240967 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.774208068847656 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.9838080406188965 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.787519931793213 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0039680004119873 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.988927841186523 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.790592193603516 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.995071887969971 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.778304100036621 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.972544193267822 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 7.763967990875244 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.013184070587158 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.981760025024414 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.754752159118652 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.02239990234375 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.001215934753418 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.773183822631836 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.761919975280762 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.941823959350586 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.708672046661377 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.957183837890625 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.782400131225586 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.774208068847656 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.990975856781006 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.754752159118652 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.0032639503479 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 7.759871959686279 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.009407997131348 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.79366397857666 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.030911922454834 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.791615962982178 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0039680004119873 ms
		out projection: 0.24166400730609894 ms
	self-attn: 5.082111835479736 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.880703926086426 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0090880393981934 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.033984184265137 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.828479766845703 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.012160062789917 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.080063819885254 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.885824203491211 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.24473600089550018 ms
	self-attn: 5.083136081695557 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.887872219085693 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.2529279887676239 ms
	self-attn: 5.004288196563721 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 7.791615962982178 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.415168046951294 ms
forward last block: 7.728127956390381 ms
##### model time: 237.41542053222656 #####
	 Transformer denoising step 2/4 completed in 237.91 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0142080783843994 ms
		out projection: 0.24166400730609894 ms
	self-attn: 5.038080215454102 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.8837761878967285 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0049920082092285 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.975615978240967 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.756800174713135 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9886080026626587 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.948991775512695 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 7.7414398193359375 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.001215934753418 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 7.840767860412598 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.008064031600952 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.038080215454102 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 7.848959922790527 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.012160062789917 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.987904071807861 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.756800174713135 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0285439491271973 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.002240180969238 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.789567947387695 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0008959770202637 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.946944236755371 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.708672046661377 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9886080026626587 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.948991775512695 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.738368034362793 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.979712009429932 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.759871959686279 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 7.737343788146973 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.007040023803711 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.976640224456787 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.742464065551758 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.96230411529541 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 7.747583866119385 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.979712009429932 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.7363200187683105 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.967423915863037 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.727104187011719 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.24883200228214264 ms
	self-attn: 4.972544193267822 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.733248233795166 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.952064037322998 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.709695816040039 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.936704158782959 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.728127956390381 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.985856056213379 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.782400131225586 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.995071887969971 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.79366397857666 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.023744106292725 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.856128215789795 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.956160068511963 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.7414398193359375 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.936704158782959 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.692287921905518 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.944896221160889 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 7.7414398193359375 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.009407997131348 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 7.8008317947387695 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0090880393981934 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.972544193267822 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.724031925201416 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0254719257354736 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.9694719314575195 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.7608962059021 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.938752174377441 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.699456214904785 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.960256099700928 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.732223987579346 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.9838080406188965 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.423359990119934 ms
forward last block: 7.759871959686279 ms
##### model time: 236.4303436279297 #####
	 Transformer denoising step 3/4 completed in 236.91 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.0942080020904541 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.994048118591309 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.772160053253174 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0142080783843994 ms
		out projection: 0.2539519965648651 ms
	self-attn: 5.002240180969238 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 7.782400131225586 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.979712009429932 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.754752159118652 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9927040338516235 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.955135822296143 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.733248233795166 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.990975856781006 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.811071872711182 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.030911922454834 ms
	cross-attn: 1.1018240451812744 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 7.884799957275391 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.001215934753418 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 7.819263935089111 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.97049617767334 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.786496162414551 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.972544193267822 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.788544178009033 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.008064031600952 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.979712009429932 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.777279853820801 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0541439056396484 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.025792121887207 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.7946882247924805 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.24371199309825897 ms
	self-attn: 4.984831809997559 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 7.792640209197998 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.984831809997559 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.756800174713135 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0183041095733643 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.947968006134033 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.698431968688965 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0244479179382324 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.012479782104492 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.778304100036621 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.95308780670166 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.708672046661377 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.952064037322998 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.770112037658691 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0480000972747803 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.053440093994141 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.848959922790527 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.978687763214111 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.769087791442871 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.008064031600952 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.984831809997559 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.754752159118652 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.960256099700928 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.719935894012451 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.989952087402344 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.753727912902832 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.981760025024414 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.740416049957275 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9886080026626587 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.973567962646484 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.721983909606934 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.952064037322998 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.7209601402282715 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.952064037322998 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.729152202606201 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9886080026626587 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.960256099700928 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.770112037658691 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.988927841186523 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.789567947387695 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.945919990539551 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.7414398193359375 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9988479614257812 ms
		out projection: 0.2385919988155365 ms
	self-attn: 4.944896221160889 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4141440391540527 ms
forward last block: 7.731200218200684 ms
##### model time: 236.60646057128906 #####
	 Transformer final denoising step 4/4 completed in 237.11 ms
‚ö° Block 2 denoising completed in 1069.42 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09625600278377533 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9927040338516235 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.97049617767334 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.75167989730835 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.24371199309825897 ms
	self-attn: 5.037055969238281 ms
	cross-attn: 1.3649920225143433 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 8.41113567352295 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0741759538650513 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.1135358810424805 ms
		out projection: 0.2590720057487488 ms
	self-attn: 5.41593599319458 ms
	cross-attn: 1.2554240226745605 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 8.527872085571289 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.100224018096924 ms
		out projection: 0.24268800020217896 ms
	self-attn: 5.3831682205200195 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 8.20633602142334 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0049920082092285 ms
		out projection: 0.2385919988155365 ms
	self-attn: 4.973567962646484 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.768064022064209 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.982783794403076 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 7.769087791442871 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9988479614257812 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.985856056213379 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.7649922370910645 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0183041095733643 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.992000102996826 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.772160053253174 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.012160062789917 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.714816093444824 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.96127986907959 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.757823944091797 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.95308780670166 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.7209601402282715 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.959231853485107 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.759871959686279 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.974592208862305 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.74348783493042 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.06982421875 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.825407981872559 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.968448162078857 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.746560096740723 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.972544193267822 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.725056171417236 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.940800189971924 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.690239906311035 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.2539519965648651 ms
	self-attn: 4.990975856781006 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.756800174713135 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.22835199534893036 ms
	self-attn: 4.955135822296143 ms
	cross-attn: 1.1407359838485718 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.821311950683594 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9988479614257812 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.963327884674072 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.7506561279296875 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9988479614257812 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.944896221160889 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.740416049957275 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.754752159118652 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.97049617767334 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.773183822631836 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.978687763214111 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.778304100036621 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0049920082092285 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.951039791107178 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.705599784851074 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.966400146484375 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 7.746560096740723 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.960256099700928 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.713791847229004 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0183041095733643 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.987904071807861 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.753727912902832 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0152320861816406 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.96127986907959 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.714816093444824 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.947968006134033 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4172159433364868 ms
forward last block: 7.7414398193359375 ms
##### model time: 238.03187561035156 #####
KV cache update for next block completed in 238.56 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 2 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 2 VAE decoding completed in 537.41 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 2 for sending...
‚úÖ Block 2 completed in 1887.98 ms (12 frames queued in 0.042 ms)
In loop: idx 2, current_num_frames 3, current_start_frame 6
üîÑ Processing block 3/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.16793599724769592 ms
---------------- attention block ----------------
		qkv norm: 12.295167922973633 ms
		rope: 1.1601920127868652 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.934783935546875 ms
		out projection: 0.2519040107727051 ms
	self-attn: 18.141183853149414 ms
	cross-attn: 1.265663981437683 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 27.853824615478516 ms
---------------- attention block ----------------
		qkv norm: 4.835328102111816 ms
		rope: 1.117184042930603 ms
		kv cache update (if evict): 2.7197439670562744 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 3.063807964324951 ms
		out projection: 0.3543039858341217 ms
	self-attn: 20.21990394592285 ms
	cross-attn: 1.6640000343322754 ms
	ffn: 1.5052800178527832 ms
forward previous block to here: 29.228031158447266 ms
---------------- attention block ----------------
		qkv norm: 0.92876797914505 ms
		rope: 1.141759991645813 ms
		kv cache update (if evict): 7.580671787261963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 6.916096210479736 ms
		out projection: 0.27238398790359497 ms
	self-attn: 19.901439666748047 ms
	cross-attn: 1.3527040481567383 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 30.00217628479004 ms
---------------- attention block ----------------
		qkv norm: 4.809728145599365 ms
		rope: 1.6517119407653809 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8784639835357666 ms
		out projection: 0.29388800263404846 ms
	self-attn: 20.24038314819336 ms
	cross-attn: 5.171199798583984 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 29.38982391357422 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0813440084457397 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 12.17024040222168 ms
		out projection: 0.28569599986076355 ms
	self-attn: 15.456255912780762 ms
	cross-attn: 1.1663360595703125 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 19.921920776367188 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 10.205183982849121 ms
		out projection: 0.39423999190330505 ms
	self-attn: 15.400959968566895 ms
	cross-attn: 1.5441919565200806 ms
	ffn: 3.8584320545196533 ms
forward previous block to here: 21.532672882080078 ms
---------------- attention block ----------------
		qkv norm: 8.052736282348633 ms
		rope: 5.182464122772217 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8968958854675293 ms
		out projection: 0.25702399015426636 ms
	self-attn: 18.24870491027832 ms
	cross-attn: 1.252351999282837 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 28.654592514038086 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.894848108291626 ms
		out projection: 0.2385919988155365 ms
	self-attn: 12.443648338317871 ms
	cross-attn: 1.1489280462265015 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 15.37945556640625 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.9173760414123535 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.997568130493164 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 8.833024024963379 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.2175359725952148 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.802687883377075 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.099967956542969 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 8.91801643371582 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.790719985961914 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 8.596480369567871 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8098559379577637 ms
		out projection: 0.25702399015426636 ms
	self-attn: 5.863423824310303 ms
	cross-attn: 1.1519999504089355 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 8.770560264587402 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.5298559665679932 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.24063999950885773 ms
	self-attn: 6.367231845855713 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 9.298944473266602 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.873663902282715 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.7193603515625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.979840040206909 ms
		out projection: 0.2457599937915802 ms
	self-attn: 6.115327835083008 ms
	cross-attn: 1.2410880327224731 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 9.234432220458984 ms
---------------- attention block ----------------
		qkv norm: 1.1796480417251587 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7985920906066895 ms
		out projection: 0.24063999950885773 ms
	self-attn: 6.449151992797852 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 9.332736015319824 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.863423824310303 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.658944129943848 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.773312091827393 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 8.579071998596191 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.1806720495224 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7842559814453125 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.003712177276611 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 8.81049633026123 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8293120861053467 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.819392204284668 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.597503662109375 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.772287845611572 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.621055603027344 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7883520126342773 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.875711917877197 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.660991668701172 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.758975982666016 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.583168029785156 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.77945613861084 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.555520057678223 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.786303997039795 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.7630720138549805 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.530943870544434 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.801664113998413 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.798912048339844 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.630271911621094 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.78220796585083 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.545280456542969 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.775360107421875 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.554495811462402 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.784575939178467 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.553471565246582 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.783552169799805 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4161920547485352 ms
forward last block: 8.580096244812012 ms
##### model time: 398.8346862792969 #####
	 Transformer denoising step 1/4 completed in 399.32 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09625600278377533 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.7876482009887695 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 8.6046724319458 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.791744232177734 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.624128341674805 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.764095783233643 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 8.570879936218262 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.761023998260498 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 8.581119537353516 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7750399112701416 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.767168045043945 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.578047752380371 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7842559814453125 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.7784318923950195 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.539135932922363 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.758975982666016 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.523776054382324 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.767168045043945 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.551424026489258 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.771967887878418 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.76204776763916 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.591360092163086 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.801664113998413 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.784575939178467 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.565759658813477 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8272640705108643 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.774335861206055 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.576000213623047 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2529279887676239 ms
	self-attn: 5.7876482009887695 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.563712120056152 ms
---------------- attention block ----------------
		qkv norm: 0.8919039964675903 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.2457599937915802 ms
	self-attn: 5.819392204284668 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.588288307189941 ms
---------------- attention block ----------------
		qkv norm: 0.8847360014915466 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7944960594177246 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.819392204284668 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.58521556854248 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.771967887878418 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.7630720138549805 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.547327995300293 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.739520072937012 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 8.518655776977539 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.8224639892578125 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 8.652799606323242 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.756927967071533 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.566783905029297 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7883520126342773 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.793791770935059 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 8.59340763092041 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.785600185394287 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 8.574975967407227 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.772991895675659 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.769216060638428 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.536064147949219 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.725183963775635 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.500224113464355 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7904000282287598 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.766143798828125 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.535039901733398 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.22937600314617157 ms
	self-attn: 5.724160194396973 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.526847839355469 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.22937600314617157 ms
	self-attn: 5.746687889099121 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 8.50227165222168 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.742591857910156 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.482815742492676 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.766848087310791 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.735424041748047 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 8.480768203735352 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.746687889099121 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 8.524800300598145 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.727231979370117 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.499199867248535 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.786303997039795 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.755904197692871 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.418239951133728 ms
forward last block: 8.550399780273438 ms
##### model time: 260.2342529296875 #####
	 Transformer denoising step 2/4 completed in 260.72 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.787328004837036 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.767168045043945 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 8.588288307189941 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.589311599731445 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.24166400730609894 ms
	self-attn: 5.730303764343262 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.546303749084473 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.24166400730609894 ms
	self-attn: 5.7784318923950195 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.543231964111328 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.770944118499756 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.756927967071533 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.52070426940918 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.774335861206055 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.545280456542969 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.751808166503906 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.531968116760254 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.821120023727417 ms
		out projection: 0.24063999950885773 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.578047752380371 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.24268800020217896 ms
	self-attn: 5.7927680015563965 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.60262393951416 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.77945613861084 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.547327995300293 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.735424041748047 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.50432014465332 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.766848087310791 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.741568088531494 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 8.560640335083008 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.24166400730609894 ms
	self-attn: 5.747712135314941 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.515583992004395 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.7282562255859375 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.497152328491211 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.796544075012207 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.7876482009887695 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.584192276000977 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.734399795532227 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.514559745788574 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.770944118499756 ms
		out projection: 0.24985599517822266 ms
	self-attn: 5.760000228881836 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.547327995300293 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.724160194396973 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.488960266113281 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.7292799949646 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.52889633178711 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7678720951080322 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.7487359046936035 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.515583992004395 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.735424041748047 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.49612808227539 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7975680828094482 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.767168045043945 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 8.544256210327148 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.7292799949646 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.515583992004395 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.26521599292755127 ms
	self-attn: 5.77126407623291 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.558591842651367 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.743616104125977 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.501248359680176 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.725183963775635 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.508416175842285 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.752831935882568 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.529919624328613 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.72108793258667 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.48691177368164 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7996160984039307 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.766143798828125 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.547327995300293 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.792448043823242 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4120960235595703 ms
forward last block: 8.523776054382324 ms
##### model time: 259.7519226074219 #####
	 Transformer denoising step 3/4 completed in 260.23 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09011200070381165 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8272640705108643 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.851136207580566 ms
	cross-attn: 1.1048959493637085 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 8.712191581726074 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7893760204315186 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.8275837898254395 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.633343696594238 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7975680828094482 ms
		out projection: 0.24780799448490143 ms
	self-attn: 5.818367958068848 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.641535758972168 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7893760204315186 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.810175895690918 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 8.640512466430664 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7750399112701416 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.7876482009887695 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 8.606719970703125 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.738495826721191 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 8.526847839355469 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.753856182098389 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.559616088867188 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.770944118499756 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.7579522132873535 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.569855690002441 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.78220796585083 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.754879951477051 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.5032958984375 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.76204776763916 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.537088394165039 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.7487359046936035 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.567808151245117 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.724160194396973 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.51251220703125 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7934720516204834 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.780479907989502 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.551424026489258 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.737472057342529 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.532992362976074 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.754879951477051 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.515583992004395 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.749760150909424 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 8.50227165222168 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.736447811126709 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.526847839355469 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.772991895675659 ms
		out projection: 0.24268800020217896 ms
	self-attn: 5.746687889099121 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.549375534057617 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7678720951080322 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.718016147613525 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.493056297302246 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7975680828094482 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.782527923583984 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.558591842651367 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.7333760261535645 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.524800300598145 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7750399112701416 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.742591857910156 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.5032958984375 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 8.49407958984375 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7678720951080322 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.7333760261535645 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.517631530761719 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.24063999950885773 ms
	self-attn: 5.742591857910156 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.522751808166504 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.22835199534893036 ms
	self-attn: 5.71289587020874 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.483839988708496 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.791424036026001 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.769216060638428 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.538111686706543 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.7282562255859375 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.524800300598145 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.770944118499756 ms
		out projection: 0.22937600314617157 ms
	self-attn: 5.739520072937012 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.497152328491211 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.7487359046936035 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4120960235595703 ms
forward last block: 8.50227165222168 ms
##### model time: 260.0099792480469 #####
	 Transformer final denoising step 4/4 completed in 260.51 ms
‚ö° Block 3 denoising completed in 1182.61 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.08908800035715103 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.775360107421875 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.556544303894043 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7842559814453125 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.807104110717773 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 8.568832397460938 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.746687889099121 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.527872085571289 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.741568088531494 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.548352241516113 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7648000717163086 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.7139201164245605 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.501248359680176 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7944960594177246 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.794816017150879 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.568832397460938 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7893760204315186 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.811200141906738 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.608768463134766 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.772991895675659 ms
		out projection: 0.24780799448490143 ms
	self-attn: 5.755904197692871 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.530943870544434 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.24268800020217896 ms
	self-attn: 5.749760150909424 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.517631530761719 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.7630720138549805 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.563712120056152 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7678720951080322 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.764095783233643 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.566783905029297 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.738495826721191 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 8.524800300598145 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7904000282287598 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.753856182098389 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.559616088867188 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7842559814453125 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.745664119720459 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.538111686706543 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.775360107421875 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.540160179138184 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.771967887878418 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.773312091827393 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.531968116760254 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.786303997039795 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.764095783233643 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.527872085571289 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7678720951080322 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.741568088531494 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.531968116760254 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.72211217880249 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 8.51353645324707 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.753856182098389 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.557567596435547 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.7579522132873535 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.568832397460938 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7842559814453125 ms
		out projection: 0.22937600314617157 ms
	self-attn: 5.753856182098389 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.515583992004395 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.751808166503906 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.51046371459961 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.769216060638428 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.523776054382324 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.751808166503906 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.549375534057617 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.738495826721191 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.516608238220215 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.772991895675659 ms
		out projection: 0.22937600314617157 ms
	self-attn: 5.7630720138549805 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.561663627624512 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.794816017150879 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.552448272705078 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.750783920288086 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.499199867248535 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.741568088531494 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.410048007965088 ms
forward last block: 8.481792449951172 ms
##### model time: 259.7498779296875 #####
KV cache update for next block completed in 260.29 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 3 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 3 VAE decoding completed in 537.94 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 3 for sending...
‚úÖ Block 3 completed in 2018.95 ms (12 frames queued in 0.038 ms)
In loop: idx 3, current_num_frames 3, current_start_frame 9
üîÑ Processing block 4/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.0870399996638298 ms
---------------- attention block ----------------
		qkv norm: 0.8970239758491516 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.7693440914154053 ms
		out projection: 0.6399999856948853 ms
	self-attn: 14.509056091308594 ms
	cross-attn: 5.548031806945801 ms
	ffn: 1.521664023399353 ms
forward previous block to here: 23.70355224609375 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.699712038040161 ms
		out projection: 0.27955201268196106 ms
	self-attn: 11.121664047241211 ms
	cross-attn: 5.338111877441406 ms
	ffn: 1.7274880409240723 ms
forward previous block to here: 20.45235252380371 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 11.064319610595703 ms
		out projection: 0.4065279960632324 ms
	self-attn: 15.851519584655762 ms
	cross-attn: 1.3199360370635986 ms
	ffn: 2.91430401802063 ms
forward previous block to here: 20.45439910888672 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0895359516143799 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 12.406784057617188 ms
		out projection: 0.2734079957008362 ms
	self-attn: 15.758336067199707 ms
	cross-attn: 1.2031999826431274 ms
	ffn: 2.9900801181793213 ms
forward previous block to here: 20.319232940673828 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 7.358463764190674 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.7273600101470947 ms
		out projection: 0.3051519989967346 ms
	self-attn: 15.20025634765625 ms
	cross-attn: 1.4254080057144165 ms
	ffn: 3.486720085144043 ms
forward previous block to here: 20.750335693359375 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 7.934976100921631 ms
		kv cache update (if evict): 0.24883200228214264 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 7.60422420501709 ms
		out projection: 0.29183998703956604 ms
	self-attn: 19.374080657958984 ms
	cross-attn: 1.3772799968719482 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 30.021631240844727 ms
---------------- attention block ----------------
		qkv norm: 5.30841588973999 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 6.116352081298828 ms
		out projection: 0.28467199206352234 ms
	self-attn: 20.826112747192383 ms
	cross-attn: 5.52345609664917 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 30.481407165527344 ms
---------------- attention block ----------------
		qkv norm: 0.8919039964675903 ms
		rope: 1.099776029586792 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 11.2424955368042 ms
		out projection: 0.3491840064525604 ms
	self-attn: 16.234495162963867 ms
	cross-attn: 1.2349439859390259 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 20.754432678222656 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.1089919805526733 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 11.960320472717285 ms
		out projection: 0.3420160114765167 ms
	self-attn: 16.457727432250977 ms
	cross-attn: 1.2472319602966309 ms
	ffn: 3.009536027908325 ms
forward previous block to here: 21.07699203491211 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5891199111938477 ms
		out projection: 0.25088000297546387 ms
	self-attn: 6.67852783203125 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 9.564160346984863 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5788800716400146 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.6263041496276855 ms
	cross-attn: 1.623039960861206 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.017791748046875 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.571712017059326 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.599679946899414 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 9.4269437789917 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.579904079437256 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.642687797546387 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.457663536071777 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.568960189819336 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 9.398271560668945 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.58022403717041 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 9.425919532775879 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.24883200228214264 ms
	self-attn: 6.584320068359375 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.367551803588867 ms
---------------- attention block ----------------
		qkv norm: 1.1079679727554321 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.831103801727295 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.62662410736084 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5624959468841553 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.534143924713135 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 9.348095893859863 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5543038845062256 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.571008205413818 ms
	cross-attn: 1.4090240001678467 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 9.71673583984375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5635199546813965 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.563839912414551 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.332736015319824 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5737600326538086 ms
		out projection: 0.2385919988155365 ms
	self-attn: 6.575104236602783 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.362431526184082 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5676159858703613 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.551551818847656 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 9.375743865966797 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.557375907897949 ms
		out projection: 0.2385919988155365 ms
	self-attn: 6.576128005981445 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 9.391103744506836 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5819520950317383 ms
		out projection: 0.2529279887676239 ms
	self-attn: 6.622208118438721 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 9.456640243530273 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.581247806549072 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 9.389056205749512 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.561791896820068 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 9.4269437789917 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5686399936676025 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.605823993682861 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 9.417728424072266 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.612991809844971 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.407487869262695 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6075520515441895 ms
		out projection: 0.25600001215934753 ms
	self-attn: 6.69593620300293 ms
	cross-attn: 1.1673599481582642 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 9.655296325683594 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6034560203552246 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.7051520347595215 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4295040369033813 ms
forward last block: 9.499648094177246 ms
##### model time: 411.1595458984375 #####
	 Transformer denoising step 1/4 completed in 411.65 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5686399936676025 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.58841609954834 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.398271560668945 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 9.355263710021973 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.545407772064209 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.352191925048828 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5543038845062256 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.547455787658691 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 9.31942367553711 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5279998779296875 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.334783554077148 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.563839912414551 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.349120140075684 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.565567970275879 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.567935943603516 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 9.392127990722656 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.57478404045105 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.556672096252441 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 9.457663536071777 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.610943794250488 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 9.463808059692383 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.547455787658691 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 9.343999862670898 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5543038845062256 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.31123161315918 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.565567970275879 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.53004789352417 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.32249641418457 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.24371199309825897 ms
	self-attn: 6.545407772064209 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.324543952941895 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.565567970275879 ms
		out projection: 0.2519040107727051 ms
	self-attn: 6.560768127441406 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.335807800292969 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.533120155334473 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.306112289428711 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.288703918457031 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.571712017059326 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.560768127441406 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.327615737915039 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.534143924713135 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.300992012023926 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.514688014984131 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.327615737915039 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5624959468841553 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.296895980834961 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.546432018280029 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.31123161315918 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.545088052749634 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.502399921417236 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.30508804321289 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5727360248565674 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.53107213973999 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.285632133483887 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.24063999950885773 ms
	self-attn: 6.5423359870910645 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.351167678833008 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.55020809173584 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.499328136444092 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 9.315327644348145 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5635199546813965 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.514688014984131 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.265151977539062 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.533120155334473 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.30406379699707 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.317376136779785 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.55020809173584 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.545407772064209 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.296895980834961 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.52185583114624 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4141440391540527 ms
forward last block: 9.30406379699707 ms
##### model time: 283.546630859375 #####
	 Transformer denoising step 2/4 completed in 284.04 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.08908800035715103 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.604800224304199 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.383935928344727 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.57478404045105 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.581247806549072 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.399295806884766 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5624959468841553 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.53926420211792 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.343999862670898 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.541312217712402 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 9.3306884765625 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.567935943603516 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.336832046508789 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.56659197807312 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5279998779296875 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.339903831481934 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.529024124145508 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.308159828186035 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.24985599517822266 ms
	self-attn: 6.560768127441406 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.32249641418457 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.560447931289673 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.524928092956543 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.335807800292969 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.288703918457031 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.565567970275879 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.582272052764893 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.358336448669434 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.2385919988155365 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.300992012023926 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.24473600089550018 ms
	self-attn: 6.5382399559021 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.3306884765625 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.511616230010986 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.281536102294922 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.560447931289673 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.512639999389648 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.297920227050781 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.525951862335205 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.32249641418457 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.53004789352417 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.302016258239746 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.523903846740723 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.274368286132812 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.540287971496582 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.340928077697754 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.532095909118652 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 9.296895980834961 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.5279998779296875 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.283583641052246 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.32863998413086 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.565567970275879 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5279998779296875 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.292799949645996 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.533120155334473 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.300992012023926 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.545088052749634 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.508543968200684 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.286656379699707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.510591983795166 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.271295547485352 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.519807815551758 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.287679672241211 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.545088052749634 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.504447937011719 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.289728164672852 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.504447937011719 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.279487609863281 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.533120155334473 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4131200313568115 ms
forward last block: 9.31942367553711 ms
##### model time: 283.03668212890625 #####
	 Transformer denoising step 3/4 completed in 283.51 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.08806400001049042 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.577151775360107 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 9.350144386291504 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5819520950317383 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.554624080657959 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.345024108886719 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.552576065063477 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.332736015319824 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 9.336832046508789 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5532801151275635 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.569983959197998 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 9.331711769104004 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.523903846740723 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.312255859375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 9.309184074401855 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.25804799795150757 ms
	self-attn: 6.58841609954834 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.365504264831543 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5624959468841553 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.52185583114624 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.31942367553711 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.545088052749634 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.517759799957275 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.281536102294922 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.2590720057487488 ms
	self-attn: 6.564864158630371 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 9.312255859375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.52185583114624 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 9.30406379699707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.299967765808105 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.547455787658691 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.30508804321289 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.517759799957275 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.312255859375 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.547455787658691 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.32966423034668 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.548480033874512 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.310208320617676 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.517759799957275 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.284607887268066 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.545407772064209 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.351167678833008 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.544384002685547 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.315327644348145 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.557375907897949 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.5136637687683105 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.292799949645996 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5727360248565674 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.560768127441406 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.352191925048828 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5624959468841553 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.536191940307617 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.317376136779785 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.543360233306885 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.308159828186035 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.5279998779296875 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.323519706726074 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.557375907897949 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.5382399559021 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.30303955078125 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 9.31328010559082 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5532801151275635 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.501376152038574 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.307135581970215 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.532095909118652 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.285632133483887 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.565567970275879 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.573056221008301 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.415168046951294 ms
forward last block: 9.336832046508789 ms
##### model time: 283.14727783203125 #####
	 Transformer final denoising step 4/4 completed in 283.64 ms
‚ö° Block 4 denoising completed in 1264.78 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5532801151275635 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.567935943603516 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.345024108886719 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5840001106262207 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.567935943603516 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.370623588562012 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.582272052764893 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.336832046508789 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5737600326538086 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.554624080657959 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.325568199157715 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.585024118423462 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.53107213973999 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.31328010559082 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.589439868927002 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.346048355102539 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.511616230010986 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 9.30303955078125 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.5576958656311035 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.346048355102539 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.533120155334473 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.32044792175293 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5686399936676025 ms
		out projection: 0.24780799448490143 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 9.326592445373535 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.559743881225586 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.331711769104004 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.532095909118652 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.324543952941895 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.525951862335205 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.307135581970215 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.2539519965648651 ms
	self-attn: 6.571008205413818 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.336832046508789 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.515711784362793 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.296895980834961 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.534143924713135 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.296895980834961 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.5423359870910645 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.298944473266602 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.510591983795166 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.274368286132812 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.30406379699707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.557375907897949 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.546432018280029 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.317376136779785 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.558720111846924 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.324543952941895 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.55020809173584 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.294848442077637 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5870718955993652 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.556672096252441 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.316351890563965 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.548480033874512 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.315327644348145 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.520832061767578 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.31328010559082 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.278464317321777 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.24063999950885773 ms
	self-attn: 6.541312217712402 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.32044792175293 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.55020809173584 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.499328136444092 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 9.306112289428711 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.520832061767578 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.278464317321777 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.24371199309825897 ms
	self-attn: 6.546432018280029 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4202879667282104 ms
forward last block: 9.324543952941895 ms
##### model time: 283.09912109375 #####
KV cache update for next block completed in 283.63 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 4 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 4 VAE decoding completed in 538.31 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 4 for sending...
‚úÖ Block 4 completed in 2125.41 ms (12 frames queued in 0.038 ms)
In loop: idx 4, current_num_frames 3, current_start_frame 12
üîÑ Processing block 5/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 2.4350719451904297 ms
---------------- attention block ----------------
		qkv norm: 0.8755199909210205 ms
		rope: 1.1048959493637085 ms
		kv cache update (if evict): 7.938047885894775 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 9.193471908569336 ms
		out projection: 0.3092480003833771 ms
	self-attn: 21.201919555664062 ms
	cross-attn: 1.3731839656829834 ms
	ffn: 1.5134719610214233 ms
forward previous block to here: 31.066112518310547 ms
---------------- attention block ----------------
		qkv norm: 4.835328102111816 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 7.217152118682861 ms
		out projection: 0.30003198981285095 ms
	self-attn: 20.08678436279297 ms
	cross-attn: 5.4231038093566895 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 29.033472061157227 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 7.623680114746094 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 8.473600387573242 ms
		out projection: 0.2662400007247925 ms
	self-attn: 20.195327758789062 ms
	cross-attn: 1.3015040159225464 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 29.817855834960938 ms
---------------- attention block ----------------
		qkv norm: 4.827136039733887 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 6.93555212020874 ms
		out projection: 0.5611519813537598 ms
	self-attn: 24.13158416748047 ms
	cross-attn: 1.210368037223816 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 29.41747283935547 ms
---------------- attention block ----------------
		qkv norm: 0.9113600254058838 ms
		rope: 8.572928428649902 ms
		kv cache update (if evict): 0.18943999707698822 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 8.472576141357422 ms
		out projection: 0.5765119791030884 ms
	self-attn: 20.173824310302734 ms
	cross-attn: 1.3567999601364136 ms
	ffn: 8.019968032836914 ms
forward previous block to here: 29.95814323425293 ms
---------------- attention block ----------------
		qkv norm: 0.9615359902381897 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 2.6275839805603027 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.459519863128662 ms
		out projection: 0.28569599986076355 ms
	self-attn: 19.49388885498047 ms
	cross-attn: 5.741568088531494 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 28.77440071105957 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.22630399465560913 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 8.440832138061523 ms
		out projection: 0.27136000990867615 ms
	self-attn: 19.6628475189209 ms
	cross-attn: 1.1724799871444702 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 22.633472442626953 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.365312099456787 ms
		out projection: 0.25702399015426636 ms
	self-attn: 7.445504188537598 ms
	cross-attn: 1.1151360273361206 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 10.331135749816895 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.356095790863037 ms
		out projection: 0.24268800020217896 ms
	self-attn: 7.4260478019714355 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 10.272768020629883 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.344831943511963 ms
		out projection: 0.24780799448490143 ms
	self-attn: 7.408639907836914 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.448896408081055 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.413760185241699 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.511360168457031 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.340735912322998 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.3963518142700195 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 10.22156810760498 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.348927974700928 ms
		out projection: 0.25702399015426636 ms
	self-attn: 7.364607810974121 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.185728073120117 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.360511779785156 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.174464225769043 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.364287853240967 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.4055681228637695 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.203136444091797 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.1884160041809082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.393983840942383 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.532544136047363 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.396672248840332 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.362559795379639 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.171392440795898 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.357439994812012 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.184703826904297 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.356095790863037 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.421951770782471 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.217472076416016 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.35097599029541 ms
		out projection: 0.2539519965648651 ms
	self-attn: 7.365632057189941 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.264575958251953 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.340735912322998 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.402495861053467 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 10.234880447387695 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.18329599499702454 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.35916805267334 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.444479942321777 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.248191833496094 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.21606400609016418 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.379648208618164 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.5489277839660645 ms
	cross-attn: 1.1059199571609497 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.417152404785156 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.43936014175415 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.271743774414062 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.392255783081055 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.218496322631836 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.354047775268555 ms
		out projection: 0.2519040107727051 ms
	self-attn: 7.390207767486572 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 10.270719528198242 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.368383884429932 ms
		out projection: 0.24166400730609894 ms
	self-attn: 7.457791805267334 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 10.316800117492676 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.35916805267334 ms
		out projection: 0.24985599517822266 ms
	self-attn: 7.4700798988342285 ms
	cross-attn: 1.1263999938964844 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 10.407936096191406 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.346879959106445 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.408639907836914 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 10.2543363571167 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3376641273498535 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.388160228729248 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4335999488830566 ms
forward last block: 10.226688385009766 ms
##### model time: 443.77703857421875 #####
	 Transformer denoising step 1/4 completed in 444.27 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09011200070381165 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.364287853240967 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.402495861053467 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 10.233856201171875 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.345856189727783 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.368703842163086 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.2041597366333 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.360511779785156 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.171392440795898 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.335936069488525 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.174464225769043 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.385791778564453 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.385087966918945 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.2041597366333 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3673601150512695 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3809919357299805 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.198016166687012 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.332543849945068 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3512959480285645 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 10.170368194580078 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.384064197540283 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.182656288146973 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.369408130645752 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.422976016998291 ms
	cross-attn: 1.1560959815979004 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.341376304626465 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.348927974700928 ms
		out projection: 0.24473600089550018 ms
	self-attn: 7.394303798675537 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.22976016998291 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.2805759906768799 ms
	self-attn: 7.425024032592773 ms
	cross-attn: 1.1898880004882812 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 10.395648002624512 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.40831995010376 ms
		out projection: 0.24883200228214264 ms
	self-attn: 7.501823902130127 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 10.387455940246582 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.379648208618164 ms
		out projection: 0.2590720057487488 ms
	self-attn: 7.468031883239746 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 10.33625602722168 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.370431900024414 ms
		out projection: 0.2457599937915802 ms
	self-attn: 7.43936014175415 ms
	cross-attn: 1.1028480529785156 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 10.32806396484375 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.35097599029541 ms
		out projection: 0.24780799448490143 ms
	self-attn: 7.428095817565918 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.256383895874023 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.2529279887676239 ms
	self-attn: 7.398399829864502 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.275839805603027 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.35097599029541 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.3758721351623535 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 10.233856201171875 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.374527931213379 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.4106879234313965 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.249216079711914 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.340735912322998 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3809919357299805 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 10.226688385009766 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.2682879865169525 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.344831943511963 ms
		out projection: 0.24371199309825897 ms
	self-attn: 7.510015964508057 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 10.346495628356934 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.352320194244385 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.210304260253906 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3520002365112305 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.349247932434082 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.177536010742188 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.341760158538818 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.3963518142700195 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.233856201171875 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.332543849945068 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.398399829864502 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.243071556091309 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.380671977996826 ms
		out projection: 0.25600001215934753 ms
	self-attn: 7.500800132751465 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 10.34547233581543 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.373824119567871 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.175488471984863 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.330495834350586 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.354368209838867 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.12019157409668 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.346879959106445 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.365632057189941 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.184703826904297 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.341760158538818 ms
		out projection: 0.24371199309825897 ms
	self-attn: 7.35641622543335 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 10.191871643066406 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.344831943511963 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.341055870056152 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4325759410858154 ms
forward last block: 10.183679580688477 ms
##### model time: 310.9150695800781 #####
	 Transformer denoising step 2/4 completed in 311.41 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3520002365112305 ms
		out projection: 0.25088000297546387 ms
	self-attn: 7.4199042320251465 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.276864051818848 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.362239837646484 ms
		out projection: 0.24268800020217896 ms
	self-attn: 7.402495861053467 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 10.274815559387207 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.340735912322998 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.385087966918945 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.182656288146973 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3376641273498535 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.377920150756836 ms
	cross-attn: 1.1223039627075195 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.241024017333984 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.348927974700928 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.353343963623047 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.148863792419434 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3673601150512695 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.3512959480285645 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.140671730041504 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.321599960327148 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 10.136575698852539 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.376895904541016 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.140671730041504 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.24473600089550018 ms
	self-attn: 7.35641622543335 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.162176132202148 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.334911823272705 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.150912284851074 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.343808174133301 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.3164801597595215 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.13043212890625 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.338687896728516 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.334911823272705 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.126336097717285 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.326720237731934 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 10.184703826904297 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.329472064971924 ms
		out projection: 0.24371199309825897 ms
	self-attn: 7.355391979217529 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.13145637512207 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.24883200228214264 ms
	self-attn: 7.353343963623047 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.134528160095215 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.340735912322998 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.3072638511657715 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.082304000854492 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.360191822052002 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.3164801597595215 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.099712371826172 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.3021440505981445 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 10.097663879394531 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.361216068267822 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.385087966918945 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.13862419128418 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.330495834350586 ms
		out projection: 0.24268800020217896 ms
	self-attn: 7.353343963623047 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.149888038635254 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.332543849945068 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.3164801597595215 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.12326431274414 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.345856189727783 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.309311866760254 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.149888038635254 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.315455913543701 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.12838363647461 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.326720237731934 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.127360343933105 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.337984085083008 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.155008316040039 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.330495834350586 ms
		out projection: 0.2662400007247925 ms
	self-attn: 7.34822416305542 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.12838363647461 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.344831943511963 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.34822416305542 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.164223670959473 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.362239837646484 ms
		out projection: 0.27238398790359497 ms
	self-attn: 7.481344223022461 ms
	cross-attn: 1.2636159658432007 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 10.568703651428223 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.424704074859619 ms
		out projection: 0.2611199915409088 ms
	self-attn: 7.733248233795166 ms
	cross-attn: 1.2810239791870117 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 10.853376388549805 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.434944152832031 ms
		out projection: 0.2611199915409088 ms
	self-attn: 7.712768077850342 ms
	cross-attn: 1.2339199781417847 ms
	ffn: 1.4858239889144897 ms
forward last block: 10.818559646606445 ms
##### model time: 310.1419372558594 #####
	 Transformer denoising step 3/4 completed in 310.74 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.12083200365304947 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.444159984588623 ms
		out projection: 0.2590720057487488 ms
	self-attn: 7.747583866119385 ms
	cross-attn: 1.2462079524993896 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 10.844160079956055 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.386816024780273 ms
		out projection: 0.24371199309825897 ms
	self-attn: 7.663616180419922 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.489855766296387 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3471999168396 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.183679580688477 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.366335868835449 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.379968166351318 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.166272163391113 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.363584041595459 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 10.176511764526367 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.382016181945801 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.2041597366333 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.355072021484375 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.372799873352051 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.208255767822266 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.358463764190674 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.174464225769043 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.361536026000977 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.193920135498047 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.367680072784424 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.195967674255371 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.35097599029541 ms
		out projection: 0.2529279887676239 ms
	self-attn: 7.35641622543335 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.168319702148438 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.338687896728516 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.342080116271973 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.187775611877441 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.363264083862305 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.403520107269287 ms
	cross-attn: 1.1550719738006592 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.305536270141602 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.345856189727783 ms
		out projection: 0.24371199309825897 ms
	self-attn: 7.379968166351318 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.202112197875977 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.347904205322266 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.33081579208374 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.162176132202148 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.33900785446167 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.162176132202148 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.374847888946533 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.181632041931152 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.332863807678223 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.158080101013184 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.35097599029541 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.359488010406494 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.148863792419434 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.348927974700928 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.350272178649902 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.152959823608398 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.345151901245117 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.1396484375 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.354047775268555 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.363584041595459 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.144767761230469 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.358463764190674 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.149888038635254 ms
---------------- attention block ----------------
		qkv norm: 0.9205759763717651 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.456448078155518 ms
		out projection: 0.26009601354599 ms
	self-attn: 7.756800174713135 ms
	cross-attn: 1.2472319602966309 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 10.858495712280273 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.439040184020996 ms
		out projection: 0.2590720057487488 ms
	self-attn: 7.713791847229004 ms
	cross-attn: 1.2492799758911133 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 10.798080444335938 ms
---------------- attention block ----------------
		qkv norm: 0.8826879858970642 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.447231769561768 ms
		out projection: 0.25702399015426636 ms
	self-attn: 7.727104187011719 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 10.876928329467773 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0833920240402222 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.432896137237549 ms
		out projection: 0.26009601354599 ms
	self-attn: 7.725056171417236 ms
	cross-attn: 1.2349439859390259 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 10.807295799255371 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.1079679727554321 ms
		kv cache update (if evict): 0.19763199985027313 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.435967922210693 ms
		out projection: 0.2549760043621063 ms
	self-attn: 7.8305277824401855 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 10.695679664611816 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.337984085083008 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.174464225769043 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.386112213134766 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4172159433364868 ms
forward last block: 10.170368194580078 ms
##### model time: 313.5436706542969 #####
	 Transformer final denoising step 4/4 completed in 314.04 ms
‚ö° Block 5 denoising completed in 1382.51 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.2529279887676239 ms
	self-attn: 7.393280029296875 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 10.233856201171875 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.369728088378906 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.168319702148438 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.346879959106445 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.350272178649902 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.192895889282227 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.353024005889893 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3369598388671875 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 10.145792007446289 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.357439994812012 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.178560256958008 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.2457599937915802 ms
	self-attn: 7.377920150756836 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.183679580688477 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.354368209838867 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.152959823608398 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3427839279174805 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.335936069488525 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.161151885986328 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.355072021484375 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3471999168396 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.147839546203613 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.361536026000977 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.158080101013184 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.332543849945068 ms
		out projection: 0.2457599937915802 ms
	self-attn: 7.357439994812012 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.14681625366211 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3758721351623535 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.195967674255371 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.3369598388671875 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.134528160095215 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.3318400382995605 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.147839546203613 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.35641622543335 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.142720222473145 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.329472064971924 ms
		out projection: 0.24371199309825897 ms
	self-attn: 7.327744007110596 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.133503913879395 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.338687896728516 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.3461761474609375 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.132479667663574 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3427839279174805 ms
		out projection: 0.25702399015426636 ms
	self-attn: 7.342080116271973 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.145792007446289 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.376895904541016 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 10.23795223236084 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.341760158538818 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.377920150756836 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.168319702148438 ms
---------------- attention block ----------------
		qkv norm: 0.8765439987182617 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.357120037078857 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.391232013702393 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.201087951660156 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.341760158538818 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.337984085083008 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.142720222473145 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.360511779785156 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.187775611877441 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.344831943511963 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.33081579208374 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.12838363647461 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.350272178649902 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.193920135498047 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.24268800020217896 ms
	self-attn: 7.359488010406494 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.166272163391113 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.348927974700928 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.358463764190674 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.132479667663574 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.319551944732666 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.116095542907715 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.361216068267822 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.385087966918945 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.174464225769043 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.357439994812012 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward last block: 10.126336097717285 ms
##### model time: 308.4984436035156 #####
KV cache update for next block completed in 309.05 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 5 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 5 VAE decoding completed in 538.94 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 5 for sending...
‚úÖ Block 5 completed in 2269.46 ms (12 frames queued in 0.039 ms)
In loop: idx 5, current_num_frames 3, current_start_frame 15
üîÑ Processing block 6/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.9236479997634888 ms
		rope: 1.0895359516143799 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 13.278207778930664 ms
		out projection: 0.2682879865169525 ms
	self-attn: 16.659456253051758 ms
	cross-attn: 3.5287039279937744 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 22.040576934814453 ms
---------------- attention block ----------------
		qkv norm: 7.9534077644348145 ms
		rope: 5.041152000427246 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.185535907745361 ms
		out projection: 0.2539519965648651 ms
	self-attn: 19.480575561523438 ms
	cross-attn: 5.243904113769531 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 35.98438262939453 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.2385919988155365 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 9.525247573852539 ms
		out projection: 0.2744320034980774 ms
	self-attn: 19.944448471069336 ms
	cross-attn: 8.328191757202148 ms
	ffn: 4.855807781219482 ms
forward previous block to here: 33.514495849609375 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 3.3177599906921387 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.212160110473633 ms
		out projection: 4.384768009185791 ms
	self-attn: 18.455551147460938 ms
	cross-attn: 1.2175359725952148 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 22.817792892456055 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.19968000054359436 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 9.066495895385742 ms
		out projection: 0.2764799892902374 ms
	self-attn: 19.69152069091797 ms
	cross-attn: 1.2564480304718018 ms
	ffn: 7.724031925201416 ms
forward previous block to here: 29.083648681640625 ms
---------------- attention block ----------------
		qkv norm: 0.942080020904541 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.244927883148193 ms
		out projection: 0.30105599761009216 ms
	self-attn: 19.2860164642334 ms
	cross-attn: 5.6104960441589355 ms
	ffn: 2.7842559814453125 ms
forward previous block to here: 28.039167404174805 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.21094399690628052 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 9.156607627868652 ms
		out projection: 0.27033600211143494 ms
	self-attn: 19.46214485168457 ms
	cross-attn: 1.1601920127868652 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 22.395904541015625 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.141503810882568 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.237055778503418 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.104255676269531 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.142848014831543 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 10.981375694274902 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.144895553588867 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.950655937194824 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.134335994720459 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.143872261047363 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 10.973183631896973 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.18892765045166 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.001855850219727 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.141823768615723 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.968064308166504 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.114880084991455 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.138751983642578 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.941439628601074 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.138432025909424 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.230912208557129 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.02847957611084 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.157183647155762 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.926079750061035 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.105663776397705 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.134655952453613 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.961919784545898 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.144895553588867 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.935296058654785 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.141503810882568 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.155136108398438 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.95577621459961 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.125439643859863 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.902527809143066 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.133631706237793 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.951680183410645 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.1909761428833 ms
	cross-attn: 1.1079679727554321 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 11.087871551513672 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.129216194152832 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.23193645477295 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.086848258972168 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1220479011535645 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.157183647155762 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.995712280273438 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.129216194152832 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.196096420288086 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.0632963180542 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.177663803100586 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.964991569519043 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.143872261047363 ms
	cross-attn: 1.2974079847335815 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.189248085021973 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.10700798034668 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.883071899414062 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.134655952453613 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.289600372314453 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.20019245147705 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4305280447006226 ms
forward last block: 11.052032470703125 ms
##### model time: 453.54290771484375 #####
	 Transformer denoising step 1/4 completed in 454.04 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.164352416992188 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.941439628601074 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.09984016418457 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.870783805847168 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.11622428894043 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.927103996276855 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.096768379211426 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.875904083251953 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.138432025909424 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.136704444885254 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.93836784362793 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.114175796508789 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.891263961791992 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.104960441589355 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.915840148925781 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.10700798034668 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.912768363952637 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.139455795288086 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.139776229858398 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.950655937194824 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.127488136291504 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.915840148925781 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.089599609375 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.897407531738281 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.080384254455566 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.886143684387207 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.126143932342529 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.085503578186035 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.85644817352295 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.139776229858398 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.909695625305176 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1220479011535645 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.09779167175293 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.874879837036133 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.103936195373535 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.873855590820312 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.126463890075684 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.908672332763672 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.154816150665283 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.153087615966797 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.952704429626465 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.132608413696289 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.949631690979004 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.124095916748047 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.138751983642578 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.920960426330566 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.072192192077637 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.83903980255127 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.08243179321289 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.876928329467773 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.105663776397705 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.121343612670898 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 10.900480270385742 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.10905647277832 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.884096145629883 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.102911949157715 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.872832298278809 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.129535675048828 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.962944030761719 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.174592018127441 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.0315523147583 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.19148799777030945 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.249023914337158 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.449024200439453 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.283455848693848 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.09164810180664 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.873855590820312 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.154111862182617 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4192639589309692 ms
forward last block: 10.982399940490723 ms
##### model time: 331.315185546875 #####
	 Transformer denoising step 2/4 completed in 331.82 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09011200070381165 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.128191947937012 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.154111862182617 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.962944030761719 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.125120162963867 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.125439643859863 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.904576301574707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1220479011535645 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.11622428894043 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.92300796508789 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.123392105102539 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.914815902709961 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.170176029205322 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.175616264343262 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.990592002868652 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.113151550292969 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.896384239196777 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1220479011535645 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.168448448181152 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.979328155517578 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.126143932342529 ms
		out projection: 0.27750399708747864 ms
	self-attn: 8.271871566772461 ms
	cross-attn: 1.134592056274414 ms
	ffn: 1.5144959688186646 ms
forward previous block to here: 11.302911758422852 ms
---------------- attention block ----------------
		qkv norm: 0.9021440148353577 ms
		rope: 1.1479040384292603 ms
		kv cache update (if evict): 0.18636800348758698 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.280767917633057 ms
		out projection: 0.2744320034980774 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.1499520540237427 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.849727630615234 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.124095916748047 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.148991584777832 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.985471725463867 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.164031982421875 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.19916820526123 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.041791915893555 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.20736026763916 ms
	cross-attn: 1.1386879682540894 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.130880355834961 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.11622428894043 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.95680046081543 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.122367858886719 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.907648086547852 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.157887935638428 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.157183647155762 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.966015815734863 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.147968292236328 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.92198371887207 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.126463890075684 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.968064308166504 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.126463890075684 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.94655990600586 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1671037673950195 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.187904357910156 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.975232124328613 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.104960441589355 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.872832298278809 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.103936195373535 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.935296058654785 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.127488136291504 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.907648086547852 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.135359764099121 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.119296073913574 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.899456024169922 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.167424201965332 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.011072158813477 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.179712295532227 ms
	cross-attn: 1.1048959493637085 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.053055763244629 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.125120162963867 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.175616264343262 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 11.03872013092041 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.125120162963867 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.192000389099121 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.998784065246582 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.111104011535645 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.890239715576172 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.132608413696289 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.953727722167969 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.125120162963867 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.117247581481934 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4428160190582275 ms
forward last block: 10.952704429626465 ms
##### model time: 333.73797607421875 #####
	 Transformer denoising step 3/4 completed in 334.22 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09728000313043594 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.112128257751465 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 10.928128242492676 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.112128257751465 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.934271812438965 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.113151550292969 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 10.93120002746582 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.142848014831543 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.945535659790039 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.10905647277832 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.880000114440918 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.095744132995605 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.901503562927246 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.104960441589355 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 10.899456024169922 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.142528057098389 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.136704444885254 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.906623840332031 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.09164810180664 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.876928329467773 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.10086441040039 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.914815902709961 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.129535675048828 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.934271812438965 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.133312225341797 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.170495986938477 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.002880096435547 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.121343612670898 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.901503562927246 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.102911949157715 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 10.961919784545898 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.120320320129395 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.901503562927246 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1374077796936035 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.126463890075684 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.93222427368164 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.142848014831543 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 10.933247566223145 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.09164810180664 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.903552055358887 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.09779167175293 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.884096145629883 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.09164810180664 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 10.885120391845703 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.09984016418457 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.877951622009277 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.09881591796875 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.902527809143066 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.129216194152832 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.113151550292969 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.909695625305176 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.124095916748047 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.125439643859863 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.915840148925781 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1312642097473145 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.19711971282959 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.011072158813477 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.126143932342529 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.133631706237793 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.985471725463867 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.125120162963867 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.129535675048828 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.909695625305176 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.10086441040039 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.870783805847168 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.117247581481934 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.904576301574707 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.122367858886719 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4254080057144165 ms
forward last block: 10.918911933898926 ms
##### model time: 331.1677551269531 #####
	 Transformer final denoising step 4/4 completed in 331.67 ms
‚ö° Block 6 denoising completed in 1453.78 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.165375709533691 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.95577621459961 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.114175796508789 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.920960426330566 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.095744132995605 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 10.892288208007812 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.129216194152832 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.10086441040039 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.867712020874023 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.118271827697754 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.896384239196777 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.101887702941895 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.902527809143066 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.11520004272461 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.919936180114746 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.120320320129395 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.945535659790039 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.11520004272461 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.897407531738281 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.088576316833496 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.901503562927246 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.078335762023926 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.865663528442383 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.09062385559082 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.902527809143066 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1220479011535645 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.10700798034668 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.884096145629883 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.096768379211426 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.863615989685059 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.085503578186035 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.864640235900879 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.094719886779785 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.85644817352295 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1374077796936035 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.236031532287598 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.03769588470459 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.127488136291504 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.925056457519531 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.11520004272461 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.904576301574707 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.103616237640381 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.071167945861816 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.845184326171875 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.102911949157715 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.916864395141602 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.132608413696289 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 10.919936180114746 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.153087615966797 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.93017578125 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.10905647277832 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.865663528442383 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.09881591796875 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.896384239196777 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.104960441589355 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.878975868225098 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.138432025909424 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.148991584777832 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.926079750061035 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.3020800054073334 ms
	self-attn: 8.39782428741455 ms
	cross-attn: 1.4325759410858154 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.087488055229187 ms
		kv cache update (if evict): 0.20070399343967438 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.237760066986084 ms
		out projection: 0.31334400177001953 ms
	self-attn: 8.670207977294922 ms
	cross-attn: 1.376255989074707 ms
	ffn: 1.5104000568389893 ms
forward previous block to here: 11.94598388671875 ms
---------------- attention block ----------------
		qkv norm: 0.9472000002861023 ms
		rope: 1.099776029586792 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.231616020202637 ms
		out projection: 0.39423999190330505 ms
	self-attn: 9.010175704956055 ms
	cross-attn: 1.5052800178527832 ms
	ffn: 1.5073280334472656 ms
forward last block: 12.425215721130371 ms
##### model time: 334.3953857421875 #####
KV cache update for next block completed in 335.13 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 6 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 6 VAE decoding completed in 540.45 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 6 for sending...
‚úÖ Block 6 completed in 2370.39 ms (12 frames queued in 0.041 ms)
In loop: idx 6, current_num_frames 3, current_start_frame 18
üîÑ Processing block 7/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11980800330638885 ms
---------------- attention block ----------------
		qkv norm: 1.0024960041046143 ms
		rope: 13.27616024017334 ms
		kv cache update (if evict): 0.1945600062608719 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.402048110961914 ms
		out projection: 7.528448104858398 ms
	self-attn: 29.455360412597656 ms
	cross-attn: 5.4824957847595215 ms
	ffn: 1.5073280334472656 ms
forward previous block to here: 39.08505630493164 ms
---------------- attention block ----------------
		qkv norm: 0.8765439987182617 ms
		rope: 1.117184042930603 ms
		kv cache update (if evict): 0.2529279887676239 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.061823844909668 ms
		out projection: 0.35942399501800537 ms
	self-attn: 20.68992042541504 ms
	cross-attn: 1.4131200313568115 ms
	ffn: 7.791615962982178 ms
forward previous block to here: 30.34726333618164 ms
---------------- attention block ----------------
		qkv norm: 1.2421120405197144 ms
		rope: 1.0751999616622925 ms
		kv cache update (if evict): 0.19148799777030945 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.080512046813965 ms
		out projection: 0.3307519853115082 ms
	self-attn: 20.333568572998047 ms
	cross-attn: 5.36575984954834 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 29.227008819580078 ms
---------------- attention block ----------------
		qkv norm: 0.8796160221099854 ms
		rope: 1.0967040061950684 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 18.03366470336914 ms
		out projection: 0.317440003156662 ms
	self-attn: 21.382144927978516 ms
	cross-attn: 1.4284800291061401 ms
	ffn: 7.838719844818115 ms
forward previous block to here: 31.110143661499023 ms
---------------- attention block ----------------
		qkv norm: 4.7923197746276855 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.5550079941749573 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.062079906463623 ms
		out projection: 0.38604798913002014 ms
	self-attn: 20.562944412231445 ms
	cross-attn: 5.550079822540283 ms
	ffn: 1.5134719610214233 ms
forward previous block to here: 29.75436782836914 ms
---------------- attention block ----------------
		qkv norm: 0.9031680226325989 ms
		rope: 1.117184042930603 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.86188793182373 ms
		out projection: 0.2959359884262085 ms
	self-attn: 17.225727081298828 ms
	cross-attn: 3.9587841033935547 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 30.342144012451172 ms
---------------- attention block ----------------
		qkv norm: 5.049344062805176 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.25088000297546387 ms
	self-attn: 16.073728561401367 ms
	cross-attn: 1.1950080394744873 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 19.170303344726562 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.22835199534893036 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.338879585266113 ms
	cross-attn: 1.201151967048645 ms
	ffn: 1.551360011100769 ms
forward previous block to here: 12.54911994934082 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 1.457152009010315 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.934080123901367 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.50271987915039 ms
	cross-attn: 1.1325440406799316 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 12.411904335021973 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.147071838378906 ms
		out projection: 0.3051519989967346 ms
	self-attn: 9.377792358398438 ms
	cross-attn: 1.1335680484771729 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.313599586486816 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.3250559568405151 ms
		kv cache update (if evict): 0.187391996383667 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.486335754394531 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.371968269348145 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.037823677062988 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 11.967488288879395 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.5007359981536865 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.975039958953857 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.704447746276855 ms
	cross-attn: 1.2154879570007324 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.785663604736328 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.984576225280762 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.997183799743652 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.12384033203125 ms
	cross-attn: 1.3578239679336548 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 12.397567749023438 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.00915241241455 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.895808219909668 ms
---------------- attention block ----------------
		qkv norm: 0.9390079975128174 ms
		rope: 1.2113920450210571 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.2744320034980774 ms
	self-attn: 9.350144386291504 ms
	cross-attn: 1.1141120195388794 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.300288200378418 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.2539519965648651 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.975039958953857 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.362431526184082 ms
	cross-attn: 1.1386879682540894 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.291071891784668 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.92793607711792 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.246720314025879 ms
	cross-attn: 1.1786240339279175 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.238847732543945 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.03545618057251 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.239551544189453 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.123135566711426 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.943295955657959 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.149439811706543 ms
	cross-attn: 1.2216320037841797 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.190719604492188 ms
---------------- attention block ----------------
		qkv norm: 1.0711040496826172 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9699201583862305 ms
		out projection: 0.3102720081806183 ms
	self-attn: 9.654272079467773 ms
	cross-attn: 1.2718080282211304 ms
	ffn: 1.5953919887542725 ms
forward previous block to here: 12.924927711486816 ms
---------------- attention block ----------------
		qkv norm: 0.9215999841690063 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.2088959962129593 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.967872142791748 ms
		out projection: 0.27136000990867615 ms
	self-attn: 9.331711769104004 ms
	cross-attn: 1.1427839994430542 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.35865592956543 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.986303806304932 ms
		out projection: 0.33689600229263306 ms
	self-attn: 9.362431526184082 ms
	cross-attn: 1.4284800291061401 ms
	ffn: 1.5155199766159058 ms
forward previous block to here: 12.690431594848633 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.1991039514541626 ms
		kv cache update (if evict): 0.18534399569034576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.060031890869141 ms
		out projection: 0.39321601390838623 ms
	self-attn: 9.797632217407227 ms
	cross-attn: 1.4766080379486084 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 13.114368438720703 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.2677119970321655 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.051839828491211 ms
		out projection: 0.4249599874019623 ms
	self-attn: 9.921536445617676 ms
	cross-attn: 1.5206400156021118 ms
	ffn: 1.5032320022583008 ms
forward previous block to here: 13.321215629577637 ms
---------------- attention block ----------------
		qkv norm: 1.0127359628677368 ms
		rope: 1.090559959411621 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.057983875274658 ms
		out projection: 0.4044800102710724 ms
	self-attn: 9.836544036865234 ms
	cross-attn: 1.5319039821624756 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 13.234175682067871 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0895359516143799 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.055935859680176 ms
		out projection: 0.4249599874019623 ms
	self-attn: 9.766912460327148 ms
	cross-attn: 1.4561280012130737 ms
	ffn: 1.5841280221939087 ms
forward previous block to here: 13.25772762298584 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.21299199759960175 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.193151950836182 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.70956802368164 ms
	cross-attn: 1.3987840414047241 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.922880172729492 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 1.0782719850540161 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.03545618057251 ms
		out projection: 0.3389439880847931 ms
	self-attn: 9.382911682128906 ms
	cross-attn: 1.3864959478378296 ms
	ffn: 1.4704639911651611 ms
forward last block: 12.712960243225098 ms
##### model time: 505.8467712402344 #####
	 Transformer denoising step 1/4 completed in 506.75 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10342399775981903 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.133055686950684 ms
	cross-attn: 1.1110399961471558 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.048383712768555 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.916288375854492 ms
---------------- attention block ----------------
		qkv norm: 0.8847360014915466 ms
		rope: 1.1253759860992432 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.959680080413818 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.521151542663574 ms
	cross-attn: 1.5534080266952515 ms
	ffn: 1.5820800065994263 ms
forward previous block to here: 13.041664123535156 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.290752410888672 ms
	cross-attn: 1.3158400058746338 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.453887939453125 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.2467840015888214 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.255616188049316 ms
		out projection: 0.35123199224472046 ms
	self-attn: 9.83244800567627 ms
	cross-attn: 1.3322240114212036 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 13.030400276184082 ms
---------------- attention block ----------------
		qkv norm: 0.9748479723930359 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.1955839991569519 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.3409920036792755 ms
	self-attn: 9.778176307678223 ms
	cross-attn: 1.3404159545898438 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.934144020080566 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.982207775115967 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.138175964355469 ms
	cross-attn: 1.206272006034851 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.230655670166016 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.981184005737305 ms
		out projection: 0.29388800263404846 ms
	self-attn: 9.241600036621094 ms
	cross-attn: 1.3178880214691162 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.388352394104004 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.2252800017595291 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.056960105895996 ms
		out projection: 0.2887679934501648 ms
	self-attn: 9.401344299316406 ms
	cross-attn: 1.558527946472168 ms
	ffn: 1.5472639799118042 ms
forward previous block to here: 12.899328231811523 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.1878399848937988 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.992447853088379 ms
		out projection: 0.3563520014286041 ms
	self-attn: 9.456640243530273 ms
	cross-attn: 1.3475840091705322 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.604415893554688 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.245696067810059 ms
	cross-attn: 1.321984052658081 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.36787223815918 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.271295547485352 ms
	cross-attn: 1.3281279802322388 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.435456275939941 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.275391578674316 ms
	cross-attn: 1.321984052658081 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.419072151184082 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.050816059112549 ms
		out projection: 0.33689600229263306 ms
	self-attn: 9.288703918457031 ms
	cross-attn: 1.3199360370635986 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 12.46003246307373 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.1884160041809082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.32044792175293 ms
	cross-attn: 1.3629440069198608 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.515328407287598 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.289728164672852 ms
	cross-attn: 1.388543963432312 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.517375946044922 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0833920240402222 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.2754560112953186 ms
	self-attn: 9.22316837310791 ms
	cross-attn: 1.2247040271759033 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.239871978759766 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.93612813949585 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.975359916687012 ms
	cross-attn: 1.1192320585250854 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.86303997039795 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.936448097229004 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.785216331481934 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.919039726257324 ms
	cross-attn: 1.1048959493637085 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.810815811157227 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.013248443603516 ms
	cross-attn: 1.1315200328826904 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 11.924480438232422 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.99071979522705 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.882495880126953 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.3409920036792755 ms
	self-attn: 9.18835163116455 ms
	cross-attn: 1.376255989074707 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.387328147888184 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.3409920036792755 ms
	self-attn: 9.318400382995605 ms
	cross-attn: 1.3434879779815674 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.480511665344238 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.283583641052246 ms
	cross-attn: 1.2892160415649414 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.372991561889648 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.300992012023926 ms
	cross-attn: 1.3332480192184448 ms
	ffn: 1.5247360467910767 ms
forward previous block to here: 12.515328407287598 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.34508800506591797 ms
	self-attn: 9.378815650939941 ms
	cross-attn: 1.4008320569992065 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.643327713012695 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.071296215057373 ms
		out projection: 0.3461120128631592 ms
	self-attn: 9.40236759185791 ms
	cross-attn: 1.4069759845733643 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.682239532470703 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.052864074707031 ms
		out projection: 0.3420160114765167 ms
	self-attn: 9.379839897155762 ms
	cross-attn: 1.4295040369033813 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.651519775390625 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.094655990600586 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.39417552947998 ms
	cross-attn: 1.376255989074707 ms
	ffn: 1.4786560535430908 ms
forward last block: 12.694527626037598 ms
##### model time: 376.11212158203125 #####
	 Transformer denoising step 2/4 completed in 376.63 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.966847896575928 ms
		out projection: 0.2693119943141937 ms
	self-attn: 9.083904266357422 ms
	cross-attn: 1.2165119647979736 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.077055931091309 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.964799880981445 ms
		out projection: 0.2805759906768799 ms
	self-attn: 9.085951805114746 ms
	cross-attn: 1.294335961341858 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.182527542114258 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.975039958953857 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.176063537597656 ms
	cross-attn: 1.2718080282211304 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.262399673461914 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.992447853088379 ms
		out projection: 0.35839998722076416 ms
	self-attn: 9.269248008728027 ms
	cross-attn: 1.32915198802948 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.392448425292969 ms
---------------- attention block ----------------
		qkv norm: 0.8796160221099854 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9996161460876465 ms
		out projection: 0.3051519989967346 ms
	self-attn: 9.20473575592041 ms
	cross-attn: 1.2830719947814941 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.291071891784668 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.268223762512207 ms
	cross-attn: 1.30457603931427 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.398591995239258 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.994495868682861 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.234432220458984 ms
	cross-attn: 1.321984052658081 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.383232116699219 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.278464317321777 ms
	cross-attn: 1.351680040359497 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.4651517868042 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9996161460876465 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.3670400381088257 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.452863693237305 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.165823936462402 ms
	cross-attn: 1.206272006034851 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.166144371032715 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.939199924468994 ms
		out projection: 0.27955201268196106 ms
	self-attn: 9.088000297546387 ms
	cross-attn: 1.2349439859390259 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.12723159790039 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.964799880981445 ms
		out projection: 0.2949120104312897 ms
	self-attn: 9.176063537597656 ms
	cross-attn: 1.3209600448608398 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 12.294143676757812 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.138175964355469 ms
	cross-attn: 1.2083200216293335 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.14361572265625 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.984255790710449 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.21190357208252 ms
	cross-attn: 1.3056000471115112 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.329983711242676 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.003712177276611 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.21497631072998 ms
	cross-attn: 1.257472038269043 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.26035213470459 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.949440002441406 ms
		out projection: 0.28467199206352234 ms
	self-attn: 9.103360176086426 ms
	cross-attn: 1.2165119647979736 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.113920211791992 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9699201583862305 ms
		out projection: 0.30822399258613586 ms
	self-attn: 9.158656120300293 ms
	cross-attn: 1.4120960235595703 ms
	ffn: 1.5063040256500244 ms
forward previous block to here: 12.445695877075195 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.983232021331787 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.134079933166504 ms
	cross-attn: 1.1929600238800049 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.138496398925781 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.225215911865234 ms
	cross-attn: 1.3096959590911865 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.36787223815918 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.228287696838379 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.389375686645508 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01804780960083 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.243647575378418 ms
	cross-attn: 1.294335961341858 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 12.363776206970215 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.334272027015686 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.427264213562012 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.227264404296875 ms
	cross-attn: 1.3772799968719482 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.4334077835083 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.3399679958820343 ms
	self-attn: 9.302016258239746 ms
	cross-attn: 1.3844480514526367 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.527615547180176 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.292799949645996 ms
	cross-attn: 1.3619199991226196 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.45798397064209 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.2805759906768799 ms
	self-attn: 9.200639724731445 ms
	cross-attn: 1.2840960025787354 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.271615982055664 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.980160236358643 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.081855773925781 ms
	cross-attn: 1.1253759860992432 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.000255584716797 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.29183998703956604 ms
	self-attn: 9.072640419006348 ms
	cross-attn: 1.2124160528182983 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.113920211791992 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.980160236358643 ms
		out projection: 0.29900801181793213 ms
	self-attn: 9.19654369354248 ms
	cross-attn: 1.282047986984253 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.286975860595703 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.234432220458984 ms
	cross-attn: 1.2912640571594238 ms
	ffn: 1.4551039934158325 ms
forward last block: 12.383232116699219 ms
##### model time: 372.70220947265625 #####
	 Transformer denoising step 3/4 completed in 373.22 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11468800157308578 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.225215911865234 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.362751960754395 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.002687931060791 ms
		out projection: 0.32870399951934814 ms
	self-attn: 9.199616432189941 ms
	cross-attn: 1.3209600448608398 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.336128234863281 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.025216102600098 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.254912376403809 ms
	cross-attn: 1.3025280237197876 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.388352394104004 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.242624282836914 ms
	cross-attn: 1.3445119857788086 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.408831596374512 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.994495868682861 ms
		out projection: 0.3123199939727783 ms
	self-attn: 9.219072341918945 ms
	cross-attn: 1.3527040481567383 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.390399932861328 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.979135990142822 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.10745620727539 ms
	cross-attn: 1.1479040384292603 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.039168357849121 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.953536033630371 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.039872169494629 ms
	cross-attn: 1.1315200328826904 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.952128410339355 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.945343971252441 ms
		out projection: 0.2836480140686035 ms
	self-attn: 9.101311683654785 ms
	cross-attn: 1.1950080394744873 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 12.085247993469238 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.965824127197266 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.156607627868652 ms
	cross-attn: 1.2912640571594238 ms
	ffn: 1.5206400156021118 ms
forward previous block to here: 12.335103988647461 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.07315194606781 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.398271560668945 ms
	cross-attn: 1.3649920225143433 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.590080261230469 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.2908160090446472 ms
	self-attn: 9.242624282836914 ms
	cross-attn: 1.260543942451477 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.344320297241211 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.002687931060791 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.292799949645996 ms
	cross-attn: 1.3547519445419312 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.481535911560059 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.3123199939727783 ms
	self-attn: 9.245696067810059 ms
	cross-attn: 1.3281279802322388 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 12.427264213562012 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.053887844085693 ms
		out projection: 0.33792001008987427 ms
	self-attn: 9.3951997756958 ms
	cross-attn: 1.4899200201034546 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.8471040725708 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 1.0741759538650513 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.441280364990234 ms
	cross-attn: 1.4264320135116577 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.724224090576172 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.250816345214844 ms
	cross-attn: 1.2093440294265747 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.265472412109375 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.963776111602783 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.179136276245117 ms
	cross-attn: 1.2677119970321655 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.254207611083984 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.30720001459121704 ms
	self-attn: 9.226240158081055 ms
	cross-attn: 1.2861440181732178 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.324864387512207 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01804780960083 ms
		out projection: 0.2754560112953186 ms
	self-attn: 9.251839637756348 ms
	cross-attn: 1.2892160415649414 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.386303901672363 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.030335903167725 ms
		out projection: 0.35225600004196167 ms
	self-attn: 9.380864143371582 ms
	cross-attn: 1.4223359823226929 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 12.667903900146484 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0967040061950684 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.083583831787109 ms
		out projection: 0.3553279936313629 ms
	self-attn: 9.494527816772461 ms
	cross-attn: 1.4264320135116577 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.77337646484375 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.165823936462402 ms
	cross-attn: 1.2544000148773193 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.239871978759766 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.987328052520752 ms
		out projection: 0.2764799892902374 ms
	self-attn: 9.147392272949219 ms
	cross-attn: 1.282047986984253 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.25216007232666 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0803200006484985 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.2826240062713623 ms
	self-attn: 9.2293119430542 ms
	cross-attn: 1.2687360048294067 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.311552047729492 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.279487609863281 ms
	cross-attn: 1.3475840091705322 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.462080001831055 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.32863998413086 ms
	cross-attn: 1.351680040359497 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.502016067504883 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.025216102600098 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.31123161315918 ms
	cross-attn: 1.3271039724349976 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.477439880371094 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.33587199449539185 ms
	self-attn: 9.315327644348145 ms
	cross-attn: 1.3557759523391724 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.505087852478027 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0762239694595337 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.045695781707764 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.380864143371582 ms
	cross-attn: 1.3987840414047241 ms
	ffn: 1.5011839866638184 ms
forward previous block to here: 12.633088111877441 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0293121337890625 ms
		out projection: 0.34508800506591797 ms
	self-attn: 9.343999862670898 ms
	cross-attn: 1.4643199443817139 ms
	ffn: 1.4807039499282837 ms
forward last block: 12.726271629333496 ms
##### model time: 376.4009094238281 #####
	 Transformer final denoising step 4/4 completed in 376.93 ms
‚ö° Block 7 denoising completed in 1635.69 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11059200018644333 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.056960105895996 ms
		out projection: 0.3409920036792755 ms
	self-attn: 9.371647834777832 ms
	cross-attn: 1.4295040369033813 ms
	ffn: 1.504256010055542 ms
forward previous block to here: 12.668928146362305 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0149760246276855 ms
		out projection: 0.2744320034980774 ms
	self-attn: 9.290752410888672 ms
	cross-attn: 1.3025280237197876 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.419072151184082 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.150464057922363 ms
	cross-attn: 1.1612160205841064 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.119039535522461 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9852800369262695 ms
		out projection: 0.3051519989967346 ms
	self-attn: 9.1975679397583 ms
	cross-attn: 1.2728320360183716 ms
	ffn: 1.4960639476776123 ms
forward previous block to here: 12.314623832702637 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.316351890563965 ms
	cross-attn: 1.341439962387085 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.47436809539795 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.352191925048828 ms
	cross-attn: 1.3598719835281372 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.547072410583496 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.054912090301514 ms
		out projection: 0.33792001008987427 ms
	self-attn: 9.375743865966797 ms
	cross-attn: 1.3527040481567383 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.57369613647461 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.045695781707764 ms
		out projection: 0.3235839903354645 ms
	self-attn: 9.33785629272461 ms
	cross-attn: 1.3332480192184448 ms
	ffn: 1.4960639476776123 ms
forward previous block to here: 12.528639793395996 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.046720027923584 ms
		out projection: 0.3338240087032318 ms
	self-attn: 9.339903831481934 ms
	cross-attn: 1.388543963432312 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.57472038269043 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.335807800292969 ms
	cross-attn: 1.3567999601364136 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.530688285827637 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.35225600004196167 ms
	self-attn: 9.348095893859863 ms
	cross-attn: 1.3752319812774658 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.538880348205566 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.189375877380371 ms
	cross-attn: 1.1704319715499878 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.15283203125 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.984255790710449 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.10540771484375 ms
	cross-attn: 1.1560959815979004 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.058624267578125 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.008831977844238 ms
		out projection: 0.28569599986076355 ms
	self-attn: 9.21395206451416 ms
	cross-attn: 1.230847954750061 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.297216415405273 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9996161460876465 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.261055946350098 ms
	cross-attn: 1.3598719835281372 ms
	ffn: 1.5063040256500244 ms
forward previous block to here: 12.489727973937988 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02726411819458 ms
		out projection: 0.3481599986553192 ms
	self-attn: 9.375743865966797 ms
	cross-attn: 1.4223359823226929 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.651519775390625 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.19148799777030945 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.06822395324707 ms
		out projection: 0.3532800078392029 ms
	self-attn: 9.469951629638672 ms
	cross-attn: 1.4612480401992798 ms
	ffn: 1.5052800178527832 ms
forward previous block to here: 12.811264038085938 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0854400396347046 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.052864074707031 ms
		out projection: 0.34508800506591797 ms
	self-attn: 9.480192184448242 ms
	cross-attn: 1.4704639911651611 ms
	ffn: 1.4981119632720947 ms
forward previous block to here: 12.817407608032227 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0844160318374634 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.040575981140137 ms
		out projection: 0.3563520014286041 ms
	self-attn: 9.4453763961792 ms
	cross-attn: 1.462272047996521 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.76416015625 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.35020801424980164 ms
	self-attn: 9.425919532775879 ms
	cross-attn: 1.4254080057144165 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.700672149658203 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.054912090301514 ms
		out projection: 0.29183998703956604 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.2902400493621826 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.383232116699219 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.989376068115234 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.145343780517578 ms
	cross-attn: 1.227776050567627 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.179455757141113 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.18432000279426575 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.984255790710449 ms
		out projection: 0.30617600679397583 ms
	self-attn: 9.290752410888672 ms
	cross-attn: 1.2933119535446167 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.412927627563477 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.2979840040206909 ms
	self-attn: 9.218048095703125 ms
	cross-attn: 1.3260799646377563 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.364800453186035 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.30303955078125 ms
	cross-attn: 1.3537280559539795 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.493824005126953 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0293121337890625 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.307135581970215 ms
	cross-attn: 1.307647943496704 ms
	ffn: 1.4858239889144897 ms
forward previous block to here: 12.44262409210205 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.958655834197998 ms
		out projection: 0.2734079957008362 ms
	self-attn: 9.139200210571289 ms
	cross-attn: 1.2503039836883545 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.182527542114258 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.979135990142822 ms
		out projection: 0.2959359884262085 ms
	self-attn: 9.186304092407227 ms
	cross-attn: 1.2984319925308228 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.297216415405273 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.003712177276611 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.228287696838379 ms
	cross-attn: 1.21343994140625 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.282879829406738 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.239551544189453 ms
	cross-attn: 1.3424639701843262 ms
	ffn: 1.465343952178955 ms
forward last block: 12.403712272644043 ms
##### model time: 377.4412841796875 #####
KV cache update for next block completed in 378.04 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 7 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 7 VAE decoding completed in 538.51 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 7 for sending...
‚úÖ Block 7 completed in 2594.39 ms (12 frames queued in 0.042 ms)
In loop: idx 7, current_num_frames 3, current_start_frame 21
üîÑ Processing block 8/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10956799983978271 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.1622400283813477 ms
		kv cache update (if evict): 0.6881279945373535 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.196224212646484 ms
		out projection: 4.545536041259766 ms
	self-attn: 25.806848526000977 ms
	cross-attn: 1.6138240098953247 ms
	ffn: 3.3320960998535156 ms
forward previous block to here: 31.287296295166016 ms
---------------- attention block ----------------
		qkv norm: 0.9400320053100586 ms
		rope: 8.773632049560547 ms
		kv cache update (if evict): 0.8191999793052673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.573056221008301 ms
		out projection: 7.762944221496582 ms
	self-attn: 30.282751083374023 ms
	cross-attn: 5.681151866912842 ms
	ffn: 1.5001599788665771 ms
forward previous block to here: 39.349246978759766 ms
---------------- attention block ----------------
		qkv norm: 0.9297919869422913 ms
		rope: 1.1233279705047607 ms
		kv cache update (if evict): 8.240127563476562 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.267647743225098 ms
		out projection: 0.39423999190330505 ms
	self-attn: 22.06105613708496 ms
	cross-attn: 9.043968200683594 ms
	ffn: 5.797887802124023 ms
forward previous block to here: 37.47635269165039 ms
---------------- attention block ----------------
		qkv norm: 0.9472000002861023 ms
		rope: 1.120255947113037 ms
		kv cache update (if evict): 0.6983680129051208 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.551615715026855 ms
		out projection: 0.3686400055885315 ms
	self-attn: 20.203519821166992 ms
	cross-attn: 3.9884800910949707 ms
	ffn: 1.5411200523376465 ms
forward previous block to here: 26.175487518310547 ms
---------------- attention block ----------------
		qkv norm: 1.1315200328826904 ms
		rope: 1.1182080507278442 ms
		kv cache update (if evict): 0.7167999744415283 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.1675519943237305 ms
		out projection: 0.5386239886283875 ms
	self-attn: 32.484352111816406 ms
	cross-attn: 4.591616153717041 ms
	ffn: 1.6117759943008423 ms
forward previous block to here: 39.29702377319336 ms
---------------- attention block ----------------
		qkv norm: 8.269824028015137 ms
		rope: 5.3585920333862305 ms
		kv cache update (if evict): 0.7147520184516907 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.48588752746582 ms
		out projection: 4.296703815460205 ms
	self-attn: 33.261566162109375 ms
	cross-attn: 1.5872000455856323 ms
	ffn: 3.2716801166534424 ms
forward previous block to here: 38.533119201660156 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0844160318374634 ms
		kv cache update (if evict): 0.6901760101318359 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0733442306518555 ms
		out projection: 0.35839998722076416 ms
	self-attn: 10.080256462097168 ms
	cross-attn: 1.4018559455871582 ms
	ffn: 1.511423945426941 ms
forward previous block to here: 13.433856010437012 ms
---------------- attention block ----------------
		qkv norm: 0.8755199909210205 ms
		rope: 1.6158720254898071 ms
		kv cache update (if evict): 0.6891520023345947 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.07539176940918 ms
		out projection: 0.3665919899940491 ms
	self-attn: 10.607616424560547 ms
	cross-attn: 1.4079999923706055 ms
	ffn: 1.5052800178527832 ms
forward previous block to here: 13.97657585144043 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0782719850540161 ms
		kv cache update (if evict): 0.7208960056304932 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.069248199462891 ms
		out projection: 0.3696640133857727 ms
	self-attn: 10.115072250366211 ms
	cross-attn: 1.7868800163269043 ms
	ffn: 1.7704960107803345 ms
forward previous block to here: 14.442496299743652 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.30105599761009216 ms
	self-attn: 9.785344123840332 ms
	cross-attn: 1.3864959478378296 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 13.014016151428223 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.6666240096092224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.005760192871094 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.830400466918945 ms
	cross-attn: 1.4428160190582275 ms
	ffn: 1.6373759508132935 ms
forward previous block to here: 13.566975593566895 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.1141120195388794 ms
		kv cache update (if evict): 0.7188479900360107 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.258687973022461 ms
		out projection: 0.3563520014286041 ms
	self-attn: 10.332159996032715 ms
	cross-attn: 1.481727957725525 ms
	ffn: 1.5083520412445068 ms
forward previous block to here: 13.767680168151855 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0792959928512573 ms
		kv cache update (if evict): 0.6830080151557922 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.11737585067749 ms
		out projection: 0.3624959886074066 ms
	self-attn: 10.109951972961426 ms
	cross-attn: 1.4335999488830566 ms
	ffn: 1.504256010055542 ms
forward previous block to here: 13.470720291137695 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.6881279945373535 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.057983875274658 ms
		out projection: 0.36556801199913025 ms
	self-attn: 10.02291202545166 ms
	cross-attn: 1.4213119745254517 ms
	ffn: 1.5032320022583008 ms
forward previous block to here: 13.38368034362793 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 1.0741759538650513 ms
		kv cache update (if evict): 0.6830080151557922 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.046720027923584 ms
		out projection: 0.3696640133857727 ms
	self-attn: 10.091520309448242 ms
	cross-attn: 1.439743995666504 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 13.460479736328125 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0680320262908936 ms
		kv cache update (if evict): 0.7167999744415283 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0590081214904785 ms
		out projection: 0.3563520014286041 ms
	self-attn: 10.016768455505371 ms
	cross-attn: 1.4264320135116577 ms
	ffn: 1.5144959688186646 ms
forward previous block to here: 13.38368034362793 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.129472017288208 ms
		kv cache update (if evict): 0.6952959895133972 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.045695781707764 ms
		out projection: 0.347135990858078 ms
	self-attn: 10.166272163391113 ms
	cross-attn: 1.3824000358581543 ms
	ffn: 1.499135971069336 ms
forward previous block to here: 13.44921588897705 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.082368016242981 ms
		kv cache update (if evict): 0.6850559711456299 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.062079906463623 ms
		out projection: 0.3850240111351013 ms
	self-attn: 10.10483169555664 ms
	cross-attn: 1.4561280012130737 ms
	ffn: 1.5329279899597168 ms
forward previous block to here: 13.517824172973633 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0833920240402222 ms
		kv cache update (if evict): 0.681984007358551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0497918128967285 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.962495803833008 ms
	cross-attn: 1.3824000358581543 ms
	ffn: 1.4960639476776123 ms
forward previous block to here: 13.278207778930664 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.6840320229530334 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.047743797302246 ms
		out projection: 0.2805759906768799 ms
	self-attn: 9.851903915405273 ms
	cross-attn: 1.2113920450210571 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.94540786743164 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.966847896575928 ms
		out projection: 0.28672000765800476 ms
	self-attn: 9.763839721679688 ms
	cross-attn: 1.2492799758911133 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.940287590026855 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.7290880084037781 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.039552211761475 ms
		out projection: 0.3399679958820343 ms
	self-attn: 9.951231956481934 ms
	cross-attn: 1.3905919790267944 ms
	ffn: 1.5063040256500244 ms
forward previous block to here: 13.243391990661621 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.0803200006484985 ms
		kv cache update (if evict): 0.6871039867401123 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.052864074707031 ms
		out projection: 0.34508800506591797 ms
	self-attn: 10.01471996307373 ms
	cross-attn: 1.3537280559539795 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 13.263872146606445 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.6993920207023621 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.033408164978027 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.993215560913086 ms
	cross-attn: 1.3731839656829834 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 13.212672233581543 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.6778879761695862 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.045695781707764 ms
		out projection: 0.35737600922584534 ms
	self-attn: 10.0065279006958 ms
	cross-attn: 1.415168046951294 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 13.26591968536377 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.6758400201797485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.042623996734619 ms
		out projection: 0.34508800506591797 ms
	self-attn: 9.89798355102539 ms
	cross-attn: 1.3486080169677734 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 13.099007606506348 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.6666240096092224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.033408164978027 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.846783638000488 ms
	cross-attn: 1.3455359935760498 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 13.039615631103516 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.048768043518066 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.846783638000488 ms
	cross-attn: 1.3363200426101685 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.995583534240723 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.694271981716156 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.671680450439453 ms
	cross-attn: 1.2206079959869385 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.693504333496094 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.953536033630371 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.556991577148438 ms
	cross-attn: 1.1632640361785889 ms
	ffn: 1.4581760168075562 ms
forward last block: 12.528639793395996 ms
##### model time: 536.0435180664062 #####
	 Transformer denoising step 1/4 completed in 536.58 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.239551544189453 ms
	cross-attn: 1.3946880102157593 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.453887939453125 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0813440084457397 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0293121337890625 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.31328010559082 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.55014419555664 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0680320262908936 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.025216102600098 ms
		out projection: 0.32972800731658936 ms
	self-attn: 9.357312202453613 ms
	cross-attn: 1.410048007965088 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.593152046203613 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.043647766113281 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.356287956237793 ms
	cross-attn: 1.3629440069198608 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.54911994934082 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.032383918762207 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.300992012023926 ms
	cross-attn: 1.3475840091705322 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.482560157775879 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0733442306518555 ms
		out projection: 0.3389439880847931 ms
	self-attn: 9.393152236938477 ms
	cross-attn: 1.4049279689788818 ms
	ffn: 1.4970879554748535 ms
forward previous block to here: 12.65766429901123 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0293121337890625 ms
		out projection: 0.32972800731658936 ms
	self-attn: 9.312255859375 ms
	cross-attn: 1.4632960557937622 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 12.634112358093262 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.090559959411621 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.275391578674316 ms
	cross-attn: 1.2380160093307495 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.316672325134277 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.1884160041809082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.050816059112549 ms
		out projection: 0.37785598635673523 ms
	self-attn: 9.539584159851074 ms
	cross-attn: 1.3905919790267944 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.742655754089355 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.054912090301514 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.340928077697754 ms
	cross-attn: 1.3547519445419312 ms
	ffn: 1.4970879554748535 ms
forward previous block to here: 12.579839706420898 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01804780960083 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.383935928344727 ms
	cross-attn: 1.3701119422912598 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.603391647338867 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0794878005981445 ms
		out projection: 0.3491840064525604 ms
	self-attn: 9.407487869262695 ms
	cross-attn: 1.3967360258102417 ms
	ffn: 1.509376049041748 ms
forward previous block to here: 12.675071716308594 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.056960105895996 ms
		out projection: 0.30105599761009216 ms
	self-attn: 9.335807800292969 ms
	cross-attn: 1.3015040159225464 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.454912185668945 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.975039958953857 ms
		out projection: 0.28569599986076355 ms
	self-attn: 9.143296241760254 ms
	cross-attn: 1.2933119535446167 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.25216007232666 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0751999616622925 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.983232021331787 ms
		out projection: 0.3092480003833771 ms
	self-attn: 9.266176223754883 ms
	cross-attn: 1.3557759523391724 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.44262409210205 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.277440071105957 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.401663780212402 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.001664161682129 ms
		out projection: 0.35942399501800537 ms
	self-attn: 9.358336448669434 ms
	cross-attn: 1.4315520524978638 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.622847557067871 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0864640474319458 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.045695781707764 ms
		out projection: 0.38809600472450256 ms
	self-attn: 9.640959739685059 ms
	cross-attn: 1.5175679922103882 ms
	ffn: 1.504256010055542 ms
forward previous block to here: 13.05190372467041 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0794878005981445 ms
		out projection: 0.38604798913002014 ms
	self-attn: 9.582592010498047 ms
	cross-attn: 1.5196160078048706 ms
	ffn: 1.511423945426941 ms
forward previous block to here: 12.981247901916504 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 1.0792959928512573 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.052864074707031 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.425919532775879 ms
	cross-attn: 1.4213119745254517 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.684288024902344 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.03545618057251 ms
		out projection: 0.33689600229263306 ms
	self-attn: 9.3306884765625 ms
	cross-attn: 1.3475840091705322 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.502016067504883 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.056960105895996 ms
		out projection: 0.3491840064525604 ms
	self-attn: 9.42796802520752 ms
	cross-attn: 1.4008320569992065 ms
	ffn: 1.5001599788665771 ms
forward previous block to here: 12.725248336791992 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.0751999616622925 ms
		kv cache update (if evict): 0.21299199759960175 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0794878005981445 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.498623847961426 ms
	cross-attn: 1.3649920225143433 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.692480087280273 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.050816059112549 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.226240158081055 ms
	cross-attn: 1.2441600561141968 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.291071891784668 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.163776397705078 ms
	cross-attn: 1.2318719625473022 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.215295791625977 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.966847896575928 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.095168113708496 ms
	cross-attn: 1.1560959815979004 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.090368270874023 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.35020801424980164 ms
	self-attn: 9.299967765808105 ms
	cross-attn: 1.3905919790267944 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.541952133178711 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.012928009033203 ms
		out projection: 0.37068799138069153 ms
	self-attn: 9.50271987915039 ms
	cross-attn: 1.6547839641571045 ms
	ffn: 1.539072036743164 ms
forward previous block to here: 13.111295700073242 ms
---------------- attention block ----------------
		qkv norm: 0.8939520120620728 ms
		rope: 1.0680320262908936 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.303743839263916 ms
		out projection: 0.3389439880847931 ms
	self-attn: 9.986047744750977 ms
	cross-attn: 1.3598719835281372 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 13.252608299255371 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0497918128967285 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.316351890563965 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.4714879989624023 ms
forward last block: 12.534784317016602 ms
##### model time: 381.6437683105469 #####
	 Transformer denoising step 2/4 completed in 382.16 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11776000261306763 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.347135990858078 ms
	self-attn: 9.30508804321289 ms
	cross-attn: 1.3998080492019653 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.54911994934082 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.3306884765625 ms
	cross-attn: 1.376255989074707 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.525568008422852 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.282560348510742 ms
	cross-attn: 1.4028799533843994 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.507136344909668 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0782719850540161 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.326592445373535 ms
	cross-attn: 1.363968014717102 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.529664039611816 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.30720001459121704 ms
	self-attn: 9.252863883972168 ms
	cross-attn: 1.2533760070800781 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.301312446594238 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.952511787414551 ms
		out projection: 0.2734079957008362 ms
	self-attn: 9.082880020141602 ms
	cross-attn: 1.1827199459075928 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.04531192779541 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.935103893280029 ms
		out projection: 0.2979840040206909 ms
	self-attn: 9.120767593383789 ms
	cross-attn: 1.307647943496704 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 12.256256103515625 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.002687931060791 ms
		out projection: 0.34406399726867676 ms
	self-attn: 9.340928077697754 ms
	cross-attn: 1.3783040046691895 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.563455581665039 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.308159828186035 ms
	cross-attn: 1.346560001373291 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.478464126586914 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.256959915161133 ms
	cross-attn: 1.3260799646377563 ms
	ffn: 1.499135971069336 ms
forward previous block to here: 12.436479568481445 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.18534399569034576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.34508800506591797 ms
	self-attn: 9.333760261535645 ms
	cross-attn: 1.3824000358581543 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 12.55628776550293 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.291775703430176 ms
	cross-attn: 1.393664002418518 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.524543762207031 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0813440084457397 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.043647766113281 ms
		out projection: 0.347135990858078 ms
	self-attn: 9.41875171661377 ms
	cross-attn: 1.405951976776123 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.668928146362305 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.30406379699707 ms
	cross-attn: 1.3742079734802246 ms
	ffn: 1.4960639476776123 ms
forward previous block to here: 12.515328407287598 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.247743606567383 ms
	cross-attn: 1.206272006034851 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.26956844329834 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.957632064819336 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.138175964355469 ms
	cross-attn: 1.2922879457473755 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.24294376373291 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.987328052520752 ms
		out projection: 0.30822399258613586 ms
	self-attn: 9.237504005432129 ms
	cross-attn: 1.2830719947814941 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.385279655456543 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.082368016242981 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0641279220581055 ms
		out projection: 0.2836480140686035 ms
	self-attn: 9.30406379699707 ms
	cross-attn: 1.2595200538635254 ms
	ffn: 1.5052800178527832 ms
forward previous block to here: 12.422143936157227 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.20479999482631683 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.040575981140137 ms
		out projection: 0.35942399501800537 ms
	self-attn: 9.42899227142334 ms
	cross-attn: 1.4141440391540527 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.679167747497559 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01907205581665 ms
		out projection: 0.30617600679397583 ms
	self-attn: 9.282560348510742 ms
	cross-attn: 1.3649920225143433 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.4651517868042 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.37068799138069153 ms
	self-attn: 9.377792358398438 ms
	cross-attn: 1.4325759410858154 ms
	ffn: 1.4858239889144897 ms
forward previous block to here: 12.64742374420166 ms
---------------- attention block ----------------
		qkv norm: 0.9021440148353577 ms
		rope: 1.0721280574798584 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.045695781707764 ms
		out projection: 0.3665919899940491 ms
	self-attn: 9.503744125366211 ms
	cross-attn: 1.4663679599761963 ms
	ffn: 1.5032320022583008 ms
forward previous block to here: 12.86246395111084 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0751999616622925 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.066175937652588 ms
		out projection: 0.3624959886074066 ms
	self-attn: 9.496576309204102 ms
	cross-attn: 1.4387199878692627 ms
	ffn: 1.5001599788665771 ms
forward previous block to here: 12.811264038085938 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0792959928512573 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.072319984436035 ms
		out projection: 0.33689600229263306 ms
	self-attn: 9.441280364990234 ms
	cross-attn: 1.427456021308899 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.706815719604492 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.030335903167725 ms
		out projection: 0.3235839903354645 ms
	self-attn: 9.291775703430176 ms
	cross-attn: 1.3875199556350708 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.507136344909668 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.087488055229187 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0344319343566895 ms
		out projection: 0.33689600229263306 ms
	self-attn: 9.346048355102539 ms
	cross-attn: 1.439743995666504 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.633088111877441 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.338879585266113 ms
	cross-attn: 1.3598719835281372 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.530688285827637 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02726411819458 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.327615737915039 ms
	cross-attn: 1.3209600448608398 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.547072410583496 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.03545618057251 ms
		out projection: 0.3543039858341217 ms
	self-attn: 9.400320053100586 ms
	cross-attn: 1.397760033607483 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 12.66380786895752 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.07315194606781 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.051839828491211 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.368576049804688 ms
	cross-attn: 1.410048007965088 ms
	ffn: 1.4940160512924194 ms
forward last block: 12.633088111877441 ms
##### model time: 379.36334228515625 #####
	 Transformer denoising step 3/4 completed in 379.91 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10649599879980087 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.184255599975586 ms
	cross-attn: 1.235967993736267 ms
	ffn: 1.4970879554748535 ms
forward previous block to here: 12.256256103515625 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.953536033630371 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.059328079223633 ms
	cross-attn: 1.1304960250854492 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 11.998208045959473 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.994495868682861 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.278464317321777 ms
	cross-attn: 1.369088053703308 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.490752220153809 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.000639915466309 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.289728164672852 ms
	cross-attn: 1.3537280559539795 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.464127540588379 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0680320262908936 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.323519706726074 ms
	cross-attn: 1.3424639701843262 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.495871543884277 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.264127731323242 ms
	cross-attn: 1.3557759523391724 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.428288459777832 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.005760192871094 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.283583641052246 ms
	cross-attn: 1.324031949043274 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.464127540588379 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.291775703430176 ms
	cross-attn: 1.334272027015686 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.46003246307373 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.32972800731658936 ms
	self-attn: 9.260031700134277 ms
	cross-attn: 1.3189120292663574 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.409855842590332 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02726411819458 ms
		out projection: 0.27238398790359497 ms
	self-attn: 9.236479759216309 ms
	cross-attn: 1.2052479982376099 ms
	ffn: 1.4858239889144897 ms
forward previous block to here: 12.266495704650879 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.957632064819336 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.12281608581543 ms
	cross-attn: 1.1929600238800049 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.156928062438965 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.944320201873779 ms
		out projection: 0.30617600679397583 ms
	self-attn: 9.129983901977539 ms
	cross-attn: 1.2922879457473755 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.24396800994873 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.96889591217041 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.185279846191406 ms
	cross-attn: 1.3301759958267212 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.324864387512207 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.991424083709717 ms
		out projection: 0.30720001459121704 ms
	self-attn: 9.250816345214844 ms
	cross-attn: 1.324031949043274 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.385279655456543 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.30105599761009216 ms
	self-attn: 9.241600036621094 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.388352394104004 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.284607887268066 ms
	cross-attn: 1.334272027015686 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.44159984588623 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.284607887268066 ms
	cross-attn: 1.3189120292663574 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.447744369506836 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.30303955078125 ms
	cross-attn: 1.3772799968719482 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.524543762207031 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.040575981140137 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.32966423034668 ms
	cross-attn: 1.4069759845733643 ms
	ffn: 1.5022079944610596 ms
forward previous block to here: 12.592127799987793 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.040575981140137 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.228287696838379 ms
	cross-attn: 1.1868159770965576 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.215295791625977 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.29388800263404846 ms
	self-attn: 9.09823989868164 ms
	cross-attn: 1.2922879457473755 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.193792343139648 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.970943927764893 ms
		out projection: 0.289792001247406 ms
	self-attn: 9.129983901977539 ms
	cross-attn: 1.2738560438156128 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.229632377624512 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9996161460876465 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.191424369812012 ms
	cross-attn: 1.252351999282837 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 12.240896224975586 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0680320262908936 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.244671821594238 ms
	cross-attn: 1.30457603931427 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.364800453186035 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.201663970947266 ms
	cross-attn: 1.2390400171279907 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.247039794921875 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.946368217468262 ms
		out projection: 0.30105599761009216 ms
	self-attn: 9.152511596679688 ms
	cross-attn: 1.248255968093872 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.193792343139648 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.970943927764893 ms
		out projection: 0.2908160090446472 ms
	self-attn: 9.153535842895508 ms
	cross-attn: 1.274880051612854 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.231679916381836 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.987328052520752 ms
		out projection: 0.27750399708747864 ms
	self-attn: 9.184255599975586 ms
	cross-attn: 1.2881920337677002 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.304384231567383 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.991424083709717 ms
		out projection: 0.34406399726867676 ms
	self-attn: 9.373696327209473 ms
	cross-attn: 1.358847975730896 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.567551612854004 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.279487609863281 ms
	cross-attn: 1.3373440504074097 ms
	ffn: 1.4786560535430908 ms
forward last block: 12.447744369506836 ms
##### model time: 374.3866882324219 #####
	 Transformer final denoising step 4/4 completed in 374.89 ms
‚ö° Block 8 denoising completed in 1675.53 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.1013759970664978 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.002687931060791 ms
		out projection: 0.30822399258613586 ms
	self-attn: 9.201663970947266 ms
	cross-attn: 1.3148159980773926 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.356608390808105 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.3506560325622559 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.45695972442627 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.040575981140137 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.341952323913574 ms
	cross-attn: 1.3096959590911865 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.491776466369629 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.288703918457031 ms
	cross-attn: 1.405951976776123 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.536831855773926 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0926079750061035 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.3491840064525604 ms
	self-attn: 9.415679931640625 ms
	cross-attn: 1.4602240324020386 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.697600364685059 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.3041279911994934 ms
	self-attn: 9.233407974243164 ms
	cross-attn: 1.2615679502487183 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.285951614379883 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02623987197876 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.203712463378906 ms
	cross-attn: 1.1735039949417114 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.142592430114746 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.956607818603516 ms
		out projection: 0.2734079957008362 ms
	self-attn: 9.102335929870605 ms
	cross-attn: 1.1724799871444702 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 12.05452823638916 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.982207775115967 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.194496154785156 ms
	cross-attn: 1.2902400493621826 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.312576293945312 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.986303806304932 ms
		out projection: 0.3041279911994934 ms
	self-attn: 9.201663970947266 ms
	cross-attn: 1.269760012626648 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.282879829406738 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.000639915466309 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.251839637756348 ms
	cross-attn: 1.2974079847335815 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.37606430053711 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.267200469970703 ms
	cross-attn: 1.307647943496704 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.400639533996582 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.983232021331787 ms
		out projection: 0.30617600679397583 ms
	self-attn: 9.20473575592041 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.35763168334961 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.002687931060791 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.215999603271484 ms
	cross-attn: 1.3281279802322388 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.362751960754395 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.008831977844238 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.309184074401855 ms
	cross-attn: 1.3527040481567383 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.472319602966309 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0149760246276855 ms
		out projection: 0.2836480140686035 ms
	self-attn: 9.184255599975586 ms
	cross-attn: 1.2042239904403687 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.191743850708008 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.945343971252441 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.027584075927734 ms
	cross-attn: 1.129472017288208 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.927552223205566 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.28569599986076355 ms
	self-attn: 9.042943954467773 ms
	cross-attn: 1.2247040271759033 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.085247993469238 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9555840492248535 ms
		out projection: 0.3461120128631592 ms
	self-attn: 9.187328338623047 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.293120384216309 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.29183998703956604 ms
	self-attn: 9.179136276245117 ms
	cross-attn: 1.2625919580459595 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.241920471191406 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.031167984008789 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9903998374938965 ms
		out projection: 0.30617600679397583 ms
	self-attn: 9.184255599975586 ms
	cross-attn: 1.2718080282211304 ms
	ffn: 1.4929920434951782 ms
forward previous block to here: 12.275712013244629 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97811222076416 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.200639724731445 ms
	cross-attn: 1.3015040159225464 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.310527801513672 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.276415824890137 ms
	cross-attn: 1.3363200426101685 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.435456275939941 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.072319984436035 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.326592445373535 ms
	cross-attn: 1.3537280559539795 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.521471977233887 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.17919999361038208 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.288703918457031 ms
	cross-attn: 1.3824000358581543 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.497920036315918 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.025216102600098 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.160703659057617 ms
	cross-attn: 1.2492799758911133 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.205056190490723 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.934080123901367 ms
		out projection: 0.30720001459121704 ms
	self-attn: 9.12281608581543 ms
	cross-attn: 1.346560001373291 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.284928321838379 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.081535816192627 ms
		out projection: 0.35225600004196167 ms
	self-attn: 9.423871994018555 ms
	cross-attn: 1.3998080492019653 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.725248336791992 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.102015972137451 ms
		out projection: 0.33792001008987427 ms
	self-attn: 9.455615997314453 ms
	cross-attn: 1.3957120180130005 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.737536430358887 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.271295547485352 ms
	cross-attn: 1.3424639701843262 ms
	ffn: 1.4663679599761963 ms
forward last block: 12.444671630859375 ms
##### model time: 374.6723937988281 #####
KV cache update for next block completed in 375.24 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 8 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 8 VAE decoding completed in 538.46 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 8 for sending...
‚úÖ Block 8 completed in 2628.14 ms (12 frames queued in 0.039 ms)
In loop: idx 8, current_num_frames 3, current_start_frame 24
üîÑ Processing block 9/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11264000087976456 ms
---------------- attention block ----------------
		qkv norm: 0.9082880020141602 ms
		rope: 1.0741759538650513 ms
		kv cache update (if evict): 0.6850559711456299 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 14.254079818725586 ms
		out projection: 0.31436800956726074 ms
	self-attn: 20.46054458618164 ms
	cross-attn: 3.777535915374756 ms
	ffn: 1.5308799743652344 ms
forward previous block to here: 26.12838363647461 ms
---------------- attention block ----------------
		qkv norm: 4.966400146484375 ms
		rope: 1.3619199991226196 ms
		kv cache update (if evict): 3.0013439655303955 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.179840087890625 ms
		out projection: 0.48025599122047424 ms
	self-attn: 32.163841247558594 ms
	cross-attn: 1.610751986503601 ms
	ffn: 3.3699839115142822 ms
forward previous block to here: 37.68217468261719 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.1253759860992432 ms
		kv cache update (if evict): 5.014527797698975 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.151167869567871 ms
		out projection: 0.3676159977912903 ms
	self-attn: 21.975040435791016 ms
	cross-attn: 13.207551956176758 ms
	ffn: 1.9660799503326416 ms
forward previous block to here: 39.80595016479492 ms
---------------- attention block ----------------
		qkv norm: 1.0024960041046143 ms
		rope: 1.1427839994430542 ms
		kv cache update (if evict): 8.134655952453613 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.457088470458984 ms
		out projection: 0.3911679983139038 ms
	self-attn: 22.29350471496582 ms
	cross-attn: 13.689855575561523 ms
	ffn: 1.5411200523376465 ms
forward previous block to here: 38.014976501464844 ms
---------------- attention block ----------------
		qkv norm: 0.8970239758491516 ms
		rope: 1.0936319828033447 ms
		kv cache update (if evict): 0.6758400201797485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.478912353515625 ms
		out projection: 0.4106239974498749 ms
	self-attn: 20.152320861816406 ms
	cross-attn: 4.434944152832031 ms
	ffn: 1.5022079944610596 ms
forward previous block to here: 26.50111961364746 ms
---------------- attention block ----------------
		qkv norm: 5.066751956939697 ms
		rope: 1.1448320150375366 ms
		kv cache update (if evict): 3.082240104675293 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.172671794891357 ms
		out projection: 4.853759765625 ms
	self-attn: 34.28249740600586 ms
	cross-attn: 1.5677440166473389 ms
	ffn: 3.210239887237549 ms
forward previous block to here: 39.46803283691406 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.050816059112549 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.879551887512207 ms
	cross-attn: 1.3670400381088257 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 13.0764799118042 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.5575040578842163 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.3491840064525604 ms
	self-attn: 10.480640411376953 ms
	cross-attn: 1.4039039611816406 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 13.722623825073242 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0864640474319458 ms
		kv cache update (if evict): 0.737280011177063 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.069248199462891 ms
		out projection: 0.35123199224472046 ms
	self-attn: 9.979904174804688 ms
	cross-attn: 1.427456021308899 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 13.254655838012695 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.6758400201797485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.667584419250488 ms
	cross-attn: 1.1806720495224 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.633088111877441 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.974016189575195 ms
		out projection: 0.3041279911994934 ms
	self-attn: 9.650176048278809 ms
	cross-attn: 1.7100800275802612 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 13.16966438293457 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.994495868682861 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.70751953125 ms
	cross-attn: 1.3086719512939453 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.83788776397705 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.6850559711456299 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.000639915466309 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.765888214111328 ms
	cross-attn: 1.3107199668884277 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.87987232208252 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.6707199811935425 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.784319877624512 ms
	cross-attn: 1.3537280559539795 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.939264297485352 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.953536033630371 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.565183639526367 ms
	cross-attn: 1.1571199893951416 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.511232376098633 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.959680080413818 ms
		out projection: 0.28467199206352234 ms
	self-attn: 9.688063621520996 ms
	cross-attn: 1.2410880327224731 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.768256187438965 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.987328052520752 ms
		out projection: 0.2836480140686035 ms
	self-attn: 9.683967590332031 ms
	cross-attn: 1.1950080394744873 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.677120208740234 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.6891520023345947 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.775103569030762 ms
	cross-attn: 1.3660160303115845 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.95359992980957 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.6799359917640686 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.794560432434082 ms
	cross-attn: 1.405951976776123 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 13.027327537536621 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.6696959733963013 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.821184158325195 ms
	cross-attn: 1.3281279802322388 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.997632026672363 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.6748160123825073 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.032383918762207 ms
		out projection: 0.3389439880847931 ms
	self-attn: 9.83244800567627 ms
	cross-attn: 1.3250559568405151 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.982272148132324 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02623987197876 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.810943603515625 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.977151870727539 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.34508800506591797 ms
	self-attn: 9.862144470214844 ms
	cross-attn: 1.405951976776123 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 13.097984313964844 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.32870399951934814 ms
	self-attn: 9.83347225189209 ms
	cross-attn: 1.3670400381088257 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 13.039615631103516 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.051839828491211 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.729023933410645 ms
	cross-attn: 1.2288000583648682 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.768256187438965 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.6686720252037048 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.980160236358643 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.629695892333984 ms
	cross-attn: 1.1079679727554321 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 12.511232376098633 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.945343971252441 ms
		out projection: 0.30822399258613586 ms
	self-attn: 9.613311767578125 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.75596809387207 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.994495868682861 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.797632217407227 ms
	cross-attn: 1.299456000328064 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.900351524353027 ms
---------------- attention block ----------------
		qkv norm: 0.886784017086029 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.6737920045852661 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.33587199449539185 ms
	self-attn: 9.890815734863281 ms
	cross-attn: 1.415168046951294 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 13.184000015258789 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.030335903167725 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.827327728271484 ms
	cross-attn: 1.3056000471115112 ms
	ffn: 1.4714879989624023 ms
forward last block: 12.955648422241211 ms
##### model time: 522.5462036132812 #####
	 Transformer denoising step 1/4 completed in 523.07 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10342399775981903 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.069248199462891 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.356287956237793 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.534784317016602 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01907205581665 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.285632133483887 ms
	cross-attn: 1.3824000358581543 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.499967575073242 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.32972800731658936 ms
	self-attn: 9.293824195861816 ms
	cross-attn: 1.358847975730896 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.464127540588379 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02726411819458 ms
		out projection: 0.27750399708747864 ms
	self-attn: 9.2293119430542 ms
	cross-attn: 1.2042239904403687 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.224512100219727 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.956607818603516 ms
		out projection: 0.28569599986076355 ms
	self-attn: 9.080831527709961 ms
	cross-attn: 1.1397119760513306 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 11.997183799743652 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9258880615234375 ms
		out projection: 0.3123199939727783 ms
	self-attn: 9.10643196105957 ms
	cross-attn: 1.3250559568405151 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.228608131408691 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.959680080413818 ms
		out projection: 0.3092480003833771 ms
	self-attn: 9.194496154785156 ms
	cross-attn: 1.2759040594100952 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.273664474487305 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.001664161682129 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.225215911865234 ms
	cross-attn: 1.2963839769363403 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.356608390808105 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.980160236358643 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.20678424835205 ms
	cross-attn: 1.307647943496704 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.331007957458496 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.003712177276611 ms
		out projection: 0.3389439880847931 ms
	self-attn: 9.259008407592773 ms
	cross-attn: 1.3557759523391724 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.462080001831055 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.03545618057251 ms
		out projection: 0.3020800054073334 ms
	self-attn: 9.262080192565918 ms
	cross-attn: 1.299456000328064 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 12.398591995239258 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.032383918762207 ms
		out projection: 0.3420160114765167 ms
	self-attn: 9.339903831481934 ms
	cross-attn: 1.3905919790267944 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.569600105285645 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.272319793701172 ms
	cross-attn: 1.4243839979171753 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.533760070800781 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02726411819458 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.207807540893555 ms
	cross-attn: 1.183743953704834 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.197888374328613 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.989376068115234 ms
		out projection: 0.3399679958820343 ms
	self-attn: 9.22009563446045 ms
	cross-attn: 1.3004800081253052 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.323840141296387 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9852800369262695 ms
		out projection: 0.29183998703956604 ms
	self-attn: 9.169919967651367 ms
	cross-attn: 1.282047986984253 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.254207611083984 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02623987197876 ms
		out projection: 0.2979840040206909 ms
	self-attn: 9.268223762512207 ms
	cross-attn: 1.2769279479980469 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.36684799194336 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.333760261535645 ms
	cross-attn: 1.3926399946212769 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.571647644042969 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.2959359884262085 ms
	self-attn: 9.248767852783203 ms
	cross-attn: 1.264639973640442 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.323840141296387 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.176063537597656 ms
	cross-attn: 1.2492799758911133 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.26035213470459 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.986303806304932 ms
		out projection: 0.27955201268196106 ms
	self-attn: 9.199616432189941 ms
	cross-attn: 1.252351999282837 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.284928321838379 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.2815999984741211 ms
	self-attn: 9.145343780517578 ms
	cross-attn: 1.2339199781417847 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.175359725952148 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.988351821899414 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.209856033325195 ms
	cross-attn: 1.3445119857788086 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.372991561889648 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0782719850540161 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.986303806304932 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.262080192565918 ms
	cross-attn: 1.3424639701843262 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.420096397399902 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.257984161376953 ms
	cross-attn: 1.3004800081253052 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.364800453186035 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.3235839903354645 ms
	self-attn: 9.30406379699707 ms
	cross-attn: 1.3096959590911865 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.417023658752441 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.005760192871094 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.272319793701172 ms
	cross-attn: 1.3025280237197876 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.394495964050293 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.3409920036792755 ms
	self-attn: 9.293824195861816 ms
	cross-attn: 1.3629440069198608 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.480511665344238 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.3363200426101685 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 12.510208129882812 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.26316800713539124 ms
	self-attn: 9.242624282836914 ms
	cross-attn: 1.218559980392456 ms
	ffn: 1.474560022354126 ms
forward last block: 12.282879829406738 ms
##### model time: 374.7901306152344 #####
	 Transformer denoising step 2/4 completed in 375.30 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10035199671983719 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.051136016845703 ms
	cross-attn: 1.129472017288208 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.960320472717285 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.988351821899414 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.143296241760254 ms
	cross-attn: 1.2840960025787354 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.238847732543945 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.3338240087032318 ms
	self-attn: 9.316351890563965 ms
	cross-attn: 1.346560001373291 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.494848251342773 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.315327644348145 ms
	cross-attn: 1.3772799968719482 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.522496223449707 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0833920240402222 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.000639915466309 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.282560348510742 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.45900821685791 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.35839998722076416 ms
	self-attn: 9.388031959533691 ms
	cross-attn: 1.3578239679336548 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.56447982788086 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.038527965545654 ms
		out projection: 0.3399679958820343 ms
	self-attn: 9.372672080993652 ms
	cross-attn: 1.3363200426101685 ms
	ffn: 1.499135971069336 ms
forward previous block to here: 12.596223831176758 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01907205581665 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.289728164672852 ms
	cross-attn: 1.3301759958267212 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.45900821685791 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.279487609863281 ms
	cross-attn: 1.3250559568405151 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.435456275939941 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.166848182678223 ms
	cross-attn: 1.1909120082855225 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.15078353881836 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.934080123901367 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.091072082519531 ms
	cross-attn: 1.146880030632019 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.05452823638916 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.96889591217041 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.234432220458984 ms
	cross-attn: 1.264639973640442 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.304384231567383 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.246720314025879 ms
	cross-attn: 1.3250559568405151 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.398591995239258 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.003712177276611 ms
		out projection: 0.30617600679397583 ms
	self-attn: 9.243647575378418 ms
	cross-attn: 1.3608959913253784 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.434432029724121 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.316351890563965 ms
	cross-attn: 1.3332480192184448 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.472319602966309 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.271295547485352 ms
	cross-attn: 1.3373440504074097 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.439552307128906 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.33689600229263306 ms
	self-attn: 9.315327644348145 ms
	cross-attn: 1.3629440069198608 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.533760070800781 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.30303955078125 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.4651517868042 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0887041091918945 ms
		out projection: 0.3338240087032318 ms
	self-attn: 9.388031959533691 ms
	cross-attn: 1.3752319812774658 ms
	ffn: 1.539072036743164 ms
forward previous block to here: 12.65459156036377 ms
---------------- attention block ----------------
		qkv norm: 0.9635840058326721 ms
		rope: 1.1909120082855225 ms
		kv cache update (if evict): 0.18636800348758698 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.12556791305542 ms
		out projection: 0.37785598635673523 ms
	self-attn: 10.018815994262695 ms
	cross-attn: 1.628159999847412 ms
	ffn: 1.5308799743652344 ms
forward previous block to here: 13.78816032409668 ms
---------------- attention block ----------------
		qkv norm: 0.9758719801902771 ms
		rope: 1.1315200328826904 ms
		kv cache update (if evict): 0.18636800348758698 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.141952037811279 ms
		out projection: 0.3665919899940491 ms
	self-attn: 9.888768196105957 ms
	cross-attn: 1.6424959897994995 ms
	ffn: 1.5278079509735107 ms
forward previous block to here: 13.485055923461914 ms
---------------- attention block ----------------
		qkv norm: 0.936959981918335 ms
		rope: 1.159168004989624 ms
		kv cache update (if evict): 0.1884160041809082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.154240131378174 ms
		out projection: 0.3338240087032318 ms
	self-attn: 9.866239547729492 ms
	cross-attn: 1.5400960445404053 ms
	ffn: 1.5196160078048706 ms
forward previous block to here: 13.345791816711426 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 1.1898880004882812 ms
		kv cache update (if evict): 0.19148799777030945 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.1327362060546875 ms
		out projection: 0.38809600472450256 ms
	self-attn: 9.885696411132812 ms
	cross-attn: 1.5841280221939087 ms
	ffn: 1.5155199766159058 ms
forward previous block to here: 13.38265609741211 ms
---------------- attention block ----------------
		qkv norm: 0.899071991443634 ms
		rope: 1.141759991645813 ms
		kv cache update (if evict): 0.1945600062608719 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.12659215927124 ms
		out projection: 0.4044800102710724 ms
	self-attn: 9.955327987670898 ms
	cross-attn: 1.4704639911651611 ms
	ffn: 1.4970879554748535 ms
forward previous block to here: 13.315072059631348 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.001664161682129 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.163776397705078 ms
	cross-attn: 1.1622400283813477 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.15180778503418 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.957632064819336 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.135104179382324 ms
	cross-attn: 1.2093440294265747 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.176383972167969 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.004735946655273 ms
		out projection: 0.30105599761009216 ms
	self-attn: 9.267200469970703 ms
	cross-attn: 1.3486080169677734 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.430335998535156 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.970943927764893 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.255935668945312 ms
	cross-attn: 1.3783040046691895 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.447744369506836 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.07315194606781 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.32972800731658936 ms
	self-attn: 9.297920227050781 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.489727973937988 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.005760192871094 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.31123161315918 ms
	cross-attn: 1.3271039724349976 ms
	ffn: 1.4612480401992798 ms
forward last block: 12.526592254638672 ms
##### model time: 381.0672607421875 #####
	 Transformer denoising step 3/4 completed in 381.60 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09625600278377533 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.000639915466309 ms
		out projection: 0.3338240087032318 ms
	self-attn: 9.227264404296875 ms
	cross-attn: 1.299456000328064 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.325887680053711 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0293121337890625 ms
		out projection: 0.32972800731658936 ms
	self-attn: 9.306112289428711 ms
	cross-attn: 1.3445119857788086 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.470272064208984 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.32044792175293 ms
	cross-attn: 1.3547519445419312 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.511232376098633 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9903998374938965 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.124863624572754 ms
	cross-attn: 1.1622400283813477 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 12.112895965576172 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3553279936313629 ms
	self-attn: 9.296895980834961 ms
	cross-attn: 1.3670400381088257 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.500991821289062 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0803200006484985 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.054912090301514 ms
		out projection: 0.33792001008987427 ms
	self-attn: 9.417728424072266 ms
	cross-attn: 1.369088053703308 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.600319862365723 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.972991943359375 ms
		out projection: 0.28467199206352234 ms
	self-attn: 9.142271995544434 ms
	cross-attn: 1.2984319925308228 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.25216007232666 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.972991943359375 ms
		out projection: 0.27955201268196106 ms
	self-attn: 9.185279846191406 ms
	cross-attn: 1.2206079959869385 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.185600280761719 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.944320201873779 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.01734447479248 ms
	cross-attn: 1.1335680484771729 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.928576469421387 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.945343971252441 ms
		out projection: 0.3102720081806183 ms
	self-attn: 9.150464057922363 ms
	cross-attn: 1.282047986984253 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 12.248064041137695 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.290752410888672 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.436479568481445 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.987328052520752 ms
		out projection: 0.30003198981285095 ms
	self-attn: 9.233407974243164 ms
	cross-attn: 1.2789759635925293 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.305407524108887 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.005760192871094 ms
		out projection: 0.29183998703956604 ms
	self-attn: 9.185279846191406 ms
	cross-attn: 1.2707840204238892 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.25216007232666 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.246720314025879 ms
	cross-attn: 1.2902400493621826 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.35968017578125 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9996161460876465 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.244671821594238 ms
	cross-attn: 1.2922879457473755 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.365823745727539 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.243647575378418 ms
	cross-attn: 1.3322240114212036 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 12.426239967346191 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.286656379699707 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.5144959688186646 ms
forward previous block to here: 12.485631942749023 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.012928009033203 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.262080192565918 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.390399932861328 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.953536033630371 ms
		out projection: 0.2744320034980774 ms
	self-attn: 9.118720054626465 ms
	cross-attn: 1.2462079524993896 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.164095878601074 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.01529598236084 ms
	cross-attn: 1.1735039949417114 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 11.981823921203613 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.2959359884262085 ms
	self-attn: 9.252863883972168 ms
	cross-attn: 1.2912640571594238 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.353535652160645 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.987328052520752 ms
		out projection: 0.35225600004196167 ms
	self-attn: 9.293824195861816 ms
	cross-attn: 1.3916159868240356 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.522496223449707 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0497918128967285 ms
		out projection: 0.36454400420188904 ms
	self-attn: 9.443327903747559 ms
	cross-attn: 1.418239951133728 ms
	ffn: 1.5083520412445068 ms
forward previous block to here: 12.787712097167969 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0803200006484985 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.3696640133857727 ms
	self-attn: 9.463808059692383 ms
	cross-attn: 1.4458880424499512 ms
	ffn: 1.4929920434951782 ms
forward previous block to here: 12.77132797241211 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.110208034515381 ms
		out projection: 0.3563520014286041 ms
	self-attn: 9.483263969421387 ms
	cross-attn: 1.457152009010315 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.793855667114258 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.288703918457031 ms
	cross-attn: 1.3895679712295532 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.506112098693848 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.090559959411621 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.3338240087032318 ms
	self-attn: 9.348095893859863 ms
	cross-attn: 1.405951976776123 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 12.617728233337402 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.022143840789795 ms
		out projection: 0.28569599986076355 ms
	self-attn: 9.246720314025879 ms
	cross-attn: 1.257472038269043 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.291071891784668 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.939199924468994 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.078783988952637 ms
	cross-attn: 1.2328959703445435 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.104703903198242 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3020800054073334 ms
	self-attn: 9.191424369812012 ms
	cross-attn: 1.2492799758911133 ms
	ffn: 1.4520319700241089 ms
forward last block: 12.294143676757812 ms
##### model time: 375.1403503417969 #####
	 Transformer final denoising step 4/4 completed in 375.66 ms
‚ö° Block 9 denoising completed in 1657.58 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.005760192871094 ms
		out projection: 0.26316800713539124 ms
	self-attn: 9.141247749328613 ms
	cross-attn: 1.2267520427703857 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.16102409362793 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.33792001008987427 ms
	self-attn: 9.315327644348145 ms
	cross-attn: 1.3445119857788086 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.455936431884766 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.041600227355957 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.416704177856445 ms
	cross-attn: 1.3107199668884277 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.623871803283691 ms
---------------- attention block ----------------
		qkv norm: 0.9574400186538696 ms
		rope: 1.1233279705047607 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3041279911994934 ms
	self-attn: 9.568256378173828 ms
	cross-attn: 1.277951955795288 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.697600364685059 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.041600227355957 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.293824195861816 ms
	cross-attn: 1.3608959913253784 ms
	ffn: 1.5206400156021118 ms
forward previous block to here: 12.53171157836914 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.3092480003833771 ms
	self-attn: 9.267200469970703 ms
	cross-attn: 1.368064045906067 ms
	ffn: 1.528831958770752 ms
forward previous block to here: 12.585984230041504 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.112064003944397 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.3461120128631592 ms
	self-attn: 9.587712287902832 ms
	cross-attn: 1.3803520202636719 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.868608474731445 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.347135990858078 ms
	self-attn: 9.390080451965332 ms
	cross-attn: 1.376255989074707 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.569600105285645 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.012928009033203 ms
		out projection: 0.3389439880847931 ms
	self-attn: 9.376768112182617 ms
	cross-attn: 1.363968014717102 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.572671890258789 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.3553279936313629 ms
	self-attn: 9.378815650939941 ms
	cross-attn: 1.798143982887268 ms
	ffn: 1.6005120277404785 ms
forward previous block to here: 13.214719772338867 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.08460807800293 ms
		out projection: 0.3481599986553192 ms
	self-attn: 9.51193618774414 ms
	cross-attn: 1.4632960557937622 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 12.836864471435547 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.054912090301514 ms
		out projection: 0.40140798687934875 ms
	self-attn: 9.552895545959473 ms
	cross-attn: 1.8001919984817505 ms
	ffn: 1.5605759620666504 ms
forward previous block to here: 13.409279823303223 ms
---------------- attention block ----------------
		qkv norm: 0.8960000276565552 ms
		rope: 1.1182080507278442 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.34303998947143555 ms
	self-attn: 9.683967590332031 ms
	cross-attn: 1.4704639911651611 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 13.049856185913086 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.1182080507278442 ms
		kv cache update (if evict): 0.2099200040102005 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.498623847961426 ms
	cross-attn: 1.3301759958267212 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.668928146362305 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.087488055229187 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.966847896575928 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.3926399946212769 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.503040313720703 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.038527965545654 ms
		out projection: 0.46489599347114563 ms
	self-attn: 9.705471992492676 ms
	cross-attn: 1.5779839754104614 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 13.124608039855957 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.1397119760513306 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.37273600697517395 ms
	self-attn: 9.621503829956055 ms
	cross-attn: 1.4725120067596436 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.982272148132324 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0887041091918945 ms
		out projection: 0.3563520014286041 ms
	self-attn: 9.532416343688965 ms
	cross-attn: 1.4684159755706787 ms
	ffn: 1.521664023399353 ms
forward previous block to here: 12.95257568359375 ms
---------------- attention block ----------------
		qkv norm: 0.9236479997634888 ms
		rope: 1.1499520540237427 ms
		kv cache update (if evict): 0.20070399343967438 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.025216102600098 ms
		out projection: 0.3409920036792755 ms
	self-attn: 9.706496238708496 ms
	cross-attn: 1.418239951133728 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.957695960998535 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.3604480028152466 ms
	self-attn: 9.400320053100586 ms
	cross-attn: 1.7889280319213867 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 13.002752304077148 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0926079750061035 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.140927791595459 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.542655944824219 ms
	cross-attn: 1.4264320135116577 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.818431854248047 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.070271968841553 ms
		out projection: 0.33689600229263306 ms
	self-attn: 9.390080451965332 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 12.57369613647461 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.19763199985027313 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.043647766113281 ms
		out projection: 0.30617600679397583 ms
	self-attn: 9.339903831481934 ms
	cross-attn: 1.3148159980773926 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.45695972442627 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.952511787414551 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.039872169494629 ms
	cross-attn: 1.1612160205841064 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.976703643798828 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.933055877685547 ms
		out projection: 0.2887679934501648 ms
	self-attn: 9.056256294250488 ms
	cross-attn: 1.282047986984253 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.156928062438965 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.949440002441406 ms
		out projection: 0.2959359884262085 ms
	self-attn: 9.133055686950684 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.24396800994873 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.96889591217041 ms
		out projection: 0.29286399483680725 ms
	self-attn: 9.169919967651367 ms
	cross-attn: 1.2851200103759766 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.2675199508667 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.992447853088379 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.255935668945312 ms
	cross-attn: 1.274880051612854 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.34943962097168 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.991424083709717 ms
		out projection: 0.3092480003833771 ms
	self-attn: 9.246720314025879 ms
	cross-attn: 1.3086719512939453 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.35865592956543 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.261055946350098 ms
	cross-attn: 1.3056000471115112 ms
	ffn: 1.4704639911651611 ms
forward last block: 12.387328147888184 ms
##### model time: 383.3825378417969 #####
KV cache update for next block completed in 383.93 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 9 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 9 VAE decoding completed in 538.36 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 9 for sending...
‚úÖ Block 9 completed in 2619.55 ms (12 frames queued in 0.039 ms)
In loop: idx 9, current_num_frames 3, current_start_frame 27
üîÑ Processing block 10/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11264000087976456 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 1.1059199571609497 ms
		kv cache update (if evict): 8.22374439239502 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.94041633605957 ms
		out projection: 0.3604480028152466 ms
	self-attn: 23.04102325439453 ms
	cross-attn: 9.461759567260742 ms
	ffn: 6.015999794006348 ms
forward previous block to here: 39.03692626953125 ms
---------------- attention block ----------------
		qkv norm: 3.443711996078491 ms
		rope: 1.1745280027389526 ms
		kv cache update (if evict): 0.6789119839668274 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.29747200012207 ms
		out projection: 0.3604480028152466 ms
	self-attn: 19.297279357910156 ms
	cross-attn: 1.6844799518585205 ms
	ffn: 3.501055955886841 ms
forward previous block to here: 25.10540771484375 ms
---------------- attention block ----------------
		qkv norm: 0.8796160221099854 ms
		rope: 1.1253759860992432 ms
		kv cache update (if evict): 5.179391860961914 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.180863857269287 ms
		out projection: 0.3614720106124878 ms
	self-attn: 22.659072875976562 ms
	cross-attn: 8.860671997070312 ms
	ffn: 1.598464012145996 ms
forward previous block to here: 37.568511962890625 ms
---------------- attention block ----------------
		qkv norm: 0.894976019859314 ms
		rope: 3.4109439849853516 ms
		kv cache update (if evict): 0.6881279945373535 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.1030402183532715 ms
		out projection: 0.35123199224472046 ms
	self-attn: 16.20377540588379 ms
	cross-attn: 5.568511962890625 ms
	ffn: 1.521664023399353 ms
forward previous block to here: 25.2620792388916 ms
---------------- attention block ----------------
		qkv norm: 0.9308159947395325 ms
		rope: 1.1376639604568481 ms
		kv cache update (if evict): 8.752127647399902 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.700799942016602 ms
		out projection: 0.41676801443099976 ms
	self-attn: 23.631872177124023 ms
	cross-attn: 9.867263793945312 ms
	ffn: 5.923840045928955 ms
forward previous block to here: 39.979007720947266 ms
---------------- attention block ----------------
		qkv norm: 3.4058239459991455 ms
		rope: 1.1397119760513306 ms
		kv cache update (if evict): 0.7362560033798218 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.129663944244385 ms
		out projection: 0.38604798913002014 ms
	self-attn: 15.328255653381348 ms
	cross-attn: 5.607423782348633 ms
	ffn: 1.4929920434951782 ms
forward previous block to here: 24.327167510986328 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 1.0926079750061035 ms
		kv cache update (if evict): 8.896512031555176 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.281984329223633 ms
		out projection: 0.37171199917793274 ms
	self-attn: 22.863872528076172 ms
	cross-attn: 1.4602240324020386 ms
	ffn: 1.491968035697937 ms
forward previous block to here: 26.183679580688477 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0926079750061035 ms
		kv cache update (if evict): 0.6686720252037048 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.045695781707764 ms
		out projection: 0.3102720081806183 ms
	self-attn: 9.890815734863281 ms
	cross-attn: 1.9240959882736206 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 13.639679908752441 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.6666240096092224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9699201583862305 ms
		out projection: 0.27852800488471985 ms
	self-attn: 9.668607711791992 ms
	cross-attn: 1.222656011581421 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.701696395874023 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.950463771820068 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.684991836547852 ms
	cross-attn: 1.3864959478378296 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.886015892028809 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.6748160123825073 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.981184005737305 ms
		out projection: 0.3235839903354645 ms
	self-attn: 9.767935752868652 ms
	cross-attn: 1.3629440069198608 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.955648422241211 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.6635519862174988 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.30822399258613586 ms
	self-attn: 9.759743690490723 ms
	cross-attn: 1.3506560325622559 ms
	ffn: 1.5001599788665771 ms
forward previous block to here: 12.947456359863281 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.6686720252037048 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.836544036865234 ms
	cross-attn: 1.3424639701843262 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 13.029376029968262 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.8427519798278809 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0590081214904785 ms
		out projection: 0.3399679958820343 ms
	self-attn: 10.24512004852295 ms
	cross-attn: 1.427456021308899 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 13.529088020324707 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0751999616622925 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.032383918762207 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.921536445617676 ms
	cross-attn: 1.4049279689788818 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 13.371392250061035 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.07315194606781 ms
		kv cache update (if evict): 0.6707199811935425 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.054912090301514 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.888768196105957 ms
	cross-attn: 1.3445119857788086 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 13.091839790344238 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.6840320229530334 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.065152168273926 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.874431610107422 ms
	cross-attn: 1.4131200313568115 ms
	ffn: 1.4929920434951782 ms
forward previous block to here: 13.126655578613281 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.963776111602783 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.629695892333984 ms
	cross-attn: 1.1438080072402954 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.567551612854004 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.6737920045852661 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.961728096008301 ms
		out projection: 0.30720001459121704 ms
	self-attn: 9.679871559143066 ms
	cross-attn: 1.358847975730896 ms
	ffn: 1.5011839866638184 ms
forward previous block to here: 12.889087677001953 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0149760246276855 ms
		out projection: 0.2949120104312897 ms
	self-attn: 9.759743690490723 ms
	cross-attn: 1.3230079412460327 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.918784141540527 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9996161460876465 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.762816429138184 ms
	cross-attn: 1.321984052658081 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.897279739379883 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.6707199811935425 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.829376220703125 ms
	cross-attn: 1.3946880102157593 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 13.06726360321045 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.950463771820068 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.679871559143066 ms
	cross-attn: 1.311743974685669 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.791808128356934 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.981184005737305 ms
		out projection: 0.3041279911994934 ms
	self-attn: 9.72390365600586 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.887040138244629 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.6696959733963013 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.025216102600098 ms
		out projection: 0.2744320034980774 ms
	self-attn: 9.73516845703125 ms
	cross-attn: 1.260543942451477 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.832768440246582 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.6676480174064636 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.796607971191406 ms
	cross-attn: 1.3905919790267944 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 13.019136428833008 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02623987197876 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.837568283081055 ms
	cross-attn: 1.3660160303115845 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 13.020159721374512 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.038527965545654 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.870335578918457 ms
	cross-attn: 1.3670400381088257 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 13.06828784942627 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.6881279945373535 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.3420160114765167 ms
	self-attn: 9.837568283081055 ms
	cross-attn: 1.385472059249878 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 13.054976463317871 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.6686720252037048 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.792511940002441 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.4725120067596436 ms
forward last block: 13.000703811645508 ms
##### model time: 523.7176513671875 #####
	 Transformer denoising step 1/4 completed in 524.29 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11776000261306763 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.047743797302246 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.3951997756958 ms
	cross-attn: 1.4069759845733643 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.661760330200195 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.2805759906768799 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.2902400493621826 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.35865592956543 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.043647766113281 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.203712463378906 ms
	cross-attn: 1.1816960573196411 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.175359725952148 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.951488018035889 ms
		out projection: 0.28467199206352234 ms
	self-attn: 9.1975679397583 ms
	cross-attn: 1.21343994140625 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.23475170135498 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.983232021331787 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.209856033325195 ms
	cross-attn: 1.3271039724349976 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 12.384256362915039 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.281536102294922 ms
	cross-attn: 1.32915198802948 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.4334077835083 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.044672012329102 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.286656379699707 ms
	cross-attn: 1.3373440504074097 ms
	ffn: 1.4970879554748535 ms
forward previous block to here: 12.47539234161377 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.294848442077637 ms
	cross-attn: 1.3670400381088257 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.496895790100098 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.281536102294922 ms
	cross-attn: 1.3455359935760498 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.44979190826416 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.022143840789795 ms
		out projection: 0.3102720081806183 ms
	self-attn: 9.31430435180664 ms
	cross-attn: 1.3137919902801514 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.436479568481445 ms
---------------- attention block ----------------
		qkv norm: 0.881663978099823 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.32147216796875 ms
	cross-attn: 1.3363200426101685 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.4835844039917 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.209856033325195 ms
	cross-attn: 1.1560959815979004 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 12.158975601196289 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.09721565246582 ms
	cross-attn: 1.1448320150375366 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.078080177307129 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.961728096008301 ms
		out projection: 0.30003198981285095 ms
	self-attn: 9.143296241760254 ms
	cross-attn: 1.2421120405197144 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.217344284057617 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.970943927764893 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.248767852783203 ms
	cross-attn: 1.3363200426101685 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.413951873779297 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.257984161376953 ms
	cross-attn: 1.2953599691390991 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.396544456481934 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.3020800054073334 ms
	self-attn: 9.242624282836914 ms
	cross-attn: 1.2892160415649414 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.377087593078613 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.33587199449539185 ms
	self-attn: 9.357312202453613 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.56447982788086 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0803200006484985 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.002687931060791 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.316351890563965 ms
	cross-attn: 1.3864959478378296 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.545023918151855 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0344319343566895 ms
		out projection: 0.3686400055885315 ms
	self-attn: 9.41875171661377 ms
	cross-attn: 1.3752319812774658 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.637184143066406 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.042623996734619 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.362431526184082 ms
	cross-attn: 1.3660160303115845 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 12.579839706420898 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.976064205169678 ms
		out projection: 0.2836480140686035 ms
	self-attn: 9.252863883972168 ms
	cross-attn: 1.388543963432312 ms
	ffn: 1.504256010055542 ms
forward previous block to here: 12.55731201171875 ms
---------------- attention block ----------------
		qkv norm: 0.8857600092887878 ms
		rope: 1.1212799549102783 ms
		kv cache update (if evict): 0.19763199985027313 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.099967956542969 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.62764835357666 ms
	cross-attn: 1.5953919887542725 ms
	ffn: 1.5349760055541992 ms
forward previous block to here: 13.16761589050293 ms
---------------- attention block ----------------
		qkv norm: 0.8908799886703491 ms
		rope: 1.1438080072402954 ms
		kv cache update (if evict): 0.19046400487422943 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.098944187164307 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.612288475036621 ms
	cross-attn: 1.5370240211486816 ms
	ffn: 1.5923199653625488 ms
forward previous block to here: 13.148159980773926 ms
---------------- attention block ----------------
		qkv norm: 0.9195520281791687 ms
		rope: 1.1110399961471558 ms
		kv cache update (if evict): 0.1884160041809082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.099967956542969 ms
		out projection: 0.374783992767334 ms
	self-attn: 9.753600120544434 ms
	cross-attn: 1.5800319910049438 ms
	ffn: 1.5206400156021118 ms
forward previous block to here: 13.263872146606445 ms
---------------- attention block ----------------
		qkv norm: 0.881663978099823 ms
		rope: 1.1233279705047607 ms
		kv cache update (if evict): 0.1884160041809082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.1675519943237305 ms
		out projection: 0.3338240087032318 ms
	self-attn: 9.857024192810059 ms
	cross-attn: 1.3629440069198608 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 13.05395221710205 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.965824127197266 ms
		out projection: 0.27750399708747864 ms
	self-attn: 9.104384422302246 ms
	cross-attn: 1.2584960460662842 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.15078353881836 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.28672000765800476 ms
	self-attn: 9.11359977722168 ms
	cross-attn: 1.2759040594100952 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.199935913085938 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97811222076416 ms
		out projection: 0.27136000990867615 ms
	self-attn: 9.142271995544434 ms
	cross-attn: 1.257472038269043 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.197888374328613 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.272319793701172 ms
	cross-attn: 1.3486080169677734 ms
	ffn: 1.4632960557937622 ms
forward last block: 12.430335998535156 ms
##### model time: 378.7888488769531 #####
	 Transformer denoising step 2/4 completed in 379.33 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09728000313043594 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.289728164672852 ms
	cross-attn: 1.3844480514526367 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.492799758911133 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01804780960083 ms
		out projection: 0.35839998722076416 ms
	self-attn: 9.325568199157715 ms
	cross-attn: 1.368064045906067 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.499967575073242 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01907205581665 ms
		out projection: 0.347135990858078 ms
	self-attn: 9.307135581970215 ms
	cross-attn: 1.3547519445419312 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.487680435180664 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.008831977844238 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.290752410888672 ms
	cross-attn: 1.3547519445419312 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.477439880371094 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.030335903167725 ms
		out projection: 0.34406399726867676 ms
	self-attn: 9.32249641418457 ms
	cross-attn: 1.3998080492019653 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.570624351501465 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0590081214904785 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.338879585266113 ms
	cross-attn: 1.3547519445419312 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.536831855773926 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.157631874084473 ms
	cross-attn: 1.2410880327224731 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.200960159301758 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.948416233062744 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.988672256469727 ms
	cross-attn: 1.1325440406799316 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 11.902976036071777 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.942272186279297 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.070591926574707 ms
	cross-attn: 1.2707840204238892 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.16204833984375 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.980160236358643 ms
		out projection: 0.3184640109539032 ms
	self-attn: 9.215999603271484 ms
	cross-attn: 1.3619199991226196 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.409855842590332 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.228287696838379 ms
	cross-attn: 1.307647943496704 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.372991561889648 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9852800369262695 ms
		out projection: 0.35020801424980164 ms
	self-attn: 9.262080192565918 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.389375686645508 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.276415824890137 ms
	cross-attn: 1.2963839769363403 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.397567749023438 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.3092480003833771 ms
	self-attn: 9.245696067810059 ms
	cross-attn: 1.3025280237197876 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.36684799194336 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.280511856079102 ms
	cross-attn: 1.3260799646377563 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.414976119995117 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.27955201268196106 ms
	self-attn: 9.157631874084473 ms
	cross-attn: 1.206272006034851 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.167167663574219 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.950463771820068 ms
		out projection: 0.2754560112953186 ms
	self-attn: 9.078783988952637 ms
	cross-attn: 1.1704319715499878 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.033023834228516 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.933055877685547 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.035776138305664 ms
	cross-attn: 1.1991039514541626 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.043264389038086 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.965824127197266 ms
		out projection: 0.30003198981285095 ms
	self-attn: 9.143296241760254 ms
	cross-attn: 1.2922879457473755 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.26137638092041 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.983232021331787 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.209856033325195 ms
	cross-attn: 1.287168025970459 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.311552047729492 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.988351821899414 ms
		out projection: 0.2959359884262085 ms
	self-attn: 9.21292781829834 ms
	cross-attn: 1.2974079847335815 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.328960418701172 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.31328010559082 ms
	cross-attn: 1.3424639701843262 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.473343849182129 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.226240158081055 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.348416328430176 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.294848442077637 ms
	cross-attn: 1.3393919467926025 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.45081615447998 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.306112289428711 ms
	cross-attn: 1.3793280124664307 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.552191734313965 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.20473575592041 ms
	cross-attn: 1.201151967048645 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.191743850708008 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.953536033630371 ms
		out projection: 0.29286399483680725 ms
	self-attn: 9.082880020141602 ms
	cross-attn: 1.2441600561141968 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.130304336547852 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.19865599274635315 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.317376136779785 ms
	cross-attn: 1.32915198802948 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.487680435180664 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.989376068115234 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.176063537597656 ms
	cross-attn: 1.21343994140625 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.220416069030762 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.224191665649414 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.4735360145568848 ms
forward last block: 12.401663780212402 ms
##### model time: 373.9648132324219 #####
	 Transformer denoising step 3/4 completed in 374.48 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10444799810647964 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.12384033203125 ms
	cross-attn: 1.1550719738006592 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.085247993469238 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.036479949951172 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.135104179382324 ms
	cross-attn: 1.1929600238800049 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 12.142592430114746 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.988351821899414 ms
		out projection: 0.29900801181793213 ms
	self-attn: 9.139200210571289 ms
	cross-attn: 1.294335961341858 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.246015548706055 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.966847896575928 ms
		out projection: 0.3102720081806183 ms
	self-attn: 9.192447662353516 ms
	cross-attn: 1.3312000036239624 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.338175773620605 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.298944473266602 ms
	cross-attn: 1.351680040359497 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.464127540588379 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.005760192871094 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.190400123596191 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.339200019836426 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.003712177276611 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.252863883972168 ms
	cross-attn: 1.3189120292663574 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.381183624267578 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.3127679824829102 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.402688026428223 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9996161460876465 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.257984161376953 ms
	cross-attn: 1.3475840091705322 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.44057559967041 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.34508800506591797 ms
	self-attn: 9.308159828186035 ms
	cross-attn: 1.3875199556350708 ms
	ffn: 1.4858239889144897 ms
forward previous block to here: 12.529664039611816 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.028287887573242 ms
		out projection: 0.3020800054073334 ms
	self-attn: 9.280511856079102 ms
	cross-attn: 1.3148159980773926 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.424192428588867 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.158656120300293 ms
	cross-attn: 1.2298239469528198 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.198911666870117 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.944320201873779 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.9999361038208 ms
	cross-attn: 1.1448320150375366 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 11.94598388671875 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.952511787414551 ms
		out projection: 0.30822399258613586 ms
	self-attn: 9.110527992248535 ms
	cross-attn: 1.3199360370635986 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.248064041137695 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.247743606567383 ms
	cross-attn: 1.3158400058746338 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.388352394104004 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.000639915466309 ms
		out projection: 0.31436800956726074 ms
	self-attn: 9.252863883972168 ms
	cross-attn: 1.3004800081253052 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.377087593078613 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.992447853088379 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.268223762512207 ms
	cross-attn: 1.3230079412460327 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.393471717834473 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.008831977844238 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.249792098999023 ms
	cross-attn: 1.294335961341858 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.353535652160645 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3123199939727783 ms
	self-attn: 9.232383728027344 ms
	cross-attn: 1.2963839769363403 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.345343589782715 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.3092480003833771 ms
	self-attn: 9.398271560668945 ms
	cross-attn: 1.760256052017212 ms
	ffn: 1.5718400478363037 ms
forward previous block to here: 13.190143585205078 ms
---------------- attention block ----------------
		qkv norm: 0.9021440148353577 ms
		rope: 1.1335680484771729 ms
		kv cache update (if evict): 0.2027519941329956 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.110208034515381 ms
		out projection: 0.28569599986076355 ms
	self-attn: 9.650176048278809 ms
	cross-attn: 1.4305280447006226 ms
	ffn: 1.5032320022583008 ms
forward previous block to here: 12.984319686889648 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.1499520540237427 ms
		kv cache update (if evict): 0.17919999361038208 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0590081214904785 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.492480278015137 ms
	cross-attn: 1.3312000036239624 ms
	ffn: 1.4960639476776123 ms
forward previous block to here: 12.7457275390625 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0854400396347046 ms
		kv cache update (if evict): 0.18432000279426575 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.038527965545654 ms
		out projection: 0.3399679958820343 ms
	self-attn: 9.564160346984863 ms
	cross-attn: 1.943552017211914 ms
	ffn: 1.5237120389938354 ms
forward previous block to here: 13.446144104003906 ms
---------------- attention block ----------------
		qkv norm: 0.8898559808731079 ms
		rope: 1.1263999938964844 ms
		kv cache update (if evict): 0.18636800348758698 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.116352081298828 ms
		out projection: 0.3481599986553192 ms
	self-attn: 9.73311996459961 ms
	cross-attn: 1.5431679487228394 ms
	ffn: 1.5206400156021118 ms
forward previous block to here: 13.203455924987793 ms
---------------- attention block ----------------
		qkv norm: 0.8755199909210205 ms
		rope: 1.1397119760513306 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.086656093597412 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.671680450439453 ms
	cross-attn: 1.3772799968719482 ms
	ffn: 1.4858239889144897 ms
forward previous block to here: 12.900351524353027 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.009856224060059 ms
		out projection: 0.3256320059299469 ms
	self-attn: 9.31328010559082 ms
	cross-attn: 1.3608959913253784 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.515328407287598 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.011903762817383 ms
		out projection: 0.32153600454330444 ms
	self-attn: 9.32147216796875 ms
	cross-attn: 1.3608959913253784 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.514304161071777 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.3420160114765167 ms
	self-attn: 9.32249641418457 ms
	cross-attn: 1.3670400381088257 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 12.538880348205566 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0782719850540161 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.066175937652588 ms
		out projection: 0.3164159953594208 ms
	self-attn: 9.351167678833008 ms
	cross-attn: 1.3742079734802246 ms
	ffn: 1.5185920000076294 ms
forward previous block to here: 12.627967834472656 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.06822395324707 ms
		out projection: 0.3491840064525604 ms
	self-attn: 9.423871994018555 ms
	cross-attn: 1.509376049041748 ms
	ffn: 1.4878720045089722 ms
forward last block: 12.778495788574219 ms
##### model time: 379.4595947265625 #####
	 Transformer final denoising step 4/4 completed in 380.00 ms
‚ö° Block 10 denoising completed in 1660.08 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10547199845314026 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.32870399951934814 ms
	self-attn: 9.273344039916992 ms
	cross-attn: 1.4069759845733643 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 12.525568008422852 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02726411819458 ms
		out projection: 0.34406399726867676 ms
	self-attn: 9.3951997756958 ms
	cross-attn: 1.4663679599761963 ms
	ffn: 1.4888960123062134 ms
forward previous block to here: 12.707839965820312 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0741759538650513 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.2877439856529236 ms
	self-attn: 9.254912376403809 ms
	cross-attn: 1.2564480304718018 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.316672325134277 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.003712177276611 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.256959915161133 ms
	cross-attn: 1.3424639701843262 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.411904335021973 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.2836480140686035 ms
	self-attn: 9.271295547485352 ms
	cross-attn: 1.2584960460662842 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.345343589782715 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.943295955657959 ms
		out projection: 0.2979840040206909 ms
	self-attn: 9.139200210571289 ms
	cross-attn: 1.2595200538635254 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.211199760437012 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.996543884277344 ms
		out projection: 0.2815999984741211 ms
	self-attn: 9.158656120300293 ms
	cross-attn: 1.2677119970321655 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.257280349731445 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.979135990142822 ms
		out projection: 0.2693119943141937 ms
	self-attn: 9.11564826965332 ms
	cross-attn: 1.2195839881896973 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.156928062438965 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.3328000009059906 ms
	self-attn: 9.280511856079102 ms
	cross-attn: 1.3905919790267944 ms
	ffn: 1.5083520412445068 ms
forward previous block to here: 12.534784317016602 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01804780960083 ms
		out projection: 0.32256001234054565 ms
	self-attn: 9.269248008728027 ms
	cross-attn: 1.358847975730896 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.468223571777344 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01907205581665 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.369600296020508 ms
	cross-attn: 1.4387199878692627 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 12.661760330200195 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.036479949951172 ms
		out projection: 0.35839998722076416 ms
	self-attn: 9.385984420776367 ms
	cross-attn: 1.3731839656829834 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.584959983825684 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.030335903167725 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.30508804321289 ms
	cross-attn: 1.341439962387085 ms
	ffn: 1.5104000568389893 ms
forward previous block to here: 12.519424438476562 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01804780960083 ms
		out projection: 0.33792001008987427 ms
	self-attn: 9.362431526184082 ms
	cross-attn: 1.4008320569992065 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.614656448364258 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.053887844085693 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.361408233642578 ms
	cross-attn: 1.4161920547485352 ms
	ffn: 1.516543984413147 ms
forward previous block to here: 12.653568267822266 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0344319343566895 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.251839637756348 ms
	cross-attn: 1.2431360483169556 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.301312446594238 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.031167984008789 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.984255790710449 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.075712203979492 ms
	cross-attn: 1.1622400283813477 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.02892780303955 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.967872142791748 ms
		out projection: 0.3112959861755371 ms
	self-attn: 9.170944213867188 ms
	cross-attn: 1.4315520524978638 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.427264213562012 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0854400396347046 ms
		kv cache update (if evict): 0.19353599846363068 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.072319984436035 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.505791664123535 ms
	cross-attn: 1.324031949043274 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.690431594848633 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.3317759931087494 ms
	self-attn: 9.3306884765625 ms
	cross-attn: 1.369088053703308 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.527615547180176 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.373696327209473 ms
	cross-attn: 1.351680040359497 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.580863952636719 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.023168087005615 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.359359741210938 ms
	cross-attn: 1.3567999601364136 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.559359550476074 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.069248199462891 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.397248268127441 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.516543984413147 ms
forward previous block to here: 12.64025592803955 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0293121337890625 ms
		out projection: 0.32870399951934814 ms
	self-attn: 9.30508804321289 ms
	cross-attn: 1.3434879779815674 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.479488372802734 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.30105599761009216 ms
	self-attn: 9.249792098999023 ms
	cross-attn: 1.3107199668884277 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.370944023132324 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.947391986846924 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.077759742736816 ms
	cross-attn: 1.1816960573196411 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.057600021362305 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.947391986846924 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.124863624572754 ms
	cross-attn: 1.2769279479980469 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.225536346435547 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9607038497924805 ms
		out projection: 0.32051199674606323 ms
	self-attn: 9.2293119430542 ms
	cross-attn: 1.3004800081253052 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.344320297241211 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.986303806304932 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.240575790405273 ms
	cross-attn: 1.3066240549087524 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.354559898376465 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.007808208465576 ms
		out projection: 0.319487988948822 ms
	self-attn: 9.316351890563965 ms
	cross-attn: 1.321984052658081 ms
	ffn: 1.474560022354126 ms
forward last block: 12.54092788696289 ms
##### model time: 377.0931091308594 #####
KV cache update for next block completed in 377.66 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 10 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 10 VAE decoding completed in 538.50 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 10 for sending...
‚úÖ Block 10 completed in 2617.53 ms (12 frames queued in 0.041 ms)
In loop: idx 10, current_num_frames 3, current_start_frame 30
üîÑ Processing block 11/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.12492799758911133 ms
---------------- attention block ----------------
		qkv norm: 0.8929280042648315 ms
		rope: 1.0833920240402222 ms
		kv cache update (if evict): 5.278719902038574 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.178815841674805 ms
		out projection: 0.36454400420188904 ms
	self-attn: 23.193599700927734 ms
	cross-attn: 9.217023849487305 ms
	ffn: 5.964799880981445 ms
forward previous block to here: 38.85465621948242 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 1.1274240016937256 ms
		kv cache update (if evict): 0.7229440212249756 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.097919940948486 ms
		out projection: 0.40959998965263367 ms
	self-attn: 15.69279956817627 ms
	cross-attn: 5.611519813537598 ms
	ffn: 1.5032320022583008 ms
forward previous block to here: 25.017343521118164 ms
---------------- attention block ----------------
		qkv norm: 0.9185280203819275 ms
		rope: 1.1059199571609497 ms
		kv cache update (if evict): 9.234432220458984 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.446847915649414 ms
		out projection: 0.37376001477241516 ms
	self-attn: 23.68716812133789 ms
	cross-attn: 9.995264053344727 ms
	ffn: 2.0746240615844727 ms
forward previous block to here: 40.64051055908203 ms
---------------- attention block ----------------
		qkv norm: 0.9052159786224365 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.694271981716156 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.37548828125 ms
		out projection: 0.35225600004196167 ms
	self-attn: 19.776512145996094 ms
	cross-attn: 4.074495792388916 ms
	ffn: 1.5063040256500244 ms
forward previous block to here: 25.761791229248047 ms
---------------- attention block ----------------
		qkv norm: 13.434880256652832 ms
		rope: 1.1048959493637085 ms
		kv cache update (if evict): 3.0791680812835693 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.160384178161621 ms
		out projection: 0.392192006111145 ms
	self-attn: 30.548992156982422 ms
	cross-attn: 6.150144100189209 ms
	ffn: 1.5144959688186646 ms
forward previous block to here: 39.974910736083984 ms
---------------- attention block ----------------
		qkv norm: 0.9123839735984802 ms
		rope: 1.124351978302002 ms
		kv cache update (if evict): 8.439807891845703 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.704895973205566 ms
		out projection: 0.3758080005645752 ms
	self-attn: 22.72256088256836 ms
	cross-attn: 1.546239972114563 ms
	ffn: 5.566463947296143 ms
forward previous block to here: 38.05081558227539 ms
---------------- attention block ----------------
		qkv norm: 0.9492480158805847 ms
		rope: 3.547136068344116 ms
		kv cache update (if evict): 0.6963199973106384 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.086656093597412 ms
		out projection: 0.32767999172210693 ms
	self-attn: 12.638208389282227 ms
	cross-attn: 1.4018559455871582 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 15.882240295410156 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.6696959733963013 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.001664161682129 ms
		out projection: 0.29388800263404846 ms
	self-attn: 9.736191749572754 ms
	cross-attn: 1.3015040159225464 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.844032287597656 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.31539198756217957 ms
	self-attn: 9.820159912109375 ms
	cross-attn: 1.3629440069198608 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 13.008895874023438 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.3742079734802246 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.37171199917793274 ms
	self-attn: 10.286080360412598 ms
	cross-attn: 1.381376028060913 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 13.513728141784668 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0864640474319458 ms
		kv cache update (if evict): 0.6809599995613098 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.024191856384277 ms
		out projection: 0.34406399726867676 ms
	self-attn: 9.975808143615723 ms
	cross-attn: 1.602560043334961 ms
	ffn: 1.5063040256500244 ms
forward previous block to here: 13.633536338806152 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.07315194606781 ms
		kv cache update (if evict): 0.6676480174064636 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.066175937652588 ms
		out projection: 0.33587199449539185 ms
	self-attn: 9.923583984375 ms
	cross-attn: 1.3998080492019653 ms
	ffn: 1.5032320022583008 ms
forward previous block to here: 13.195263862609863 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.6932479739189148 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0497918128967285 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.921536445617676 ms
	cross-attn: 1.4776320457458496 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 13.264896392822266 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.6666240096092224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.031360149383545 ms
		out projection: 0.3461120128631592 ms
	self-attn: 9.902079582214355 ms
	cross-attn: 1.4376959800720215 ms
	ffn: 1.504256010055542 ms
forward previous block to here: 13.200384140014648 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.6840320229530334 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.067200183868408 ms
		out projection: 0.34303998947143555 ms
	self-attn: 9.956352233886719 ms
	cross-attn: 1.3875199556350708 ms
	ffn: 1.5329279899597168 ms
forward previous block to here: 13.252608299255371 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.719871997833252 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.054912090301514 ms
		out projection: 0.32870399951934814 ms
	self-attn: 9.999360084533691 ms
	cross-attn: 1.4847999811172485 ms
	ffn: 1.4847999811172485 ms
forward previous block to here: 13.337599754333496 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.6686720252037048 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.2815999984741211 ms
	self-attn: 9.750528335571289 ms
	cross-attn: 1.260543942451477 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.828672409057617 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.6717439889907837 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.83142375946045 ms
	cross-attn: 1.371135950088501 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 13.042688369750977 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.089727878570557 ms
		out projection: 0.3307519853115082 ms
	self-attn: 9.883647918701172 ms
	cross-attn: 1.4489599466323853 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 13.170687675476074 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.6748160123825073 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.055935859680176 ms
		out projection: 0.32972800731658936 ms
	self-attn: 9.874431610107422 ms
	cross-attn: 1.415168046951294 ms
	ffn: 1.4960639476776123 ms
forward previous block to here: 13.146112442016602 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0497918128967285 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.893888473510742 ms
	cross-attn: 1.363968014717102 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 13.089792251586914 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.013951778411865 ms
		out projection: 0.2805759906768799 ms
	self-attn: 9.72697639465332 ms
	cross-attn: 1.2769279479980469 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.815360069274902 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.6717439889907837 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.984255790710449 ms
		out projection: 0.2826240062713623 ms
	self-attn: 9.683967590332031 ms
	cross-attn: 1.264639973640442 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.761088371276855 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.6707199811935425 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 0.29183998703956604 ms
	self-attn: 9.72390365600586 ms
	cross-attn: 1.2892160415649414 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 12.841983795166016 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.6717439889907837 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.34406399726867676 ms
	self-attn: 9.827327728271484 ms
	cross-attn: 1.3803520202636719 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 13.031423568725586 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 0.34406399726867676 ms
	self-attn: 9.91436767578125 ms
	cross-attn: 1.3875199556350708 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 13.184000015258789 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.6799359917640686 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0344319343566895 ms
		out projection: 0.33587199449539185 ms
	self-attn: 9.912320137023926 ms
	cross-attn: 1.4254080057144165 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 13.17580795288086 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.6737920045852661 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.030335903167725 ms
		out projection: 0.35839998722076416 ms
	self-attn: 9.93075180053711 ms
	cross-attn: 1.3967360258102417 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 13.153280258178711 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.6758400201797485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.041600227355957 ms
		out projection: 0.35839998722076416 ms
	self-attn: 9.976832389831543 ms
	cross-attn: 1.4387199878692627 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 13.287424087524414 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0813440084457397 ms
		kv cache update (if evict): 0.6809599995613098 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.062079906463623 ms
		out projection: 0.3532800078392029 ms
	self-attn: 9.990143775939941 ms
	cross-attn: 1.528831958770752 ms
	ffn: 1.4981119632720947 ms
forward last block: 13.386752128601074 ms
##### model time: 533.0299072265625 #####
	 Transformer denoising step 1/4 completed in 533.56 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.039552211761475 ms
		out projection: 0.3051519989967346 ms
	self-attn: 9.310208320617676 ms
	cross-attn: 1.4172159433364868 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.57369613647461 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0967040061950684 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.042623996734619 ms
		out projection: 0.2887679934501648 ms
	self-attn: 9.30406379699707 ms
	cross-attn: 1.2861440181732178 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.44262409210205 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.964799880981445 ms
		out projection: 0.30720001459121704 ms
	self-attn: 9.182208061218262 ms
	cross-attn: 1.2390400171279907 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.239871978759766 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.992447853088379 ms
		out projection: 0.317440003156662 ms
	self-attn: 9.268223762512207 ms
	cross-attn: 1.3209600448608398 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.402688026428223 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0588159561157227 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.021120071411133 ms
		out projection: 0.3266560137271881 ms
	self-attn: 9.345024108886719 ms
	cross-attn: 1.363968014717102 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.545023918151855 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.033408164978027 ms
		out projection: 0.32870399951934814 ms
	self-attn: 9.323519706726074 ms
	cross-attn: 1.3731839656829834 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.528639793395996 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0741759538650513 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.099967956542969 ms
		out projection: 0.3235839903354645 ms
	self-attn: 9.41977596282959 ms
	cross-attn: 1.3803520202636719 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.6627836227417 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.032383918762207 ms
		out projection: 0.33484798669815063 ms
	self-attn: 9.3306884765625 ms
	cross-attn: 1.3824000358581543 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.55731201171875 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.008831977844238 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.32863998413086 ms
	cross-attn: 1.381376028060913 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.543999671936035 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.3829759955406189 ms
	self-attn: 9.422847747802734 ms
	cross-attn: 1.4417920112609863 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.701696395874023 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0639359951019287 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01907205581665 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.23033618927002 ms
	cross-attn: 1.2001279592514038 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.218367576599121 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.031167984008789 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.987328052520752 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.127936363220215 ms
	cross-attn: 1.154047966003418 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.116991996765137 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.93612813949585 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.060352325439453 ms
	cross-attn: 1.1263999938964844 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.009471893310547 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.99788761138916 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 11.926527976989746 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.004032135009766 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 11.887616157531738 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.961024284362793 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.802623748779297 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.053183555603027 ms
	cross-attn: 1.1263999938964844 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 11.980799674987793 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.993791580200195 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.826175689697266 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.01529598236084 ms
	cross-attn: 1.1018240451812744 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 11.900927543640137 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.020416259765625 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.8722562789917 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.26214399933815 ms
	self-attn: 8.99891185760498 ms
	cross-attn: 1.1048959493637085 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.874303817749023 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.992768287658691 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.84665584564209 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.028608322143555 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.886591911315918 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.985600471496582 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.835391998291016 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.780096054077148 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.26009601354599 ms
	self-attn: 8.967167854309082 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 11.799551963806152 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.930303573608398 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.736063957214355 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.754495620727539 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.792384147644043 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.27852800488471985 ms
	self-attn: 9.070591926574707 ms
	cross-attn: 1.324031949043274 ms
	ffn: 1.491968035697937 ms
forward last block: 12.262399673461914 ms
##### model time: 367.3210754394531 #####
	 Transformer denoising step 2/4 completed in 367.96 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.12492799758911133 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.26316800713539124 ms
	self-attn: 9.362431526184082 ms
	cross-attn: 1.2666879892349243 ms
	ffn: 1.4858239889144897 ms
forward previous block to here: 12.484607696533203 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.975039958953857 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.261055946350098 ms
	cross-attn: 1.2759040594100952 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.382207870483398 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.989376068115234 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.334783554077148 ms
	cross-attn: 1.2492799758911133 ms
	ffn: 1.4858239889144897 ms
forward previous block to here: 12.491776466369629 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0557440519332886 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.976064205169678 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.271295547485352 ms
	cross-attn: 1.252351999282837 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.409855842590332 ms
---------------- attention block ----------------
		qkv norm: 0.971776008605957 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.163776397705078 ms
	cross-attn: 1.1161600351333618 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.051456451416016 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.737088203430176 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.729920387268066 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.694080352783203 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.00607967376709 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.842559814453125 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.979455947875977 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.803647994995117 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.919039726257324 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.727871894836426 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.957951545715332 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.793408393859863 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.936448097229004 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.74937629699707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.737088203430176 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.750399589538574 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.803647994995117 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.117184042930603 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.8220796585083 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.928256034851074 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.773951530456543 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.751423835754395 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.2969599962234497 ms
	self-attn: 9.1975679397583 ms
	cross-attn: 1.287168025970459 ms
	ffn: 1.5022079944610596 ms
forward previous block to here: 12.362751960754395 ms
---------------- attention block ----------------
		qkv norm: 0.9000959992408752 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.928959846496582 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.237504005432129 ms
	cross-attn: 1.2503039836883545 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.336128234863281 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.2590720057487488 ms
	self-attn: 9.233407974243164 ms
	cross-attn: 1.2625919580459595 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.354559898376465 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0721280574798584 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.995520114898682 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.294848442077637 ms
	cross-attn: 1.2922879457473755 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.424192428588867 ms
---------------- attention block ----------------
		qkv norm: 0.9031680226325989 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.341952323913574 ms
	cross-attn: 1.2369920015335083 ms
	ffn: 1.465343952178955 ms
forward last block: 12.419072151184082 ms
##### model time: 363.1073303222656 #####
	 Transformer denoising step 3/4 completed in 363.72 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10035199671983719 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.033727645874023 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.898880004882812 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.961024284362793 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.805695533752441 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.9169921875 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.75551986694336 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.945664405822754 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.819007873535156 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.729920387268066 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.944640159606934 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.5073280334472656 ms
forward previous block to here: 11.881471633911133 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.940544128417969 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.819007873535156 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.968192100524902 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.841535568237305 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.74015998840332 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.967167854309082 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 11.815936088562012 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.957951545715332 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 11.834367752075195 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.938495635986328 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.781120300292969 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.92793607711792 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.946687698364258 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.796480178833008 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.723775863647461 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.702272415161133 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.912896156311035 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.73094367980957 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.966143608093262 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.807744026184082 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.725824356079102 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.940544128417969 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.759615898132324 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.946687698364258 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.760640144348145 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.710463523864746 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.159168004989624 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.860992431640625 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.27033600211143494 ms
	self-attn: 8.979455947875977 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.804672241210938 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.012224197387695 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.827199935913086 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.91494369506836 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.760640144348145 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.73196792602539 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.732992172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.423359990119934 ms
forward last block: 11.724800109863281 ms
##### model time: 357.2254638671875 #####
	 Transformer final denoising step 4/4 completed in 357.73 ms
‚ö° Block 11 denoising completed in 1625.19 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.1085439994931221 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.720704078674316 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.770879745483398 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.74732780456543 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.986623764038086 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.806719779968262 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.935103893280029 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.973312377929688 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.779071807861328 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.74835205078125 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.940544128417969 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.758591651916504 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 11.827199935913086 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.02451229095459 ms
	cross-attn: 1.1079679727554321 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.911168098449707 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.03270435333252 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.883520126342773 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.960000038146973 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.85587215423584 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.941247940063477 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.102335929870605 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.931648254394531 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.769856452941895 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.957951545715332 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.805695533752441 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.926207542419434 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.768832206726074 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.752448081970215 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.785216331481934 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.779071807861328 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.954879760742188 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.842559814453125 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.947711944580078 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 11.799551963806152 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9310078620910645 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.983551979064941 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 11.84665584564209 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.968192100524902 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.807744026184082 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.786239624023438 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.00812816619873 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.850751876831055 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.691007614135742 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.772928237915039 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.944640159606934 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.806719779968262 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.018367767333984 ms
	cross-attn: 1.1560959815979004 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 11.97158432006836 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.972288131713867 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4407680034637451 ms
forward last block: 11.812864303588867 ms
##### model time: 357.84088134765625 #####
KV cache update for next block completed in 358.36 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 11 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 11 VAE decoding completed in 538.72 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 11 for sending...
‚úÖ Block 11 completed in 2564.05 ms (12 frames queued in 0.041 ms)
In loop: idx 11, current_num_frames 3, current_start_frame 33
üîÑ Processing block 12/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10547199845314026 ms
---------------- attention block ----------------
		qkv norm: 4.760575771331787 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 2.777087926864624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.131711959838867 ms
		out projection: 0.27136000990867615 ms
	self-attn: 26.91584014892578 ms
	cross-attn: 5.022719860076904 ms
	ffn: 2.9757440090179443 ms
forward previous block to here: 35.26860809326172 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 4.25164794921875 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.03545618057251 ms
		out projection: 0.2744320034980774 ms
	self-attn: 20.46566390991211 ms
	cross-attn: 5.488639831542969 ms
	ffn: 1.4929920434951782 ms
forward previous block to here: 36.927486419677734 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.1028480529785156 ms
		kv cache update (if evict): 7.592959880828857 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.81503963470459 ms
		out projection: 0.30003198981285095 ms
	self-attn: 20.573183059692383 ms
	cross-attn: 8.443903923034668 ms
	ffn: 4.7861762046813965 ms
forward previous block to here: 34.22822570800781 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0762239694595337 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.040575981140137 ms
		out projection: 4.265984058380127 ms
	self-attn: 18.06438446044922 ms
	cross-attn: 1.299456000328064 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 22.49625587463379 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 11.401215553283691 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.13478422164917 ms
		out projection: 0.2969599962234497 ms
	self-attn: 20.711423873901367 ms
	cross-attn: 12.499967575073242 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 35.062782287597656 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 11.826175689697266 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.056960105895996 ms
		out projection: 0.30003198981285095 ms
	self-attn: 21.032960891723633 ms
	cross-attn: 1.307647943496704 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 24.187904357910156 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.956607818603516 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.803775787353516 ms
	cross-attn: 1.4305280447006226 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 13.058048248291016 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.6717439889907837 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.941247940063477 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.537535667419434 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.408831596374512 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.937151908874512 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.505791664123535 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.371968269348145 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.49350357055664 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.410880088806152 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.525247573852539 ms
	cross-attn: 1.1151360273361206 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.418047904968262 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.6635519862174988 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.27852800488471985 ms
	self-attn: 9.54470443725586 ms
	cross-attn: 1.1847679615020752 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.524543762207031 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.6686720252037048 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.92793607711792 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.574399948120117 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.44876766204834 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.935103893280029 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.50271987915039 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.35148811340332 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.583616256713867 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.431360244750977 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.27852800488471985 ms
	self-attn: 9.631744384765625 ms
	cross-attn: 1.311743974685669 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.825599670410156 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.6952959895133972 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.989376068115234 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.810943603515625 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.660736083984375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.469951629638672 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 12.301312446594238 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.439231872558594 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.344320297241211 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.450495719909668 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 12.315648078918457 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.537535667419434 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.424192428588867 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.476096153259277 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 12.293120384216309 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.466879844665527 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.285951614379883 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.447423934936523 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.294143676757812 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.416704177856445 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.239871978759766 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.430015563964844 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 12.26137638092041 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9310078620910645 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.472000122070312 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.306431770324707 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.6758400201797485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.452544212341309 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.298239707946777 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.494527816772461 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 12.331007957458496 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.399295806884766 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.4469120502471924 ms
forward last block: 12.26137638092041 ms
##### model time: 492.2849426269531 #####
	 Transformer denoising step 1/4 completed in 492.79 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09932799637317657 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.935103893280029 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.979455947875977 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.83129596710205 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.952511787414551 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.967167854309082 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.804672241210938 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.728896141052246 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.761664390563965 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.949760437011719 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 11.842559814453125 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.25600001215934753 ms
	self-attn: 8.991744041442871 ms
	cross-attn: 1.1550719738006592 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 11.981823921203613 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.02451229095459 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 11.92959976196289 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.986623764038086 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 11.887616157531738 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.954879760742188 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.786239624023438 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.771903991699219 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.934399604797363 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.75551986694336 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.953856468200684 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.767807960510254 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.782143592834473 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.972288131713867 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.797504425048828 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.783167839050293 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.771903991699219 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.978431701660156 ms
	cross-attn: 1.1438080072402954 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.893759727478027 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.98969554901123 ms
	cross-attn: 1.1304960250854492 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.904000282287598 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.987648010253906 ms
	cross-attn: 1.112064003944397 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.875328063964844 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.19353599846363068 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.950463771820068 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.148415565490723 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.023807525634766 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.813887596130371 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.963071823120117 ms
	cross-attn: 1.120255947113037 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.866111755371094 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.1192320585250854 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.83743953704834 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.905728340148926 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.74835205078125 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.696127891540527 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.783167839050293 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 11.790335655212402 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.955904006958008 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4284800291061401 ms
forward last block: 11.778047561645508 ms
##### model time: 358.139892578125 #####
	 Transformer denoising step 2/4 completed in 358.64 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.768832206726074 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.805695533752441 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.952832221984863 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.790335655212402 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.994815826416016 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.83743953704834 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.010175704956055 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.84870433807373 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.75551986694336 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.714559555053711 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.961024284362793 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 11.841535568237305 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.91494369506836 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.76576042175293 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.76473617553711 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.1192320585250854 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.812864303588867 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.961024284362793 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 11.833344459533691 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.980480194091797 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.83948802947998 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.896512031555176 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.752448081970215 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.786239624023438 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.763711929321289 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.948736190795898 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.772928237915039 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.905728340148926 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 11.74630355834961 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.7391357421875 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.928256034851074 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.74835205078125 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.7391357421875 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.717632293701172 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.75654411315918 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.704319953918457 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.777024269104004 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2764799892902374 ms
	self-attn: 9.04089641571045 ms
	cross-attn: 1.2687360048294067 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.262399673461914 ms
---------------- attention block ----------------
		qkv norm: 1.035264015197754 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.2252800017595291 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.052864074707031 ms
		out projection: 0.2662400007247925 ms
	self-attn: 9.764863967895508 ms
	cross-attn: 1.2636159658432007 ms
	ffn: 1.5083520412445068 ms
forward previous block to here: 12.938240051269531 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.3434879779815674 ms
		kv cache update (if evict): 0.25088000297546387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.2566399574279785 ms
		out projection: 0.27238398790359497 ms
	self-attn: 10.072064399719238 ms
	cross-attn: 1.6158720254898071 ms
	ffn: 1.5882240533828735 ms
forward previous block to here: 13.757439613342285 ms
---------------- attention block ----------------
		qkv norm: 0.9861119985580444 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.366527557373047 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.302335739135742 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.933055877685547 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.077759742736816 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.439743995666504 ms
forward last block: 11.959296226501465 ms
##### model time: 361.4238586425781 #####
	 Transformer denoising step 3/4 completed in 361.94 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.0942080020904541 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.939199924468994 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.033727645874023 ms
	cross-attn: 1.1059199571609497 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.927552223205566 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.951807975769043 ms
	cross-attn: 1.1130880117416382 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.84768009185791 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.00915241241455 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.894783973693848 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.955904006958008 ms
	cross-attn: 1.117184042930603 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.858943939208984 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.952832221984863 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.836416244506836 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.955904006958008 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.819007873535156 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.940544128417969 ms
	cross-attn: 1.1182080507278442 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.836416244506836 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.965120315551758 ms
	cross-attn: 1.1212799549102783 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 11.877375602722168 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.961024284362793 ms
	cross-attn: 1.1141120195388794 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.8538236618042 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.973312377929688 ms
	cross-attn: 1.1274240016937256 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 11.891712188720703 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.953856468200684 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.824128150939941 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.25804799795150757 ms
	self-attn: 8.981504440307617 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.83948802947998 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.934399604797363 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.780096054077148 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.983551979064941 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 11.84768009185791 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.949760437011719 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.763711929321289 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.926207542419434 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.776000022888184 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.936448097229004 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.766783714294434 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.74835205078125 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.935423851013184 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.797504425048828 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.977408409118652 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.817983627319336 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.790335655212402 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.795455932617188 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.965120315551758 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.821056365966797 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.910847663879395 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.956928253173828 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.784192085266113 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.759615898132324 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.953856468200684 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.784192085266113 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.047039985656738 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.923456192016602 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.956928253173828 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4315520524978638 ms
forward last block: 11.788288116455078 ms
##### model time: 358.2054443359375 #####
	 Transformer final denoising step 4/4 completed in 358.71 ms
‚ö° Block 12 denoising completed in 1573.96 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.0942080020904541 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9555840492248535 ms
		out projection: 0.27852800488471985 ms
	self-attn: 9.103360176086426 ms
	cross-attn: 1.1909120082855225 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.076031684875488 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9402241706848145 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.00710391998291 ms
	cross-attn: 1.1233279705047607 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.897855758666992 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.829248428344727 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.928256034851074 ms
	cross-attn: 1.1427839994430542 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.85689640045166 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.962047576904297 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 11.812864303588867 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.974335670471191 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 11.829248428344727 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.039872169494629 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.885567665100098 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.953856468200684 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.796480178833008 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9258880615234375 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.951807975769043 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.792384147644043 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.942591667175293 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.776000022888184 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.003007888793945 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.843584060668945 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.912896156311035 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.786239624023438 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.973312377929688 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.800576210021973 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.74732780456543 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.75551986694336 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.767807960510254 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.922112464904785 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.7391357421875 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.25600001215934753 ms
	self-attn: 8.942591667175293 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.768832206726074 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.948736190795898 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.76576042175293 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.936448097229004 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.781120300292969 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.922112464904785 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.770879745483398 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.802623748779297 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.802623748779297 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.945664405822754 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.781120300292969 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.743231773376465 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 11.794431686401367 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4510079622268677 ms
forward last block: 11.76576042175293 ms
##### model time: 357.70367431640625 #####
KV cache update for next block completed in 358.24 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 12 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 12 VAE decoding completed in 540.45 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 12 for sending...
‚úÖ Block 12 completed in 2509.38 ms (12 frames queued in 0.036 ms)
In loop: idx 12, current_num_frames 3, current_start_frame 36
üîÑ Processing block 13/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 2.309119939804077 ms
---------------- attention block ----------------
		qkv norm: 0.9093120098114014 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 4.512767791748047 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.097919940948486 ms
		out projection: 0.30105599761009216 ms
	self-attn: 21.28384017944336 ms
	cross-attn: 12.405759811401367 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 37.45996856689453 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0956799983978271 ms
		kv cache update (if evict): 4.339712142944336 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0497918128967285 ms
		out projection: 7.344128131866455 ms
	self-attn: 27.993087768554688 ms
	cross-attn: 5.058559894561768 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 36.527103424072266 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 4.420608043670654 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.017024040222168 ms
		out projection: 0.27955201268196106 ms
	self-attn: 20.71552085876465 ms
	cross-attn: 12.581888198852539 ms
	ffn: 1.5800319910049438 ms
forward previous block to here: 35.339263916015625 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0598399639129639 ms
		kv cache update (if evict): 8.164352416992188 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 11.462656021118164 ms
		out projection: 0.3307519853115082 ms
	self-attn: 26.072063446044922 ms
	cross-attn: 1.4714879989624023 ms
	ffn: 1.5595519542694092 ms
forward previous block to here: 40.7828483581543 ms
---------------- attention block ----------------
		qkv norm: 7.0010881423950195 ms
		rope: 1.087488055229187 ms
		kv cache update (if evict): 0.7086079716682434 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.270848274230957 ms
		out projection: 0.32870399951934814 ms
	self-attn: 22.816768646240234 ms
	cross-attn: 5.273600101470947 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 31.520767211914062 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.1223039627075195 ms
		kv cache update (if evict): 4.297728061676025 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.06822395324707 ms
		out projection: 0.3338240087032318 ms
	self-attn: 21.312511444091797 ms
	cross-attn: 1.3895679712295532 ms
	ffn: 1.504256010055542 ms
forward previous block to here: 24.628223419189453 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.694271981716156 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0344319343566895 ms
		out projection: 0.5560320019721985 ms
	self-attn: 10.413056373596191 ms
	cross-attn: 1.1857919692993164 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 13.498368263244629 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.27750399708747864 ms
	self-attn: 9.747455596923828 ms
	cross-attn: 1.1622400283813477 ms
	ffn: 1.5206400156021118 ms
forward previous block to here: 12.882944107055664 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.7639039754867554 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.946368217468262 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.698304176330566 ms
	cross-attn: 1.1745280027389526 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.667903900146484 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9310078620910645 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.556991577148438 ms
	cross-attn: 1.154047966003418 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.490752220153809 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.524224281311035 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.384256362915039 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.507840156555176 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 12.34227180480957 ms
---------------- attention block ----------------
		qkv norm: 0.8796160221099854 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.8335360288619995 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.076416015625 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.857024192810059 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.739583969116211 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.461759567260742 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 12.304384231567383 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.077247977256775 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.658368110656738 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 12.497920036315918 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.523200035095215 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.378111839294434 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.485312461853027 ms
	cross-attn: 1.1079679727554321 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.36684799194336 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.464832305908203 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.322815895080566 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.478143692016602 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 12.304384231567383 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.468928337097168 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.333056449890137 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.928959846496582 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.521151542663574 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 12.372991561889648 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.450495719909668 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.320768356323242 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.439231872558594 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.286975860595703 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.463808059692383 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.337151527404785 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.628671646118164 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.492799758911133 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.6809599995613098 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9258880615234375 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.52627182006836 ms
	cross-attn: 1.1141120195388794 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.405759811401367 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.424896240234375 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.298239707946777 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.504768371582031 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.356608390808105 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.486335754394531 ms
	cross-attn: 1.1182080507278442 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 12.356608390808105 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.42080020904541 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4335999488830566 ms
forward last block: 12.236800193786621 ms
##### model time: 511.5187072753906 #####
	 Transformer denoising step 1/4 completed in 512.03 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.0942080020904541 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.953856468200684 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.816960334777832 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.01632022857666 ms
	cross-attn: 1.1438080072402954 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.92959976196289 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.935103893280029 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.083904266357422 ms
	cross-attn: 1.1386879682540894 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.992064476013184 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.01529598236084 ms
	cross-attn: 1.1509759426116943 ms
	ffn: 1.5278079509735107 ms
forward previous block to here: 12.118016242980957 ms
---------------- attention block ----------------
		qkv norm: 0.9021440148353577 ms
		rope: 1.0926079750061035 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.278464317321777 ms
	cross-attn: 1.1304960250854492 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 12.192768096923828 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.933055877685547 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.144319534301758 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.108799934387207 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.153535842895508 ms
	cross-attn: 1.1028480529785156 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.105728149414062 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.29183998703956604 ms
	self-attn: 9.12281608581543 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.07910442352295 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.125887870788574 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.5923199653625488 ms
forward previous block to here: 12.25830364227295 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.18943999707698822 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0200958251953125 ms
		out projection: 0.2734079957008362 ms
	self-attn: 9.554944038391113 ms
	cross-attn: 1.2728320360183716 ms
	ffn: 1.4970879554748535 ms
forward previous block to here: 12.813311576843262 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.1141120195388794 ms
		kv cache update (if evict): 0.187391996383667 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01804780960083 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.503744125366211 ms
	cross-attn: 1.2861440181732178 ms
	ffn: 1.5063040256500244 ms
forward previous block to here: 12.75494384765625 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0782719850540161 ms
		kv cache update (if evict): 0.23244799673557281 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.038527965545654 ms
		out projection: 0.2877439856529236 ms
	self-attn: 9.529343605041504 ms
	cross-attn: 1.3260799646377563 ms
	ffn: 1.5134719610214233 ms
forward previous block to here: 12.829695701599121 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0987520217895508 ms
		kv cache update (if evict): 0.33689600229263306 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.947391986846924 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.487360000610352 ms
	cross-attn: 1.1335680484771729 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.46720027923584 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.933055877685547 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.13100814819336 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.086272239685059 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.937151908874512 ms
		out projection: 0.26316800713539124 ms
	self-attn: 9.120767593383789 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.06067180633545 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9310078620910645 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.096192359924316 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.022784233093262 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.2805759906768799 ms
	self-attn: 9.099264144897461 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.010496139526367 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.2836480140686035 ms
	self-attn: 9.103360176086426 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.031999588012695 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.10643196105957 ms
	cross-attn: 1.1427839994430542 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.103679656982422 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.090047836303711 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.011520385742188 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.091072082519531 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.0381441116333 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9402241706848145 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.103360176086426 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.014592170715332 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.111552238464355 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.031999588012695 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.928959846496582 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.141247749328613 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.04428768157959 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.08460807800293 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.274368286132812 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.275712013244629 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.10540771484375 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.056575775146484 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2887679934501648 ms
	self-attn: 9.190400123596191 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.15999984741211 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.934080123901367 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.084927558898926 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.05350399017334 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9310078620910645 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.094143867492676 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.063743591308594 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.942272186279297 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.10745620727539 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4663679599761963 ms
forward last block: 12.055551528930664 ms
##### model time: 368.6553649902344 #####
	 Transformer denoising step 2/4 completed in 369.19 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09932799637317657 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9402241706848145 ms
		out projection: 0.2754560112953186 ms
	self-attn: 9.159680366516113 ms
	cross-attn: 1.1673599481582642 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.194815635681152 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.952511787414551 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.146368026733398 ms
	cross-attn: 1.1141120195388794 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.173312187194824 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.946368217468262 ms
		out projection: 0.2734079957008362 ms
	self-attn: 9.125887870788574 ms
	cross-attn: 1.134592056274414 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.142592430114746 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.2734079957008362 ms
	self-attn: 9.202688217163086 ms
	cross-attn: 1.146880030632019 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.231679916381836 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.939199924468994 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.20678424835205 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.509376049041748 ms
forward previous block to here: 12.240896224975586 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.935103893280029 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.166848182678223 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.141568183898926 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.937151908874512 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.116671562194824 ms
	cross-attn: 1.146880030632019 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.134400367736816 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.151488304138184 ms
	cross-attn: 1.1479040384292603 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.172287940979004 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.085951805114746 ms
	cross-attn: 1.1509759426116943 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.103679656982422 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.932032108306885 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.143296241760254 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.115967750549316 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.932032108306885 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.13100814819336 ms
	cross-attn: 1.1304960250854492 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.12723159790039 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.2590720057487488 ms
	self-attn: 9.096192359924316 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.058624267578125 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.134079933166504 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.098560333251953 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.2754560112953186 ms
	self-attn: 9.124863624572754 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.064767837524414 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.124863624572754 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.06272029876709 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.092096328735352 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.035072326660156 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.082880020141602 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.05452823638916 ms
---------------- attention block ----------------
		qkv norm: 0.8878080248832703 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.951488018035889 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.138175964355469 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.086272239685059 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.943295955657959 ms
		out projection: 0.27136000990867615 ms
	self-attn: 9.158656120300293 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.084223747253418 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.102335929870605 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 12.021759986877441 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.27955201268196106 ms
	self-attn: 9.157631874084473 ms
	cross-attn: 1.3271039724349976 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.35763168334961 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.956928253173828 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.772928237915039 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.91494369506836 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.705344200134277 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.758591651916504 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.935423851013184 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.734016418457031 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.752448081970215 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.005056381225586 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.816960334777832 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.935423851013184 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.74015998840332 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.728896141052246 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.952832221984863 ms
	cross-attn: 1.124351978302002 ms
	ffn: 1.4366719722747803 ms
forward last block: 11.84563159942627 ms
##### model time: 364.6095275878906 #####
	 Transformer denoising step 3/4 completed in 365.10 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.967167854309082 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.806719779968262 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.940544128417969 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 11.813887596130371 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.732992172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.92793607711792 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.938495635986328 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.75551986694336 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.732992172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.699199676513672 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.73196792602539 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.789312362670898 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.695103645324707 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.957951545715332 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.76576042175293 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.930303573608398 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.73094367980957 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.723775863647461 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.960000038146973 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.771903991699219 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.913920402526855 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.707391738891602 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.720704078674316 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.967167854309082 ms
	cross-attn: 1.1274240016937256 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.84665584564209 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.948736190795898 ms
	cross-attn: 1.141759991645813 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.844608306884766 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.910847663879395 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.761664390563965 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.784192085266113 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.754495620727539 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.9169921875 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.772928237915039 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.931327819824219 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 11.763711929321289 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.720704078674316 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.777024269104004 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.745280265808105 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.699199676513672 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.7391357421875 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.954879760742188 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.796480178833008 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.427456021308899 ms
forward last block: 11.794431686401367 ms
##### model time: 356.4451904296875 #####
	 Transformer final denoising step 4/4 completed in 356.94 ms
‚ö° Block 13 denoising completed in 1605.20 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.1013759970664978 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.964096069335938 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.852800369262695 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.9999361038208 ms
	cross-attn: 1.1182080507278442 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.8722562789917 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.1612160205841064 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.876352310180664 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.967167854309082 ms
	cross-attn: 1.1253759860992432 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.862015724182129 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.01427173614502 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.904000282287598 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.018367767333984 ms
	cross-attn: 1.1192320585250854 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.909119606018066 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.974335670471191 ms
	cross-attn: 1.117184042930603 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.85587215423584 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.954879760742188 ms
	cross-attn: 1.1161600351333618 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.83948802947998 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.120255947113037 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.800576210021973 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.963071823120117 ms
	cross-attn: 1.1141120195388794 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.83129596710205 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.784192085266113 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 11.74630355834961 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.954559803009033 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.046015739440918 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 11.912192344665527 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.947711944580078 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.761664390563965 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.728896141052246 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.912896156311035 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.718655586242676 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.673600196838379 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.707391738891602 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.705344200134277 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.661312103271484 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.894463539123535 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.694080352783203 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.726847648620605 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.944640159606934 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.751423835754395 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.8985595703125 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.700223922729492 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.703295707702637 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.903679847717285 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.737088203430176 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.73196792602539 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.894463539123535 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.73196792602539 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4295040369033813 ms
forward last block: 11.694080352783203 ms
##### model time: 356.91827392578125 #####
KV cache update for next block completed in 357.43 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 13 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 13 VAE decoding completed in 539.78 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 13 for sending...
‚úÖ Block 13 completed in 2539.00 ms (12 frames queued in 0.036 ms)
In loop: idx 13, current_num_frames 3, current_start_frame 39
üîÑ Processing block 14/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11059200018644333 ms
---------------- attention block ----------------
		qkv norm: 0.9082880020141602 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 8.208383560180664 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.128640174865723 ms
		out projection: 0.27955201268196106 ms
	self-attn: 21.28384017944336 ms
	cross-attn: 12.417023658752441 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 37.66067123413086 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 4.253695964813232 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.051839828491211 ms
		out projection: 0.27136000990867615 ms
	self-attn: 20.531200408935547 ms
	cross-attn: 12.8471040725708 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 37.156864166259766 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.1141120195388794 ms
		kv cache update (if evict): 7.629824161529541 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.955327987670898 ms
		out projection: 0.2979840040206909 ms
	self-attn: 20.787200927734375 ms
	cross-attn: 8.52070426940918 ms
	ffn: 4.956160068511963 ms
forward previous block to here: 34.65420913696289 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 3.842047929763794 ms
		kv cache update (if evict): 0.7127040028572083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.170623779296875 ms
		out projection: 4.331520080566406 ms
	self-attn: 19.716096878051758 ms
	cross-attn: 1.3281279802322388 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 24.184831619262695 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 7.721983909606934 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.843711853027344 ms
		out projection: 0.2734079957008362 ms
	self-attn: 20.617216110229492 ms
	cross-attn: 8.572928428649902 ms
	ffn: 4.7329277992248535 ms
forward previous block to here: 34.302974700927734 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 3.2225279808044434 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.994495868682861 ms
		out projection: 0.419840008020401 ms
	self-attn: 18.05619239807129 ms
	cross-attn: 1.2892160415649414 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 22.612991333007812 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.6615039706230164 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.51193618774414 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.371968269348145 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.145023822784424 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.727999687194824 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.571647644042969 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.443327903747559 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.282879829406738 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.637887954711914 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.45900821685791 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.414655685424805 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 12.25113582611084 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.7772160172462463 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.554944038391113 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.416000366210938 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.43616008758545 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.239871978759766 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.6584320068359375 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.452544212341309 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 12.300288200378418 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.459712028503418 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.304384231567383 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.2175359725952148 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.637887954711914 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 12.4518404006958 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.408512115478516 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 12.214271545410156 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.465855598449707 ms
	cross-attn: 1.1059199571609497 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 12.315648078918457 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.433088302612305 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.274687767028809 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.425919532775879 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 12.247039794921875 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.449472427368164 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.25932788848877 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.146880030632019 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.93612813949585 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.622528076171875 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.46003246307373 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.6696959733963013 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.449472427368164 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.247039794921875 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.472000122070312 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.305407524108887 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.6584320068359375 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.41875171661377 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 12.277759552001953 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.401344299316406 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.237824440002441 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.41875171661377 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.254207611083984 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.6584320068359375 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.474047660827637 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.290047645568848 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.41875171661377 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 12.233728408813477 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.448448181152344 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4243839979171753 ms
forward last block: 12.239871978759766 ms
##### model time: 492.28594970703125 #####
	 Transformer denoising step 1/4 completed in 492.79 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09728000313043594 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.787263870239258 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.913920402526855 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.903679847717285 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.76576042175293 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.1028480529785156 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.781120300292969 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.715583801269531 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.721728324890137 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.758591651916504 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.935423851013184 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.74732780456543 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.89139175415039 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.696127891540527 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.675647735595703 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.8985595703125 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.73094367980957 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.705344200134277 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.919039726257324 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.727871894836426 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.944640159606934 ms
	cross-attn: 1.1304960250854492 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.817983627319336 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.718655586242676 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.9169921875 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.763711929321289 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.776000022888184 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.946687698364258 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.776000022888184 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.931327819824219 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.779071807861328 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.771903991699219 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.910847663879395 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.803647994995117 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.942591667175293 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.783167839050293 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.758591651916504 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.708415985107422 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.896512031555176 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 11.74835205078125 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.922112464904785 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.718655586242676 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.75551986694336 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.74937629699707 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4223359823226929 ms
forward last block: 11.707391738891602 ms
##### model time: 356.0919189453125 #####
	 Transformer denoising step 2/4 completed in 356.59 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.751423835754395 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.903679847717285 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.706368446350098 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.25600001215934753 ms
	self-attn: 8.928256034851074 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.76576042175293 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.717632293701172 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.734016418457031 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.936448097229004 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.768832206726074 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.922112464904785 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.922112464904785 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.776000022888184 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.73094367980957 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.9169921875 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.753472328186035 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.935423851013184 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.76473617553711 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.766783714294434 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.76576042175293 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.709440231323242 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.687935829162598 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.728896141052246 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 11.760640144348145 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.707391738891602 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.980480194091797 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.789312362670898 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.930303573608398 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.726847648620605 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.945664405822754 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.768832206726074 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.930303573608398 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.7391357421875 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.754495620727539 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.74937629699707 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.1315200328826904 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.793408393859863 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.928256034851074 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.762687683105469 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.940544128417969 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.762687683105469 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4295040369033813 ms
forward last block: 11.695103645324707 ms
##### model time: 356.0151062011719 #####
	 Transformer denoising step 3/4 completed in 356.51 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.910847663879395 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.948416233062744 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.954879760742188 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.779071807861328 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.961024284362793 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 11.836416244506836 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.74015998840332 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.752448081970215 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.929280281066895 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.725824356079102 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.696127891540527 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.704319953918457 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.7391357421875 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.758591651916504 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.956928253173828 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.824128150939941 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.745280265808105 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.905728340148926 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.759615898132324 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.910847663879395 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.729920387268066 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.2672640085220337 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.75654411315918 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.750399589538574 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.930303573608398 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.74118423461914 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.73094367980957 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.905728340148926 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.726847648620605 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.675647735595703 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.766783714294434 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.945664405822754 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.74937629699707 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.708415985107422 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.698176383972168 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.922112464904785 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.737088203430176 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.752448081970215 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4243839979171753 ms
forward last block: 11.717632293701172 ms
##### model time: 355.8635559082031 #####
	 Transformer final denoising step 4/4 completed in 356.34 ms
‚ö° Block 14 denoising completed in 1564.08 ms
üé® Decoding block 14 to pixels...
Length of feat_cache:  32
##### [vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 14 VAE decoding completed in 540.95 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 14 for sending...
‚úÖ Block 14 completed in 2145.08 ms (12 frames queued in 0.040 ms)
üéâ Generation completed in 33.28 ms! 165 frames queued for sending
üéâ Generation completed in 33283.72 ms! (recorded with cuda event!)
‚è≥ Waiting for all frames to be sent...
‚úÖ All frames sent successfully!
Video saved to ./videos/A stylish woman walk_3761933378_5665df7398.mp4
üì° Frame sender thread stopped
üì° Frame sender thread startedüîÑ Switching VAE decoder to TAEHV

‚úÖ VAE decoder initialized with TAEHV
Prompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.
Conditional dict keys: ['prompt_embeds']
	 prompt_embeds shape: torch.Size([1, 512, 4096]), dtype: torch.bfloat16
Moving DynamicSwap_WanTextEncoder to cuda:0 with preserved memory: 33.55047941207886 GB
Seed: 1402690515
rnd: <torch._C.Generator object at 0x73a344662130> (based on gpu cuda:0)
 - Using local attention size x frame seq length = 21 x 1560 to set KV cache size to 32760
Frames status: total 42, per block [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
In loop: idx 0, current_num_frames 3, current_start_frame 0
üîÑ Processing block 1/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10956799983978271 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.20684799551963806 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.3383680582046509 ms
		out projection: 0.24268800020217896 ms
	self-attn: 4.562943935394287 ms
	cross-attn: 1.4069759845733643 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 7.767039775848389 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2615679502487183 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.327424049377441 ms
	cross-attn: 1.3445119857788086 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.4496002197265625 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.2782721519470215 ms
	cross-attn: 1.3096959590911865 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 7.394303798675537 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.280320167541504 ms
	cross-attn: 1.32915198802948 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 7.357439994812012 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.237311840057373 ms
	cross-attn: 1.3137919902801514 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.297023773193359 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.236288070678711 ms
	cross-attn: 1.3209600448608398 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.320576190948486 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.271103858947754 ms
	cross-attn: 1.299456000328064 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.354368209838867 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.267007827758789 ms
	cross-attn: 1.299456000328064 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.321599960327148 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2298239469528198 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.279295921325684 ms
	cross-attn: 1.2963839769363403 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.324672222137451 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.241407871246338 ms
	cross-attn: 1.30457603931427 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.279615879058838 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.295680046081543 ms
	cross-attn: 1.3895679712295532 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.4403839111328125 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.261888027191162 ms
	cross-attn: 1.393664002418518 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 7.420928001403809 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.268032073974609 ms
	cross-attn: 1.3475840091705322 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 7.369728088378906 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.238336086273193 ms
	cross-attn: 1.3137919902801514 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 7.345151901245117 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.18636800348758698 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.265984058380127 ms
	cross-attn: 1.3230079412460327 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.329792022705078 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.223999977111816 ms
	cross-attn: 1.2881920337677002 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.265279769897461 ms
---------------- attention block ----------------
		qkv norm: 0.8765439987182617 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2031999826431274 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.262911796569824 ms
	cross-attn: 1.2984319925308228 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.299071788787842 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.237311840057373 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.29804801940918 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.223999977111816 ms
	cross-attn: 1.363968014717102 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.33081579208374 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.1970560550689697 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.2342400550842285 ms
	cross-attn: 1.294335961341858 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.300096035003662 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.271103858947754 ms
	cross-attn: 1.3025280237197876 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 7.327744007110596 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2298239469528198 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.245503902435303 ms
	cross-attn: 1.3025280237197876 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.29088020324707 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.2639360427856445 ms
	cross-attn: 1.307647943496704 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.324672222137451 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.288512229919434 ms
	cross-attn: 1.3096959590911865 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.34003210067749 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.2393598556518555 ms
	cross-attn: 1.3434879779815674 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.333888053894043 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2257280349731445 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.281343936920166 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.345151901245117 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.229119777679443 ms
	cross-attn: 1.3127679824829102 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.297023773193359 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.280320167541504 ms
	cross-attn: 1.388543963432312 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.451648235321045 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.246528148651123 ms
	cross-attn: 1.32915198802948 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.362559795379639 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2349439859390259 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.2987518310546875 ms
	cross-attn: 1.3035520315170288 ms
	ffn: 1.4202879667282104 ms
forward last block: 7.342080116271973 ms
##### model time: 228.2178497314453 #####
	 Transformer denoising step 1/4 completed in 228.70 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10751999914646149 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.254720211029053 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.026688098907471 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2267520427703857 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.279295921325684 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.057407855987549 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.264959812164307 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.090176105499268 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.210368037223816 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.1748480796813965 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 6.9468159675598145 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.171775817871094 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 6.952960014343262 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.25088000297546387 ms
	self-attn: 4.179967880249023 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 6.9621758460998535 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.2539519965648651 ms
	self-attn: 4.196352005004883 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 6.989823818206787 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.1902079582214355 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 6.990848064422607 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.219903945922852 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 6.995967864990234 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.21068811416626 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.979584217071533 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.1950080394744873 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.177919864654541 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 6.987775802612305 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.205567836761475 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.972415924072266 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2298239469528198 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.2188801765441895 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 6.977536201477051 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.1991039514541626 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.189184188842773 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 6.939648151397705 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.214784145355225 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.998015880584717 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.175871849060059 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 6.94271993637085 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2349439859390259 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.21068811416626 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.973440170288086 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.211711883544922 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.961152076721191 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.203519821166992 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.996992111206055 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.219903945922852 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 7.036928176879883 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.240384101867676 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.054336071014404 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2216320037841797 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.288512229919434 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.087103843688965 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.217855930328369 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.99289608001709 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2216320037841797 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.215807914733887 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.9918718338012695 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.188159942626953 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.964223861694336 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.193280220031738 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.953983783721924 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.189184188842773 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.94374418258667 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.22732800245285034 ms
	self-attn: 4.1902079582214355 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 6.970367908477783 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.199423789978027 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 6.969344139099121 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.178944110870361 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.411072015762329 ms
forward last block: 6.9273600578308105 ms
##### model time: 213.359619140625 #####
	 Transformer denoising step 2/4 completed in 213.85 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10035199671983719 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.256768226623535 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.031807899475098 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.182015895843506 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.9713921546936035 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.2096638679504395 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.053311824798584 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.1902079582214355 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 6.949888229370117 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.2519040107727051 ms
	self-attn: 4.212736129760742 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 6.974463939666748 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2001279592514038 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.200448036193848 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.010303974151611 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2021759748458862 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.172800064086914 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.004159927368164 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2318719625473022 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.232192039489746 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.003136157989502 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.206272006034851 ms
		out projection: 0.24268800020217896 ms
	self-attn: 4.25267219543457 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.012351989746094 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2175359725952148 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.227071762084961 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.016448020935059 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.21068811416626 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 7.008255958557129 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.249599933624268 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.015423774719238 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2001279592514038 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.206592082977295 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.958079814910889 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.182015895843506 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.968319892883301 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.1950080394744873 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.2096638679504395 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 6.9918718338012695 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.215807914733887 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.986752033233643 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.24268800020217896 ms
	self-attn: 4.192255973815918 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 6.960127830505371 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.201151967048645 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.215807914733887 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.059455871582031 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.216832160949707 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 6.981632232666016 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.22835199534893036 ms
	self-attn: 4.203519821166992 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.995967864990234 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.197375774383545 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 6.966271877288818 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.22630399465560913 ms
	self-attn: 4.227071762084961 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 6.987775802612305 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.206592082977295 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.986752033233643 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.186111927032471 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 6.953983783721924 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.176896095275879 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.922239780426025 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.235263824462891 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.977536201477051 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2298239469528198 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.203519821166992 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.979584217071533 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.178944110870361 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 6.945792198181152 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.175871849060059 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 6.939648151397705 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.212736129760742 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.415168046951294 ms
forward last block: 6.964223861694336 ms
##### model time: 213.9842529296875 #####
	 Transformer denoising step 3/4 completed in 214.46 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10649599879980087 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.223680019378662 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.212736129760742 ms
	cross-attn: 1.1059199571609497 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 7.0799360275268555 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2267520427703857 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.2045440673828125 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.005184173583984 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2031999826431274 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.235263824462891 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.024640083312988 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.196352005004883 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.0010881423950195 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2385919988155365 ms
	self-attn: 4.217855930328369 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.966271877288818 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2615679502487183 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.245503902435303 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.026688098907471 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.1980799436569214 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.212736129760742 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 6.959104061126709 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2072960138320923 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.208640098571777 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.005184173583984 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.201151967048645 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.180992126464844 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 6.985727787017822 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.193280220031738 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.950911998748779 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.197375774383545 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.956031799316406 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.1980799436569214 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.186111927032471 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.982656002044678 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2021759748458862 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.187136173248291 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 6.959104061126709 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.210368037223816 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.191232204437256 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.950911998748779 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.198400020599365 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.956031799316406 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.1902079582214355 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.9713921546936035 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2001279592514038 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.195328235626221 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.037951946258545 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2216320037841797 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.236288070678711 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.048192024230957 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.193280220031738 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 6.972415924072266 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.258815765380859 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.017471790313721 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.192255973815918 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 6.975488185882568 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.184063911437988 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 6.949888229370117 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13209599256515503 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.177919864654541 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 6.936575889587402 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13209599256515503 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2257280349731445 ms
		out projection: 0.22732800245285034 ms
	self-attn: 4.212736129760742 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.964223861694336 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.171775817871094 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.959104061126709 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.187136173248291 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.957056045532227 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9707520008087158 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.189184188842773 ms
	cross-attn: 1.2636159658432007 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 7.260159969329834 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2144639492034912 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.24345588684082 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.022592067718506 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.201151967048645 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.207615852355957 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.016448020935059 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.276224136352539 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4407680034637451 ms
forward last block: 7.058432102203369 ms
##### model time: 213.55007934570312 #####
	 Transformer final denoising step 4/4 completed in 214.04 ms
‚ö° Block 1 denoising completed in 874.81 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2021759748458862 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.219903945922852 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.022592067718506 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2001279592514038 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.197375774383545 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 6.996992111206055 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.231167793273926 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 7.026688098907471 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.246528148651123 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.014400005340576 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2093440294265747 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.199423789978027 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 6.965248107910156 ms
---------------- attention block ----------------
		qkv norm: 0.8837119936943054 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.2690558433532715 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.067647933959961 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.24371199309825897 ms
	self-attn: 4.241407871246338 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.044095993041992 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.220928192138672 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.999040126800537 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.217855930328369 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 6.98470401763916 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2165119647979736 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.237311840057373 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.011328220367432 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.218559980392456 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.222976207733154 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.024640083312988 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.210368037223816 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.196352005004883 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 6.967296123504639 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.187136173248291 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 6.961152076721191 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2206079959869385 ms
		out projection: 0.24883200228214264 ms
	self-attn: 4.227071762084961 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.012351989746094 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.199423789978027 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.017471790313721 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2124160528182983 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.195328235626221 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 6.989823818206787 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2318719625473022 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.220928192138672 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.039999961853027 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.2188801765441895 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 6.974463939666748 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.206272006034851 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.228096008300781 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.03385591506958 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.21343994140625 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.221951961517334 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.017471790313721 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2247040271759033 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.220928192138672 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 6.99289608001709 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2042239904403687 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.231167793273926 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 6.978559970855713 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2113920450210571 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.207615852355957 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.0062079429626465 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.201151967048645 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.208640098571777 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 7.005184173583984 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2083200216293335 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.211711883544922 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 6.965248107910156 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2052479982376099 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.178944110870361 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 6.936575889587402 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.201151967048645 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.1748480796813965 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 6.995967864990234 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.2782721519470215 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.067647933959961 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2195839881896973 ms
		out projection: 0.22732800245285034 ms
	self-attn: 4.206592082977295 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 6.968319892883301 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 4680, 12, 128]) 
			 v: torch.Size([1, 4680, 12, 128])
		attention: 1.2154879570007324 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.199423789978027 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4141440391540527 ms
forward last block: 6.958079814910889 ms
##### model time: 213.644287109375 #####
KV cache update for next block completed in 214.19 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 1 to pixels...
denoised_pred shape: torch.Size([1, 3, 16, 60, 104])
pixels shape: torch.Size([1, 12, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 9, 3, 480, 832])
üé® Block 1 VAE decoding completed in 117.91 ms
pixels shape after possible cropping: torch.Size([1, 9, 3, 480, 832]), block_frames: 9
üì° Queueing 9 frames from block 1 for sending...
‚úÖ Block 1 completed in 1233.99 ms (9 frames queued in 0.027 ms)
In loop: idx 1, current_num_frames 3, current_start_frame 3
üîÑ Processing block 2/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.12185599654912949 ms
---------------- attention block ----------------
		qkv norm: 0.9349120259284973 ms
		rope: 1.0956799983978271 ms
		kv cache update (if evict): 0.2887679934501648 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.219007968902588 ms
		out projection: 0.2693119943141937 ms
	self-attn: 5.848063945770264 ms
	cross-attn: 8.839167594909668 ms
	ffn: 4.5793280601501465 ms
forward previous block to here: 19.713024139404297 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 3.4211840629577637 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.111488103866577 ms
		out projection: 0.25088000297546387 ms
	self-attn: 7.882751941680908 ms
	cross-attn: 1.222656011581421 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 10.958847999572754 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 8.51353645324707 ms
		kv cache update (if evict): 0.24063999950885773 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.12172794342041 ms
		out projection: 0.5130239725112915 ms
	self-attn: 19.116031646728516 ms
	cross-attn: 1.2216320037841797 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 22.133760452270508 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.22118400037288666 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 4.421631813049316 ms
		out projection: 0.26521599292755127 ms
	self-attn: 15.134719848632812 ms
	cross-attn: 1.1786240339279175 ms
	ffn: 3.949568033218384 ms
forward previous block to here: 20.6059513092041 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.5099520087242126 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.2364161014556885 ms
		out projection: 0.2877439856529236 ms
	self-attn: 19.736576080322266 ms
	cross-attn: 1.218559980392456 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 22.743040084838867 ms
---------------- attention block ----------------
		qkv norm: 4.655104160308838 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.39526399970054626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 4.607999801635742 ms
		out projection: 0.24985599517822266 ms
	self-attn: 19.21126365661621 ms
	cross-attn: 8.82585620880127 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 32.62566375732422 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 3.441663980484009 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.1391360759735107 ms
		out projection: 0.26214399933815 ms
	self-attn: 7.73529577255249 ms
	cross-attn: 13.647871971130371 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 25.183231353759766 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.1043200492858887 ms
		out projection: 0.27136000990867615 ms
	self-attn: 11.538432121276855 ms
	cross-attn: 4.964352130889893 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 20.128768920898438 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.057215929031372 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.158912181854248 ms
	cross-attn: 1.1325440406799316 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 8.061951637268066 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0039680004119873 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.045248031616211 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 7.9042558670043945 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.007359981536865 ms
	cross-attn: 1.6537599563598633 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 8.41215991973877 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0008959770202637 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.01145601272583 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.8745598793029785 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.994048118591309 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.83462381362915 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9886080026626587 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.975615978240967 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.804927825927734 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.24166400730609894 ms
	self-attn: 5.01964807510376 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.866367816925049 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.27136000990867615 ms
	self-attn: 5.272575855255127 ms
	cross-attn: 1.1673599481582642 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.321023941040039 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.020671844482422 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.84281587600708 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9886080026626587 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.004288196563721 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.8100481033325195 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9834879636764526 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.964352130889893 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.812096118927002 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0813440084457397 ms
		kv cache update (if evict): 0.19968000054359436 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.12992000579834 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.5357441902160645 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.352767944335938 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.978687763214111 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.761919975280762 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9927040338516235 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.992000102996826 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.781375885009766 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.993023872375488 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.789567947387695 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9793920516967773 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.971519947052002 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.758848190307617 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.725056171417236 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.947968006134033 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.715839862823486 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.966400146484375 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.732223987579346 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0060160160064697 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.982783794403076 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.7506561279296875 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.008064031600952 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.997119903564453 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.75270414352417 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.967423915863037 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.411072015762329 ms
forward last block: 7.8151679039001465 ms
##### model time: 354.5948181152344 #####
	 Transformer denoising step 1/4 completed in 355.10 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.08806400001049042 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.001919984817505 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.0032639503479 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.747583866119385 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.964352130889893 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4079999923706055 ms
forward previous block to here: 7.707647800445557 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.977344036102295 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.712768077850342 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9742720127105713 ms
		out projection: 0.24985599517822266 ms
	self-attn: 4.96127986907959 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.708672046661377 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.927487850189209 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.685120105743408 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.22630399465560913 ms
	self-attn: 4.9397759437561035 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.701504230499268 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9763200283050537 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.927487850189209 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.715839862823486 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9814399480819702 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.942848205566406 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.701504230499268 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.928512096405029 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.665664196014404 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.936704158782959 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 7.757823944091797 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.977344036102295 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.930560111999512 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.668735980987549 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.9541120529174805 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.6943359375 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.926464080810547 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 7.6615681648254395 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9722239971160889 ms
		out projection: 0.22835199534893036 ms
	self-attn: 4.894720077514648 ms
	cross-attn: 0.9953280091285706 ms
	ffn: 1.5063040256500244 ms
forward previous block to here: 7.758848190307617 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0848639011383057 ms
		out projection: 0.26419198513031006 ms
	self-attn: 5.359615802764893 ms
	cross-attn: 1.230847954750061 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 8.471551895141602 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.9838080406188965 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.773183822631836 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.948991775512695 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.708672046661377 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9834879636764526 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.940800189971924 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.705599784851074 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.945919990539551 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.731200218200684 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.950016021728516 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 7.715839862823486 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.24063999950885773 ms
	self-attn: 4.986879825592041 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.7608962059021 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.968448162078857 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.719935894012451 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9804160594940186 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.947968006134033 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.7117438316345215 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.2519040107727051 ms
	self-attn: 4.975615978240967 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.737343788146973 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.960256099700928 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.7363200187683105 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.2396160066127777 ms
	self-attn: 4.97049617767334 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.7506561279296875 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.963327884674072 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.7363200187683105 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.950016021728516 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.74451208114624 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.9541120529174805 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.748608112335205 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9988479614257812 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.945919990539551 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4254080057144165 ms
forward last block: 7.726079940795898 ms
##### model time: 235.9040069580078 #####
	 Transformer denoising step 2/4 completed in 236.41 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09625600278377533 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0049920082092285 ms
		out projection: 0.24371199309825897 ms
	self-attn: 4.971519947052002 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.748608112335205 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.9541120529174805 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.698431968688965 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9804160594940186 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.974592208862305 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.746560096740723 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.979712009429932 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.739391803741455 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.968448162078857 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 7.7608962059021 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0039680004119873 ms
		out projection: 0.2467840015888214 ms
	self-attn: 5.005311965942383 ms
	cross-attn: 1.2636159658432007 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 8.09062385559082 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0889599323272705 ms
		out projection: 0.25702399015426636 ms
	self-attn: 5.364736080169678 ms
	cross-attn: 1.2165119647979736 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 8.464384078979492 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0797441005706787 ms
		out projection: 0.35225600004196167 ms
	self-attn: 5.463039875030518 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.263680458068848 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.24268800020217896 ms
	self-attn: 4.977663993835449 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.740416049957275 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9927040338516235 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.945919990539551 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.73529577255249 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.972544193267822 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.762944221496582 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9804160594940186 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.9694719314575195 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.731200218200684 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9988479614257812 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.014527797698975 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.775231838226318 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.24166400730609894 ms
	self-attn: 4.986879825592041 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.748608112335205 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.945919990539551 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.724031925201416 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.989952087402344 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 7.75270414352417 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.9541120529174805 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.721983909606934 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.24780799448490143 ms
	self-attn: 4.978687763214111 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.774208068847656 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0008959770202637 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.96127986907959 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 7.756800174713135 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9814399480819702 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.753727912902832 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.97049617767334 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.784448146820068 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.959231853485107 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.753727912902832 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.968448162078857 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.721983909606934 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.951039791107178 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 7.742464065551758 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.723008155822754 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.008064031600952 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.990975856781006 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.759871959686279 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.007040023803711 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.9991679191589355 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.7608962059021 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.948991775512695 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.729152202606201 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9906560182571411 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.956160068511963 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.725056171417236 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.935679912567139 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4120960235595703 ms
forward last block: 7.685120105743408 ms
##### model time: 237.4983673095703 #####
	 Transformer denoising step 3/4 completed in 237.97 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.031936168670654 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.796735763549805 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.972544193267822 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.730175971984863 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.2385919988155365 ms
	self-attn: 4.9838080406188965 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.783423900604248 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.96127986907959 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.757823944091797 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9834879636764526 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.73529577255249 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.973567962646484 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 7.7854719161987305 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.955135822296143 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.75167989730835 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.959231853485107 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.716864109039307 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9927040338516235 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.967423915863037 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 7.747583866119385 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9916800260543823 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.97049617767334 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.730175971984863 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.011136054992676 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.989952087402344 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 7.762944221496582 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0142080783843994 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.006336212158203 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.769087791442871 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.9541120529174805 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.74451208114624 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.977663993835449 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.746560096740723 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.95308780670166 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.718912124633789 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.964352130889893 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.745535850524902 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.955135822296143 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.705599784851074 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.24268800020217896 ms
	self-attn: 4.984831809997559 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.729152202606201 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.984831809997559 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.74348783493042 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9834879636764526 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.726079940795898 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.24985599517822266 ms
	self-attn: 4.986879825592041 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 7.756800174713135 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.959231853485107 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.719935894012451 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.955135822296143 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.761919975280762 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9834879636764526 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.96230411529541 ms
	cross-attn: 1.136639952659607 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.83462381362915 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9793920516967773 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.931583881378174 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.734272003173828 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9793920516967773 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.1059199571609497 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.8151679039001465 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.96127986907959 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.742464065551758 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.957183837890625 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 7.732223987579346 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.9541120529174805 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 7.7363200187683105 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9896320104599 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.965375900268555 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.415168046951294 ms
forward last block: 7.723008155822754 ms
##### model time: 236.05453491210938 #####
	 Transformer final denoising step 4/4 completed in 236.52 ms
‚ö° Block 2 denoising completed in 1067.87 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09011200070381165 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.981760025024414 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.754752159118652 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.002943992614746 ms
		out projection: 0.24371199309825897 ms
	self-attn: 4.987904071807861 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 7.738368034362793 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.959231853485107 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.715839862823486 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.985856056213379 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 7.753727912902832 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.717887878417969 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9937280416488647 ms
		out projection: 0.2385919988155365 ms
	self-attn: 4.944896221160889 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 7.730175971984863 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0008959770202637 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.979712009429932 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.774208068847656 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.968448162078857 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.761919975280762 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.979712009429932 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.792640209197998 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.966400146484375 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.749631881713867 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.978687763214111 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.745535850524902 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9957760572433472 ms
		out projection: 0.23654399812221527 ms
	self-attn: 4.97049617767334 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 7.763967990875244 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.963327884674072 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.733248233795166 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.0060160160064697 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.978687763214111 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 7.756800174713135 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 2.013184070587158 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.9991679191589355 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 7.768064022064209 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.95308780670166 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.746560096740723 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.956160068511963 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 7.742464065551758 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9865599870681763 ms
		out projection: 0.23449599742889404 ms
	self-attn: 4.9541120529174805 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 7.725056171417236 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9875839948654175 ms
		out projection: 0.23552000522613525 ms
	self-attn: 4.992000102996826 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 7.79366397857666 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9834879636764526 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.964352130889893 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.721983909606934 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.23347200453281403 ms
	self-attn: 4.9694719314575195 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.709695816040039 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9998719692230225 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.009407997131348 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 7.761919975280762 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.950016021728516 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.710720062255859 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.994752049446106 ms
		out projection: 0.2519040107727051 ms
	self-attn: 5.007359981536865 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 7.777279853820801 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9845119714736938 ms
		out projection: 0.2303999960422516 ms
	self-attn: 4.950016021728516 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 7.701504230499268 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.99782395362854 ms
		out projection: 0.23756800591945648 ms
	self-attn: 4.958208084106445 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 7.7414398193359375 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9834879636764526 ms
		out projection: 0.24371199309825897 ms
	self-attn: 4.966400146484375 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 7.74348783493042 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9824639558792114 ms
		out projection: 0.22937600314617157 ms
	self-attn: 4.931583881378174 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 7.725056171417236 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.985535979270935 ms
		out projection: 0.2314240038394928 ms
	self-attn: 4.959231853485107 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 7.745535850524902 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 9360, 12, 128]) 
			 v: torch.Size([1, 9360, 12, 128])
		attention: 1.9967999458312988 ms
		out projection: 0.23244799673557281 ms
	self-attn: 4.946944236755371 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4172159433364868 ms
forward last block: 7.699456214904785 ms
##### model time: 235.81797790527344 #####
KV cache update for next block completed in 236.34 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 2 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 2 VAE decoding completed in 55.04 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 2 for sending...
‚úÖ Block 2 completed in 1381.62 ms (12 frames queued in 0.022 ms)
In loop: idx 2, current_num_frames 3, current_start_frame 6
üîÑ Processing block 3/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.1157120019197464 ms
---------------- attention block ----------------
		qkv norm: 8.164352416992188 ms
		rope: 1.1489280462265015 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.974720001220703 ms
		out projection: 0.2590720057487488 ms
	self-attn: 17.23084831237793 ms
	cross-attn: 1.1868159770965576 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 20.23833656311035 ms
---------------- attention block ----------------
		qkv norm: 4.876287937164307 ms
		rope: 1.031167984008789 ms
		kv cache update (if evict): 0.19046400487422943 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.934783935546875 ms
		out projection: 0.25600001215934753 ms
	self-attn: 27.3756160736084 ms
	cross-attn: 4.931583881378174 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 36.36735916137695 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 11.607040405273438 ms
		out projection: 0.2662400007247925 ms
	self-attn: 14.905344009399414 ms
	cross-attn: 1.1560959815979004 ms
	ffn: 3.6679680347442627 ms
forward previous block to here: 20.395008087158203 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 8.346624374389648 ms
		kv cache update (if evict): 0.21094399690628052 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.9716479778289795 ms
		out projection: 0.2682879865169525 ms
	self-attn: 18.84364891052246 ms
	cross-attn: 1.1868159770965576 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 21.850112915039062 ms
---------------- attention block ----------------
		qkv norm: 4.354047775268555 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 5.851136207580566 ms
		out projection: 0.26009601354599 ms
	self-attn: 19.679231643676758 ms
	cross-attn: 12.653568267822266 ms
	ffn: 1.5298559665679932 ms
forward previous block to here: 35.82668685913086 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.117184042930603 ms
		kv cache update (if evict): 0.20377600193023682 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 10.264575958251953 ms
		out projection: 0.3624959886074066 ms
	self-attn: 16.43110466003418 ms
	cross-attn: 1.264639973640442 ms
	ffn: 3.7652480602264404 ms
forward previous block to here: 21.818368911743164 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.18943999707698822 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 6.774784088134766 ms
		out projection: 0.6236159801483154 ms
	self-attn: 19.264511108398438 ms
	cross-attn: 1.3332480192184448 ms
	ffn: 1.5257600545883179 ms
forward previous block to here: 29.652992248535156 ms
---------------- attention block ----------------
		qkv norm: 0.9308159947395325 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.88972806930542 ms
		out projection: 0.26009601354599 ms
	self-attn: 12.007424354553223 ms
	cross-attn: 13.105152130126953 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 28.99660873413086 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.880511999130249 ms
		out projection: 0.24268800020217896 ms
	self-attn: 6.0293121337890625 ms
	cross-attn: 1.1325440406799316 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 8.938495635986328 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8190720081329346 ms
		out projection: 0.24063999950885773 ms
	self-attn: 5.902336120605469 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 8.90982437133789 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.791424036026001 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.795839786529541 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 8.579071998596191 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.792448043823242 ms
		out projection: 0.24371199309825897 ms
	self-attn: 5.796864032745361 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.584192276000977 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.749760150909424 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.531968116760254 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.22937600314617157 ms
	self-attn: 5.777408123016357 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.530943870544434 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.772991895675659 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.709824085235596 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.443903923034668 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.756608009338379 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.741568088531494 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.487936019897461 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.769216060638428 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.516608238220215 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.772991895675659 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.788671970367432 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.544256210327148 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.24063999950885773 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.129472017288208 ms
	ffn: 1.7029119729995728 ms
forward previous block to here: 8.912896156311035 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.770944118499756 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.72211217880249 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.492032051086426 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8149759769439697 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.844992160797119 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.62822437286377 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7596800327301025 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.876736164093018 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.843263626098633 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.756608009338379 ms
		out projection: 0.2539519965648651 ms
	self-attn: 5.843967914581299 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.663040161132812 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.78220796585083 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.765120029449463 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 8.532992362976074 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.77023983001709 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.587264060974121 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.783552169799805 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.573951721191406 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.774335861206055 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.52889633178711 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.75763201713562 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.758975982666016 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.500224113464355 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.770944118499756 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.752831935882568 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.515583992004395 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.761728048324585 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.737472057342529 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4264320135116577 ms
forward last block: 8.51046371459961 ms
##### model time: 408.7623596191406 #####
	 Transformer denoising step 1/4 completed in 409.26 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.802687883377075 ms
		out projection: 0.24780799448490143 ms
	self-attn: 5.833727836608887 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 8.671232223510742 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.758975982666016 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 8.562687873840332 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.768191814422607 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.579071998596191 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.735424041748047 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.524800300598145 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.761023998260498 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.5862398147583 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.765120029449463 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.514559745788574 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.754879951477051 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.530943870544434 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.727231979370117 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.499199867248535 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7750399112701416 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.7333760261535645 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 8.530943870544434 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.756927967071533 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 8.574975967407227 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7648000717163086 ms
		out projection: 0.24268800020217896 ms
	self-attn: 5.739520072937012 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.534015655517578 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.771967887878418 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.750783920288086 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.516608238220215 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.752831935882568 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.499199867248535 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.750783920288086 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.527872085571289 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7607040405273438 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.740543842315674 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.506367683410645 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.725183963775635 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 8.518655776977539 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.753856182098389 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 8.546303749084473 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.766848087310791 ms
		out projection: 0.24063999950885773 ms
	self-attn: 5.745664119720459 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.531968116760254 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.758975982666016 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.544256210327148 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.783552169799805 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 8.566783905029297 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 8.588288307189941 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.761728048324585 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.732351779937744 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 8.536064147949219 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7985920906066895 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.799935817718506 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 8.587264060974121 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.7487359046936035 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 8.566783905029297 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.766848087310791 ms
		out projection: 0.25088000297546387 ms
	self-attn: 5.773312091827393 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 8.561663627624512 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.770944118499756 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.736447811126709 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.491007804870605 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.573951721191406 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.737472057342529 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.560640335083008 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.777408123016357 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 8.569855690002441 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.435647964477539 ms
forward last block: 8.574975967407227 ms
##### model time: 260.1441345214844 #####
	 Transformer denoising step 2/4 completed in 260.63 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09728000313043594 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.796544075012207 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.780479907989502 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 8.546303749084473 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.735424041748047 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.525823593139648 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.781504154205322 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.537088394165039 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.7282562255859375 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.460288047790527 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.761728048324585 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.735424041748047 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.478719711303711 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.746687889099121 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.500224113464355 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.7190399169921875 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.5032958984375 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7750399112701416 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.7190399169921875 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 8.49510383605957 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7678720951080322 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.754879951477051 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.584192276000977 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.761728048324585 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 8.52889633178711 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7750399112701416 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.785600185394287 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.526847839355469 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.7487359046936035 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.501248359680176 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7996160984039307 ms
		out projection: 0.24063999950885773 ms
	self-attn: 5.786623954772949 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 8.587264060974121 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.76204776763916 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 8.573951721191406 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7893760204315186 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.785600185394287 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.559616088867188 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.735424041748047 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.51353645324707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.761728048324585 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.740543842315674 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.506367683410645 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.771967887878418 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.747712135314941 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.493056297302246 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7596800327301025 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.724160194396973 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.51046371459961 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.72108793258667 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.479743957519531 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7607040405273438 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.711872100830078 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.561663627624512 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.786303997039795 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.775360107421875 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.52889633178711 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.772991895675659 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.710847854614258 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.484864234924316 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.755904197692871 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.49407958984375 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.771967887878418 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.72211217880249 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.484864234924316 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.761728048324585 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.734399795532227 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 8.475647926330566 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7801599502563477 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.525823593139648 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.711872100830078 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.493056297302246 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.771967887878418 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.71289587020874 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 8.493056297302246 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7699201107025146 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.743616104125977 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4284800291061401 ms
forward last block: 8.536064147949219 ms
##### model time: 259.11090087890625 #####
	 Transformer denoising step 3/4 completed in 259.58 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09932799637317657 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7934720516204834 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.785600185394287 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.574975967407227 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.736447811126709 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.529919624328613 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.25600001215934753 ms
	self-attn: 5.790719985961914 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 8.584192276000977 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.2467840015888214 ms
	self-attn: 5.7784318923950195 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 8.545280456542969 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.795520067214966 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.785600185394287 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 8.556544303894043 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7648000717163086 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.77023983001709 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.541184425354004 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7832319736480713 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.747712135314941 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 8.52070426940918 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7893760204315186 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.756927967071533 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 8.555520057678223 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.24371199309825897 ms
	self-attn: 5.730303764343262 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.554495811462402 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.784575939178467 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.581119537353516 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.760000228881836 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.516608238220215 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7883520126342773 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.775360107421875 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.526847839355469 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.730303764343262 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.48691177368164 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7791359424591064 ms
		out projection: 0.24371199309825897 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 8.546303749084473 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.760000228881836 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 8.557567596435547 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7648000717163086 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.7282562255859375 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.51148796081543 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.749760150909424 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.509440422058105 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.747712135314941 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.49612808227539 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7883520126342773 ms
		out projection: 0.2303999960422516 ms
	self-attn: 5.7579522132873535 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.52070426940918 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7607040405273438 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.7190399169921875 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 8.470527648925781 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.24473600089550018 ms
	self-attn: 5.740543842315674 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 8.522751808166504 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7648000717163086 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.767168045043945 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.560640335083008 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.24371199309825897 ms
	self-attn: 5.726208209991455 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.51251220703125 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7586560249328613 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.745664119720459 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 8.508416175842285 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7607040405273438 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.7487359046936035 ms
	cross-attn: 1.2707840204238892 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 8.761343955993652 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.24371199309825897 ms
	self-attn: 5.768191814422607 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.538111686706543 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7648000717163086 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.71289587020874 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.481792449951172 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.787328004837036 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.772287845611572 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.529919624328613 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.777087926864624 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.738495826721191 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 8.562687873840332 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.2519040107727051 ms
	self-attn: 5.814271926879883 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4335999488830566 ms
forward last block: 8.60979175567627 ms
##### model time: 259.89837646484375 #####
	 Transformer final denoising step 4/4 completed in 260.42 ms
‚ö° Block 3 denoising completed in 1191.70 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.786303997039795 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.77126407623291 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.5862398147583 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.2672640085220337 ms
	self-attn: 6.518784046173096 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 9.349120140075684 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.23756800591945648 ms
	self-attn: 5.826560020446777 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.675328254699707 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.766848087310791 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.758975982666016 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.534015655517578 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7934720516204834 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.765120029449463 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.527872085571289 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.766848087310791 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 8.548352241516113 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7688961029052734 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 8.563712120056152 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.760000228881836 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.521727561950684 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.766848087310791 ms
		out projection: 0.23449599742889404 ms
	self-attn: 5.7579522132873535 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 8.547327995300293 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.734399795532227 ms
	cross-attn: 1.2410880327224731 ms
	ffn: 1.4960639476776123 ms
forward previous block to here: 8.860671997070312 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.187391996383667 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8938241004943848 ms
		out projection: 0.25600001215934753 ms
	self-attn: 6.172671794891357 ms
	cross-attn: 1.2441600561141968 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 9.257984161376953 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0844160318374634 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8631041049957275 ms
		out projection: 0.25804799795150757 ms
	self-attn: 6.165503978729248 ms
	cross-attn: 1.2175359725952148 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 9.247743606567383 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.8692479133605957 ms
		out projection: 0.2662400007247925 ms
	self-attn: 6.148096084594727 ms
	cross-attn: 1.2584960460662842 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 9.277440071105957 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.787328004837036 ms
		out projection: 0.24268800020217896 ms
	self-attn: 5.801983833312988 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.599552154541016 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7750399112701416 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.745664119720459 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 8.549375534057617 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7648000717163086 ms
		out projection: 0.2314240038394928 ms
	self-attn: 5.776383876800537 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 8.537088394165039 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.78220796585083 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.754879951477051 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.522751808166504 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.2385919988155365 ms
	self-attn: 5.767168045043945 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 8.583168029785156 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.76582407951355 ms
		out projection: 0.24371199309825897 ms
	self-attn: 5.752831935882568 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.542207717895508 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.23654399812221527 ms
	self-attn: 5.746687889099121 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 8.52889633178711 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7893760204315186 ms
		out projection: 0.2396160066127777 ms
	self-attn: 5.831679821014404 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 8.634367942810059 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7781119346618652 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.744639873504639 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 8.544256210327148 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.7927680015563965 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 8.567808151245117 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.762752056121826 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.750783920288086 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.51968002319336 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.781183958053589 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.764095783233643 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.544256210327148 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7596800327301025 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.764095783233643 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 8.51251220703125 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7740159034729004 ms
		out projection: 0.23347200453281403 ms
	self-attn: 5.72211217880249 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 8.479743957519531 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7852799892425537 ms
		out projection: 0.23244799673557281 ms
	self-attn: 5.764095783233643 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 8.563712120056152 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.776063919067383 ms
		out projection: 0.22937600314617157 ms
	self-attn: 5.715968132019043 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 8.497152328491211 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 14040, 12, 128]) 
			 v: torch.Size([1, 14040, 12, 128])
		attention: 2.7637760639190674 ms
		out projection: 0.23552000522613525 ms
	self-attn: 5.775360107421875 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4407680034637451 ms
forward last block: 8.59340763092041 ms
##### model time: 263.4700927734375 #####
KV cache update for next block completed in 264.02 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 3 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 3 VAE decoding completed in 55.22 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 3 for sending...
‚úÖ Block 3 completed in 1532.24 ms (12 frames queued in 0.021 ms)
In loop: idx 3, current_num_frames 3, current_start_frame 9
üîÑ Processing block 4/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10444799810647964 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 5.036032199859619 ms
		kv cache update (if evict): 0.18636800348758698 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.7457919120788574 ms
		out projection: 0.3123199939727783 ms
	self-attn: 18.39411163330078 ms
	cross-attn: 1.3096959590911865 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 28.96281623840332 ms
---------------- attention block ----------------
		qkv norm: 0.8724480271339417 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6771841049194336 ms
		out projection: 0.33689600229263306 ms
	self-attn: 19.4467830657959 ms
	cross-attn: 1.3895679712295532 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 27.96031951904297 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.031167984008789 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 7.85203218460083 ms
		out projection: 3.7355520725250244 ms
	self-attn: 14.909440040588379 ms
	cross-attn: 1.2257280349731445 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 19.358720779418945 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.090559959411621 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6464641094207764 ms
		out projection: 4.110335826873779 ms
	self-attn: 15.566847801208496 ms
	cross-attn: 1.1950080394744873 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 19.82975959777832 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.19251200556755066 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 7.84281587600708 ms
		out projection: 0.26521599292755127 ms
	self-attn: 14.58176040649414 ms
	cross-attn: 1.1745280027389526 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 19.098623275756836 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.20582400262355804 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 7.853055953979492 ms
		out projection: 0.32153600454330444 ms
	self-attn: 15.229951858520508 ms
	cross-attn: 1.2687360048294067 ms
	ffn: 2.772991895675659 ms
forward previous block to here: 19.66387176513672 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.644416093826294 ms
		out projection: 0.5601279735565186 ms
	self-attn: 15.373311996459961 ms
	cross-attn: 1.1878399848937988 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 19.792896270751953 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6331520080566406 ms
		out projection: 0.2744320034980774 ms
	self-attn: 14.927871704101562 ms
	cross-attn: 1.193984031677246 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 19.670015335083008 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 7.927807807922363 ms
		out projection: 3.9086079597473145 ms
	self-attn: 14.864383697509766 ms
	cross-attn: 1.4510079622268677 ms
	ffn: 3.9075839519500732 ms
forward previous block to here: 20.6376953125 ms
---------------- attention block ----------------
		qkv norm: 0.9431040287017822 ms
		rope: 12.85427188873291 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.684351921081543 ms
		out projection: 0.2815999984741211 ms
	self-attn: 18.903039932250977 ms
	cross-attn: 1.2144639492034912 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 21.9289608001709 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5973119735717773 ms
		out projection: 0.24473600089550018 ms
	self-attn: 6.635519981384277 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 9.472000122070312 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5720319747924805 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 9.397248268127441 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.563839912414551 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 9.373696327209473 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.554624080657959 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.318400382995605 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.1816960573196411 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.754303932189941 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.506815910339355 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.536895990371704 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.500351905822754 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.254912376403809 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.4254080057144165 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.676799774169922 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5379199981689453 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.5075201988220215 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.254912376403809 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.491136074066162 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.269248008728027 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5389440059661865 ms
		out projection: 0.22937600314617157 ms
	self-attn: 6.505472183227539 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 9.265151977539062 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5348479747772217 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.525951862335205 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.299967765808105 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.22937600314617157 ms
	self-attn: 6.492159843444824 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.264127731323242 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5348479747772217 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.634496212005615 ms
	cross-attn: 0.9963520169258118 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 9.346048355102539 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.535871982574463 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.497280120849609 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 9.234432220458984 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.22937600314617157 ms
	self-attn: 6.4931840896606445 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.257984161376953 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5379199981689453 ms
		out projection: 0.22937600314617157 ms
	self-attn: 6.512639999389648 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 9.23852825164795 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.535871982574463 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.495232105255127 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 9.248767852783203 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5348479747772217 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.48089599609375 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.244671821594238 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.488063812255859 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.23033618927002 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.531775951385498 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.511616230010986 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4120960235595703 ms
forward last block: 9.242624282836914 ms
##### model time: 407.8970947265625 #####
	 Transformer denoising step 1/4 completed in 408.37 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.2385919988155365 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 9.367551803588867 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5676159858703613 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.5576958656311035 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.365504264831543 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.54201602935791 ms
		out projection: 0.24268800020217896 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.273344039916992 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.519807815551758 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.284607887268066 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.557375907897949 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.486015796661377 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.257984161376953 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5399680137634277 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.264127731323242 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.536895990371704 ms
		out projection: 0.22937600314617157 ms
	self-attn: 6.4931840896606445 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 9.228287696838379 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.481919765472412 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.254912376403809 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.540992021560669 ms
		out projection: 0.22835199534893036 ms
	self-attn: 6.501376152038574 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 9.228287696838379 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5327999591827393 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.48908805847168 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.22214412689209 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.467584133148193 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.247743606567383 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.496255874633789 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.227264404296875 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5389440059661865 ms
		out projection: 0.24063999950885773 ms
	self-attn: 6.511616230010986 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 9.251839637756348 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5327999591827393 ms
		out projection: 0.22937600314617157 ms
	self-attn: 6.456319808959961 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.5943679809570312 ms
forward previous block to here: 9.407487869262695 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.569983959197998 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.323519706726074 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.511616230010986 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.29587173461914 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5635199546813965 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.520832061767578 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 9.327615737915039 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.32044792175293 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.559743881225586 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 9.369600296020508 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.523903846740723 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.340928077697754 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5399680137634277 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.553599834442139 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.331711769104004 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.583295822143555 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 9.377792358398438 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.575808048248291 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.597631931304932 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.414655685424805 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5532801151275635 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.540287971496582 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.309184074401855 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.552576065063477 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 9.384960174560547 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5576958656311035 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.318400382995605 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.502399921417236 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.279487609863281 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.54201602935791 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.520832061767578 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 9.293824195861816 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.529024124145508 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.281536102294922 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5624959468841553 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.517759799957275 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4254080057144165 ms
forward last block: 9.280511856079102 ms
##### model time: 282.682373046875 #####
	 Transformer denoising step 2/4 completed in 283.15 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.24473600089550018 ms
	self-attn: 6.575104236602783 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.345024108886719 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.5075201988220215 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.308159828186035 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.560447931289673 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.529024124145508 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.293824195861816 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.2457599937915802 ms
	self-attn: 6.555647850036621 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.318400382995605 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 9.312255859375 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.503424167633057 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.264127731323242 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.533120155334473 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.31430435180664 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.565567970275879 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 9.32249641418457 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.512639999389648 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.252863883972168 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.2467840015888214 ms
	self-attn: 6.586368083953857 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 9.407487869262695 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.56659197807312 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.552576065063477 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 9.392127990722656 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.545088052749634 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.58022403717041 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.351167678833008 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5543038845062256 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.5382399559021 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 9.31430435180664 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.57478404045105 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.558720111846924 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.348095893859863 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5379199981689453 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.560768127441406 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.30508804321289 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.558720111846924 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.327615737915039 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5727360248565674 ms
		out projection: 0.2303999960422516 ms
	self-attn: 6.545407772064209 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.346048355102539 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.281536102294922 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5624959468841553 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.533120155334473 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.306112289428711 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.24473600089550018 ms
	self-attn: 6.569983959197998 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.368576049804688 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.536191940307617 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 9.312255859375 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.55020809173584 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.512639999389648 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 9.285632133483887 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.561471939086914 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.53004789352417 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.30508804321289 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.540287971496582 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 9.341952323913574 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.571008205413818 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 9.354240417480469 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5481600761413574 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.541312217712402 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.299967765808105 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.510591983795166 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.291775703430176 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.24473600089550018 ms
	self-attn: 6.552576065063477 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.336832046508789 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.25600001215934753 ms
	self-attn: 6.554624080657959 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.315327644348145 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5583999156951904 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.516736030578613 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4213119745254517 ms
forward last block: 9.282560348510742 ms
##### model time: 283.1749267578125 #####
	 Transformer denoising step 3/4 completed in 283.65 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5491840839385986 ms
		out projection: 0.24063999950885773 ms
	self-attn: 6.551551818847656 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.32863998413086 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5594239234924316 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.52185583114624 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 9.33785629272461 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.536895990371704 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.293824195861816 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.2385919988155365 ms
	self-attn: 6.5279998779296875 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.312255859375 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5696640014648438 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.546432018280029 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 9.361408233642578 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5399680137634277 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.540287971496582 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.294848442077637 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5635199546813965 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.543360233306885 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 9.32044792175293 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.484992027282715 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.276415824890137 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.540287971496582 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.281536102294922 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5532801151275635 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.5372161865234375 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.297920227050781 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.503424167633057 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.278464317321777 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5389440059661865 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.289728164672852 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.55020809173584 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.5228800773620605 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.299967765808105 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.519807815551758 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.333760261535645 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.559743881225586 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.315327644348145 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.600383996963501 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.608895778656006 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 9.39417552947998 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5727360248565674 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.548480033874512 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.360383987426758 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.534143924713135 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.285632133483887 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.23244799673557281 ms
	self-attn: 6.509568214416504 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 9.290752410888672 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5921919345855713 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.582272052764893 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 9.396224021911621 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.53004789352417 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.32044792175293 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5840001106262207 ms
		out projection: 0.26316800713539124 ms
	self-attn: 6.7051520347595215 ms
	cross-attn: 1.1356159448623657 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 9.652223587036133 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6075520515441895 ms
		out projection: 0.24371199309825897 ms
	self-attn: 6.702079772949219 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 9.503744125366211 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.576128005981445 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 9.356287956237793 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.514688014984131 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 9.32249641418457 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5522561073303223 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.523903846740723 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 9.297920227050781 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.515711784362793 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 9.279487609863281 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.502399921417236 ms
	cross-attn: 1.087488055229187 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 9.342975616455078 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.540992021560669 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.529024124145508 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.287679672241211 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.540992021560669 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.514688014984131 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.418239951133728 ms
forward last block: 9.30406379699707 ms
##### model time: 283.57940673828125 #####
	 Transformer final denoising step 4/4 completed in 284.06 ms
‚ö° Block 4 denoising completed in 1261.04 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5635199546813965 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.567935943603516 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.353216171264648 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5829761028289795 ms
		out projection: 0.2385919988155365 ms
	self-attn: 6.562816143035889 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.359359741210938 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23756800591945648 ms
	self-attn: 6.554624080657959 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.317376136779785 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.548480033874512 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.355263710021973 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.570688009262085 ms
		out projection: 0.24268800020217896 ms
	self-attn: 6.561791896820068 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.348095893859863 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5440640449523926 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.5382399559021 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 9.316351890563965 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6034560203552246 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.576128005981445 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 9.354240417480469 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.558720111846924 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 9.343999862670898 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.520832061767578 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 9.324543952941895 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.24780799448490143 ms
	self-attn: 6.595583915710449 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 9.407487869262695 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.25702399015426636 ms
	self-attn: 6.589439868927002 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 9.366527557373047 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.551232099533081 ms
		out projection: 0.23552000522613525 ms
	self-attn: 6.53926420211792 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 9.31430435180664 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.547136068344116 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.5423359870910645 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 9.342975616455078 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5645439624786377 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.552576065063477 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.326592445373535 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.546112060546875 ms
		out projection: 0.24371199309825897 ms
	self-attn: 6.559743881225586 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 9.3306884765625 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.503424167633057 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 9.30406379699707 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.560447931289673 ms
		out projection: 0.2314240038394928 ms
	self-attn: 6.511616230010986 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 9.280511856079102 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.543360233306885 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.30406379699707 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.54201602935791 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.488063812255859 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 9.293824195861816 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5543038845062256 ms
		out projection: 0.23654399812221527 ms
	self-attn: 6.53004789352417 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.280511856079102 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5389440059661865 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.520832061767578 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.30406379699707 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5430400371551514 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.494207859039307 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 9.299967765808105 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.555327892303467 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.508543968200684 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.256959915161133 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.5379199981689453 ms
		out projection: 0.2396160066127777 ms
	self-attn: 6.523903846740723 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 9.290752410888672 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.54201602935791 ms
		out projection: 0.23449599742889404 ms
	self-attn: 6.497280120849609 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 9.299967765808105 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.556351900100708 ms
		out projection: 0.23347200453281403 ms
	self-attn: 6.511616230010986 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.264127731323242 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.54201602935791 ms
		out projection: 0.24166400730609894 ms
	self-attn: 6.526976108551025 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 9.291775703430176 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.54201602935791 ms
		out projection: 0.289792001247406 ms
	self-attn: 6.732800006866455 ms
	cross-attn: 1.3056000471115112 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 9.888768196105957 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.688447952270508 ms
		out projection: 0.25600001215934753 ms
	self-attn: 6.948863983154297 ms
	cross-attn: 1.2738560438156128 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 10.072064399719238 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 18720, 12, 128]) 
			 v: torch.Size([1, 18720, 12, 128])
		attention: 3.6659200191497803 ms
		out projection: 0.2519040107727051 ms
	self-attn: 6.8730878829956055 ms
	cross-attn: 1.240064024925232 ms
	ffn: 1.4581760168075562 ms
forward last block: 9.953280448913574 ms
##### model time: 285.2894592285156 #####
KV cache update for next block completed in 285.92 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 4 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 4 VAE decoding completed in 55.81 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 4 for sending...
‚úÖ Block 4 completed in 1621.83 ms (12 frames queued in 0.019 ms)
In loop: idx 4, current_num_frames 3, current_start_frame 12
üîÑ Processing block 5/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10751999914646149 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 4.79641580581665 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.474880218505859 ms
		out projection: 0.2836480140686035 ms
	self-attn: 19.378175735473633 ms
	cross-attn: 1.2738560438156128 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 30.081024169921875 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.19353599846363068 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.489215850830078 ms
		out projection: 0.29388800263404846 ms
	self-attn: 19.968000411987305 ms
	cross-attn: 5.421055793762207 ms
	ffn: 3.556351900100708 ms
forward previous block to here: 29.36012840270996 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 7.949312210083008 ms
		out projection: 3.8737919330596924 ms
	self-attn: 15.037440299987793 ms
	cross-attn: 1.2984319925308228 ms
	ffn: 2.787328004837036 ms
forward previous block to here: 19.500032424926758 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.1192320585250854 ms
		kv cache update (if evict): 0.24166400730609894 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 12.056575775146484 ms
		out projection: 0.37273600697517395 ms
	self-attn: 16.143360137939453 ms
	cross-attn: 1.2113920450210571 ms
	ffn: 3.788800001144409 ms
forward previous block to here: 21.565439224243164 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 7.945216178894043 ms
		kv cache update (if evict): 0.28467199206352234 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.552703857421875 ms
		out projection: 0.4915199875831604 ms
	self-attn: 18.721792221069336 ms
	cross-attn: 1.2840960025787354 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 29.304832458496094 ms
---------------- attention block ----------------
		qkv norm: 0.9728000164031982 ms
		rope: 1.1048959493637085 ms
		kv cache update (if evict): 0.23654399812221527 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.552703857421875 ms
		out projection: 4.157440185546875 ms
	self-attn: 25.160703659057617 ms
	cross-attn: 1.4888960123062134 ms
	ffn: 3.349503993988037 ms
forward previous block to here: 30.503936767578125 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0670080184936523 ms
		kv cache update (if evict): 0.19148799777030945 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 11.842559814453125 ms
		out projection: 0.2539519965648651 ms
	self-attn: 15.071231842041016 ms
	cross-attn: 4.3079681396484375 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 21.29305648803711 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 8.2042875289917 ms
		kv cache update (if evict): 0.27750399708747864 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.50764799118042 ms
		out projection: 0.24985599517822266 ms
	self-attn: 18.63987159729004 ms
	cross-attn: 1.1878399848937988 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 28.7139835357666 ms
---------------- attention block ----------------
		qkv norm: 0.9584640264511108 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.459519863128662 ms
		out projection: 0.2744320034980774 ms
	self-attn: 13.815808296203613 ms
	cross-attn: 1.1386879682540894 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 16.78540802001953 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.512767791748047 ms
		out projection: 0.24883200228214264 ms
	self-attn: 7.630847930908203 ms
	cross-attn: 1.3783040046691895 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.085824012756348 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.415487766265869 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.450623989105225 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.263551712036133 ms
---------------- attention block ----------------
		qkv norm: 0.942080020904541 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.392960071563721 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.507967948913574 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.278911590576172 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.332543849945068 ms
		out projection: 0.2519040107727051 ms
	self-attn: 7.520256042480469 ms
	cross-attn: 1.1438080072402954 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.4202241897583 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.328767776489258 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.115072250366211 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.330495834350586 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.3164801597595215 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.068991661071777 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.284736156463623 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.127360343933105 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.354047775268555 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.304192066192627 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.071040153503418 ms
---------------- attention block ----------------
		qkv norm: 1.018880009651184 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.322303771972656 ms
		out projection: 0.24883200228214264 ms
	self-attn: 7.511040210723877 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.33523178100586 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.370751857757568 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 10.176511764526367 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.329792022705078 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.1212158203125 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.329472064971924 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.304192066192627 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.084351539611816 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.274496078491211 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.054656028747559 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.314432144165039 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.085375785827637 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.315135955810547 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.320576190948486 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.089471817016602 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.315135955810547 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.315455913543701 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.072064399719238 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31001615524292 ms
		out projection: 0.2457599937915802 ms
	self-attn: 7.285759925842285 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.041343688964844 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.314112186431885 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.273471832275391 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.036224365234375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.301119804382324 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 10.051584243774414 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.2724480628967285 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 10.044416427612305 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.28985595703125 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.418239951133728 ms
forward last block: 10.027008056640625 ms
##### model time: 445.1286926269531 #####
	 Transformer denoising step 1/4 completed in 445.63 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.08908800035715103 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.326399803161621 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.301119804382324 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.084351539611816 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.33081579208374 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.116095542907715 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.367680072784424 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.143744468688965 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.2611199915409088 ms
	self-attn: 7.354368209838867 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.155008316040039 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.318528175354004 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.090496063232422 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3581438064575195 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.326720237731934 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.12019157409668 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.24166400730609894 ms
	self-attn: 7.332863807678223 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 10.168319702148438 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.325376033782959 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.355391979217529 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.105855941772461 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.338687896728516 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.35641622543335 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.151935577392578 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.317183971405029 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.283711910247803 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.025983810424805 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.28985595703125 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.105855941772461 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.284736156463623 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.04748821258545 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.276544094085693 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.063872337341309 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.317183971405029 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.267327785491943 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.089471817016602 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.319231986999512 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.282688140869141 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.051584243774414 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.320256233215332 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.319551944732666 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.080256462097168 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.283711910247803 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.018815994262695 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31820821762085 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.283711910247803 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 10.016768455505371 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31820821762085 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.266304016113281 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.02188777923584 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.326399803161621 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.2867841720581055 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.04748821258545 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.326399803161621 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.29088020324707 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.027008056640625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.22835199534893036 ms
	self-attn: 7.267327785491943 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.001407623291016 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.347904205322266 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.3072638511657715 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 10.093567848205566 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.34822416305542 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 10.158080101013184 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.321280002593994 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.34003210067749 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.091520309448242 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.315135955810547 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.293951988220215 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.041343688964844 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.319231986999512 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.294976234436035 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.03212833404541 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.324351787567139 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.310336112976074 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.085375785827637 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.326399803161621 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.264256000518799 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.008576393127441 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.329472064971924 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.262207984924316 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4120960235595703 ms
forward last block: 10.067968368530273 ms
##### model time: 305.8841552734375 #####
	 Transformer denoising step 2/4 completed in 306.36 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.353024005889893 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.297023773193359 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.075136184692383 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.319231986999512 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.267327785491943 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 10.046463966369629 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.341760158538818 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.299071788787842 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.044416427612305 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.327744007110596 ms
	cross-attn: 1.0004479885101318 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.098688125610352 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.320256233215332 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.312384128570557 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.046463966369629 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.321280002593994 ms
		out projection: 0.24268800020217896 ms
	self-attn: 7.315455913543701 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.12224006652832 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.319231986999512 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.276544094085693 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.027008056640625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.333568096160889 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.29088020324707 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.094592094421387 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.306240081787109 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 10.108927726745605 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.323328018188477 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.28985595703125 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.117119789123535 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.323328018188477 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.358463764190674 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.133503913879395 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.329472064971924 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.3164801597595215 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.067968368530273 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.24166400730609894 ms
	self-attn: 7.291903972625732 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.04032039642334 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.319231986999512 ms
		out projection: 0.24883200228214264 ms
	self-attn: 7.287807941436768 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.033151626586914 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.421631813049316 ms
		out projection: 0.2693119943141937 ms
	self-attn: 7.469056129455566 ms
	cross-attn: 1.2564480304718018 ms
	ffn: 1.4929920434951782 ms
forward previous block to here: 10.580991744995117 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.429823875427246 ms
		out projection: 0.25702399015426636 ms
	self-attn: 7.666687965393066 ms
	cross-attn: 1.2421120405197144 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.685440063476562 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.325376033782959 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.328767776489258 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.1212158203125 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.324351787567139 ms
		out projection: 0.24268800020217896 ms
	self-attn: 7.334911823272705 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.097663879394531 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.321599960327148 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.088447570800781 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.309311866760254 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.071040153503418 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.24166400730609894 ms
	self-attn: 7.350272178649902 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.13145637512207 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.324351787567139 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.297023773193359 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.064895629882812 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.337984085083008 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.084351539611816 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.322303771972656 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.297023773193359 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.086400032043457 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.2396160066127777 ms
	self-attn: 7.335936069488525 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.101759910583496 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.344831943511963 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.3318400382995605 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.109951972961426 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.349952220916748 ms
		out projection: 0.24780799448490143 ms
	self-attn: 7.343103885650635 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 10.21132755279541 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.338687896728516 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.385087966918945 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.206208229064941 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.324672222137451 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.099712371826172 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.325376033782959 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.3318400382995605 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4213119745254517 ms
forward last block: 10.126336097717285 ms
##### model time: 307.5625 #####
	 Transformer denoising step 3/4 completed in 308.04 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.348927974700928 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.333888053894043 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 10.117119789123535 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.34003210067749 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.156031608581543 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.314432144165039 ms
	cross-attn: 1.2288000583648682 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.292223930358887 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31820821762085 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.285759925842285 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.054656028747559 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31820821762085 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.285759925842285 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 10.051584243774414 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.331520080566406 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.301119804382324 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.052607536315918 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.330495834350586 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.28166389465332 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.036224365234375 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.332543849945068 ms
		out projection: 0.24985599517822266 ms
	self-attn: 7.308288097381592 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.057727813720703 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.326399803161621 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.274496078491211 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.051584243774414 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.280640125274658 ms
	cross-attn: 1.001471996307373 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.007552146911621 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.315135955810547 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.288832187652588 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.037247657775879 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.346879959106445 ms
		out projection: 0.24985599517822266 ms
	self-attn: 7.353343963623047 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.148863792419434 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.324351787567139 ms
		out projection: 0.2467840015888214 ms
	self-attn: 7.3512959480285645 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.172415733337402 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.336639881134033 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.34003210067749 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.115072250366211 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.338687896728516 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.34822416305542 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 10.177536010742188 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.324351787567139 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.284736156463623 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.055680274963379 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.320256233215332 ms
		out projection: 0.22937600314617157 ms
	self-attn: 7.2867841720581055 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.062848091125488 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.287807941436768 ms
	cross-attn: 0.9922559857368469 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.01369571685791 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.315135955810547 ms
		out projection: 0.22937600314617157 ms
	self-attn: 7.2775678634643555 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.041343688964844 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.314112186431885 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.265279769897461 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.0065279006958 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.317183971405029 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.249919891357422 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.004480361938477 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.312064170837402 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.287807941436768 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.019840240478516 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.315135955810547 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.238656044006348 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 9.995264053344727 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.22937600314617157 ms
	self-attn: 7.268352031707764 ms
	cross-attn: 1.0004479885101318 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 10.012672424316406 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.22937600314617157 ms
	self-attn: 7.28166389465332 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.01369571685791 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13209599256515503 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.325376033782959 ms
		out projection: 0.22835199534893036 ms
	self-attn: 7.271423816680908 ms
	cross-attn: 1.0024960041046143 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 9.996288299560547 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13209599256515503 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.324351787567139 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.255040168762207 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 9.984000205993652 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.322303771972656 ms
		out projection: 0.22835199534893036 ms
	self-attn: 7.245823860168457 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 9.988096237182617 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.335616111755371 ms
		out projection: 0.2457599937915802 ms
	self-attn: 7.276544094085693 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.01369571685791 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.250944137573242 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4192639589309692 ms
forward last block: 9.990143775939941 ms
##### model time: 305.29638671875 #####
	 Transformer final denoising step 4/4 completed in 305.77 ms
‚ö° Block 5 denoising completed in 1367.89 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09932799637317657 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3581438064575195 ms
		out projection: 0.24883200228214264 ms
	self-attn: 7.371776103973389 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.225664138793945 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.339712142944336 ms
		out projection: 0.23552000522613525 ms
	self-attn: 7.34003210067749 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 10.190848350524902 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.320256233215332 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.318528175354004 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.095616340637207 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.323328018188477 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.29088020324707 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.03007984161377 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31820821762085 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.276544094085693 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.037247657775879 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.314112186431885 ms
		out projection: 0.23756800591945648 ms
	self-attn: 7.293951988220215 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.03007984161377 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.319231986999512 ms
		out projection: 0.23654399812221527 ms
	self-attn: 7.256063938140869 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.02188777923584 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.296000003814697 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.036224365234375 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.269375801086426 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 10.00550365447998 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.267327785491943 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.012672424316406 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.24780799448490143 ms
	self-attn: 7.28166389465332 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 10.01369571685791 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.326399803161621 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.258111953735352 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.0065279006958 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.29088020324707 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.04032039642334 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.314112186431885 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.243775844573975 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 9.991168022155762 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.340735912322998 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.325695991516113 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.090496063232422 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.325376033782959 ms
		out projection: 0.23449599742889404 ms
	self-attn: 7.326720237731934 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 10.101759910583496 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3284478187561035 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.328767776489258 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.082304000854492 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.322303771972656 ms
		out projection: 0.24063999950885773 ms
	self-attn: 7.323647975921631 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.107904434204102 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.3376641273498535 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.319551944732666 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.092543601989746 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.334591865539551 ms
		out projection: 0.23347200453281403 ms
	self-attn: 7.312384128570557 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.092543601989746 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.330495834350586 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.266304016113281 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.027008056640625 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9697279930114746 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.315135955810547 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.246848106384277 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.027008056640625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.294976234436035 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.04851245880127 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31820821762085 ms
		out projection: 0.22937600314617157 ms
	self-attn: 7.273471832275391 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.057727813720703 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.31820821762085 ms
		out projection: 0.2314240038394928 ms
	self-attn: 7.292928218841553 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.04032039642334 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.250944137573242 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.04748821258545 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.316160202026367 ms
		out projection: 0.2385919988155365 ms
	self-attn: 7.2867841720581055 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 10.034175872802734 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.314112186431885 ms
		out projection: 0.23244799673557281 ms
	self-attn: 7.251967906951904 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 10.012672424316406 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.325376033782959 ms
		out projection: 0.2303999960422516 ms
	self-attn: 7.276544094085693 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 10.009599685668945 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 23400, 12, 128]) 
			 v: torch.Size([1, 23400, 12, 128])
		attention: 4.327424049377441 ms
		out projection: 0.22835199534893036 ms
	self-attn: 7.250944137573242 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4090240001678467 ms
forward last block: 9.985024452209473 ms
##### model time: 305.1673583984375 #####
KV cache update for next block completed in 305.68 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 5 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 5 VAE decoding completed in 55.10 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 5 for sending...
‚úÖ Block 5 completed in 1750.07 ms (12 frames queued in 0.018 ms)
In loop: idx 5, current_num_frames 3, current_start_frame 15
üîÑ Processing block 6/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11878400295972824 ms
---------------- attention block ----------------
		qkv norm: 0.8980479836463928 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.20684799551963806 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 8.942591667175293 ms
		out projection: 0.2693119943141937 ms
	self-attn: 19.799039840698242 ms
	cross-attn: 1.3056000471115112 ms
	ffn: 1.4909440279006958 ms
forward previous block to here: 30.323711395263672 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.263360023498535 ms
		out projection: 0.35839998722076416 ms
	self-attn: 20.692991256713867 ms
	cross-attn: 1.6629760265350342 ms
	ffn: 4.205567836761475 ms
forward previous block to here: 30.944255828857422 ms
---------------- attention block ----------------
		qkv norm: 0.9113600254058838 ms
		rope: 8.377344131469727 ms
		kv cache update (if evict): 3.985408067703247 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.368832111358643 ms
		out projection: 0.3041279911994934 ms
	self-attn: 20.04070472717285 ms
	cross-attn: 1.5923199653625488 ms
	ffn: 9.061375617980957 ms
forward previous block to here: 31.107072830200195 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 3.0597119331359863 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.312511920928955 ms
		out projection: 0.30003198981285095 ms
	self-attn: 19.780607223510742 ms
	cross-attn: 5.5562238693237305 ms
	ffn: 3.930111885070801 ms
forward previous block to here: 29.928447723388672 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0813440084457397 ms
		kv cache update (if evict): 0.23449599742889404 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 9.374719619750977 ms
		out projection: 0.3389439880847931 ms
	self-attn: 20.242431640625 ms
	cross-attn: 1.4704639911651611 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 31.464448928833008 ms
---------------- attention block ----------------
		qkv norm: 0.9093120098114014 ms
		rope: 1.0485759973526 ms
		kv cache update (if evict): 0.2027519941329956 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.274623870849609 ms
		out projection: 0.3307519853115082 ms
	self-attn: 20.518911361694336 ms
	cross-attn: 5.292031764984131 ms
	ffn: 2.8098559379577637 ms
forward previous block to here: 29.683712005615234 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 8.283136367797852 ms
		out projection: 0.25702399015426636 ms
	self-attn: 11.438079833984375 ms
	cross-attn: 4.86297607421875 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 19.581951141357422 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 12.823552131652832 ms
		out projection: 0.2590720057487488 ms
	self-attn: 15.949824333190918 ms
	cross-attn: 1.120255947113037 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 21.352447509765625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.144576072692871 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.157183647155762 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.966015815734863 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.101887702941895 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.895359992980957 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.10086441040039 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.855423927307129 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.4116480052471161 ms
	self-attn: 8.463359832763672 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.263999938964844 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1025919914245605 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.111104011535645 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.872832298278809 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.097472190856934 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.062975883483887 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.901503562927246 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.096447944641113 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.330240249633789 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.163647651672363 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.142848014831543 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.914815902709961 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.212160110473633 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.274944305419922 ms
	cross-attn: 1.1100159883499146 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.15443229675293 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.138432025909424 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.224767684936523 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.076607704162598 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.156160354614258 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.953727722167969 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.143872261047363 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.936320304870605 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.11520004272461 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.884096145629883 ms
---------------- attention block ----------------
		qkv norm: 0.8744959831237793 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.134655952453613 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.9486083984375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.132608413696289 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.991616249084473 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.143872261047363 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.951680183410645 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.125439643859863 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.925056457519531 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.141823768615723 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.959872245788574 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.27955201268196106 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.246975898742676 ms
		out projection: 0.26521599292755127 ms
	self-attn: 8.522751808166504 ms
	cross-attn: 1.2769279479980469 ms
	ffn: 1.4796799421310425 ms
forward previous block to here: 11.64799976348877 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0792959928512573 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.2008957862854 ms
		out projection: 0.26419198513031006 ms
	self-attn: 8.50227165222168 ms
	cross-attn: 1.2625919580459595 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 11.611136436462402 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.060863971710205 ms
		kv cache update (if evict): 0.19251200556755066 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.192704200744629 ms
		out projection: 0.25600001215934753 ms
	self-attn: 8.478719711303711 ms
	cross-attn: 1.2492799758911133 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 11.600895881652832 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.1884160041809082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.208064079284668 ms
		out projection: 0.26214399933815 ms
	self-attn: 8.481792449951172 ms
	cross-attn: 1.230847954750061 ms
	ffn: 1.4725120067596436 ms
forward last block: 11.570176124572754 ms
##### model time: 475.7637023925781 #####
	 Transformer denoising step 1/4 completed in 476.35 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.12083200365304947 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.222400188446045 ms
		out projection: 0.25600001215934753 ms
	self-attn: 8.52889633178711 ms
	cross-attn: 1.2267520427703857 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 11.606016159057617 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0577919483184814 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.234687805175781 ms
		out projection: 0.2734079957008362 ms
	self-attn: 8.489983558654785 ms
	cross-attn: 1.2503039836883545 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 11.591679573059082 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.228544235229492 ms
		out projection: 0.26214399933815 ms
	self-attn: 8.507391929626465 ms
	cross-attn: 1.2810239791870117 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 11.66540813446045 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.204991817474365 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.470527648925781 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.290623664855957 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.12441635131836 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.933247566223145 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.128512382507324 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 10.93222427368164 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.122367858886719 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.888192176818848 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.103616237640381 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.084480285644531 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.851327896118164 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.126463890075684 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.93222427368164 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.09984016418457 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.886143684387207 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.134335994720459 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.120320320129395 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.916864395141602 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.130559921264648 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.905599594116211 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.095744132995605 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 10.92198371887207 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.114175796508789 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.911744117736816 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.133312225341797 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.088576316833496 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 10.865663528442383 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.083456039428711 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.8472318649292 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.09164810180664 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.93222427368164 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.150015830993652 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 10.973183631896973 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.121343612670898 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.896384239196777 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.146944046020508 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.919936180114746 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.125120162963867 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.101887702941895 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.869759559631348 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.09779167175293 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.884096145629883 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.095744132995605 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.874879837036133 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.136704444885254 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 10.972160339355469 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.11520004272461 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.924032211303711 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.131584167480469 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.92300796508789 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.083456039428711 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.84108829498291 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.093695640563965 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.895359992980957 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.10086441040039 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 10.881024360656738 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.129216194152832 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.113151550292969 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4223359823226929 ms
forward last block: 10.880000114440918 ms
##### model time: 333.61920166015625 #####
	 Transformer denoising step 2/4 completed in 334.10 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.117247581481934 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.92198371887207 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.11520004272461 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.891263961791992 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.154111862182617 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.951680183410645 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.125439643859863 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.907648086547852 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1025919914245605 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.117247581481934 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.953727722167969 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.114175796508789 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.898431777954102 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.143551826477051 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.156160354614258 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.954751968383789 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.135680198669434 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.934271812438965 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.139776229858398 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.019264221191406 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.171520233154297 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 10.988544464111328 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.135359764099121 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.155136108398438 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.94758415222168 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.117247581481934 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.900480270385742 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.130559921264648 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.949631690979004 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1077117919921875 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.112128257751465 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.909695625305176 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.133312225341797 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.133631706237793 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.911744117736816 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.285183906555176 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.068415641784668 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.105663776397705 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.083456039428711 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 10.893312454223633 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.086527824401855 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.855423927307129 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.094719886779785 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.876928329467773 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.114880084991455 ms
		out projection: 0.25600001215934753 ms
	self-attn: 8.133631706237793 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.891263961791992 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.08243179321289 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.883071899414062 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.104960441589355 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.892288208007812 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.114880084991455 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.084480285644531 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.845184326171875 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.102911949157715 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.906623840332031 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.26521599292755127 ms
	self-attn: 8.1725435256958 ms
	cross-attn: 1.1304960250854492 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 11.090944290161133 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.138432025909424 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.183808326721191 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.012096405029297 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.113151550292969 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.904576301574707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.121024131774902 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.12441635131836 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.909695625305176 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.1080322265625 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.898431777954102 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.09984016418457 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4213119745254517 ms
forward last block: 10.872832298278809 ms
##### model time: 331.4646911621094 #####
	 Transformer denoising step 3/4 completed in 331.94 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.18995189666748 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.971136093139648 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.085503578186035 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.867712020874023 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.09062385559082 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.863615989685059 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.105663776397705 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.10086441040039 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.859519958496094 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.120320320129395 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.03052806854248 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.214208126068115 ms
		out projection: 0.26009601354599 ms
	self-attn: 8.526847839355469 ms
	cross-attn: 1.257472038269043 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 11.63366413116455 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0721280574798584 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.20908784866333 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.522751808166504 ms
	cross-attn: 1.2472319602966309 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 11.610112190246582 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.192704200744629 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.577024459838867 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.473919868469238 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11897611618042 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.129535675048828 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.919936180114746 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.10905647277832 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 10.942463874816895 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.11078405380249 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.161279678344727 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.0315523147583 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.21913599967956543 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.129216194152832 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.236031532287598 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.03769588470459 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1220479011535645 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.158207893371582 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.977279663085938 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.125120162963867 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.138751983642578 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.928128242492676 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.150015830993652 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.951680183410645 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.125439643859863 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 10.916864395141602 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.126143932342529 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.201215744018555 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.008000373840332 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.093695640563965 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.859519958496094 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.092672348022461 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 10.878975868225098 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.097472190856934 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.084480285644531 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.853376388549805 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.100543975830078 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.071167945861816 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 10.809344291687012 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.097472190856934 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.072192192077637 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.855423927307129 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.129535675048828 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.916864395141602 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.11622428894043 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.863615989685059 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.124095916748047 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.077312469482422 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 10.826751708984375 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.096447944641113 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.09984016418457 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.845184326171875 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.057855606079102 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 10.81446361541748 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.067071914672852 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.844160079956055 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.141503810882568 ms
		out projection: 0.25804799795150757 ms
	self-attn: 8.208383560180664 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 11.092991828918457 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.123072147369385 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.18892765045166 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4284800291061401 ms
forward last block: 10.977279663085938 ms
##### model time: 333.1041259765625 #####
	 Transformer final denoising step 4/4 completed in 333.59 ms
‚ö° Block 6 denoising completed in 1477.96 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.177343845367432 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.237055778503418 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.02847957611084 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.173247814178467 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.163328170776367 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.944512367248535 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.136384010314941 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.130559921264648 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.918911933898926 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1179518699646 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.118271827697754 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 10.915840148925781 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1169281005859375 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.111104011535645 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 10.896384239196777 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.100543975830078 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.066047668457031 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.843135833740234 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.10598373413086 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.907648086547852 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1025919914245605 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.077312469482422 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 10.875904083251953 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.119999885559082 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.093695640563965 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.85747241973877 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.101568222045898 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.112128257751465 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.877951622009277 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.105663776397705 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.118271827697754 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 10.93017578125 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.101568222045898 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.09062385559082 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 10.895359992980957 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.128191947937012 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.102911949157715 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 10.881024360656738 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.10975980758667 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.114175796508789 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 10.892288208007812 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.083456039428711 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 10.874879837036133 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.113855838775635 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.093695640563965 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 10.874879837036133 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.069120407104492 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 10.84006404876709 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.114880084991455 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.1080322265625 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.907648086547852 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.115903854370117 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.10700798034668 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.905599594116211 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.1220479011535645 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.138751983642578 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.942463874816895 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.112832069396973 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.110079765319824 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.878975868225098 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.106688022613525 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.102911949157715 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 10.903552055358887 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.111807823181152 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.09984016418457 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 10.890239715576172 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.108736038208008 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.118271827697754 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 10.872832298278809 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.104640007019043 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.068096160888672 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 10.82470417022705 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.100543975830078 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.092672348022461 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 10.905599594116211 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.105663776397705 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.156160354614258 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 10.979328155517578 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.163008213043213 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.225791931152344 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.020288467407227 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.097472190856934 ms
		out projection: 0.2682879865169525 ms
	self-attn: 8.22374439239502 ms
	cross-attn: 1.193984031677246 ms
	ffn: 1.5124479532241821 ms
forward previous block to here: 11.364352226257324 ms
---------------- attention block ----------------
		qkv norm: 0.9082880020141602 ms
		rope: 1.0782719850540161 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 28080, 12, 128]) 
			 v: torch.Size([1, 28080, 12, 128])
		attention: 5.114880084991455 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.557567596435547 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4243839979171753 ms
forward last block: 11.369471549987793 ms
##### model time: 331.6807556152344 #####
KV cache update for next block completed in 332.25 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 6 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 6 VAE decoding completed in 54.97 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 6 for sending...
‚úÖ Block 6 completed in 1883.51 ms (12 frames queued in 0.018 ms)
In loop: idx 6, current_num_frames 3, current_start_frame 18
üîÑ Processing block 7/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.14643199741840363 ms
---------------- attention block ----------------
		qkv norm: 8.19711971282959 ms
		rope: 1.2503039836883545 ms
		kv cache update (if evict): 0.2027519941329956 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.1972479820251465 ms
		out projection: 0.3184640109539032 ms
	self-attn: 32.32255935668945 ms
	cross-attn: 1.3905919790267944 ms
	ffn: 3.7253119945526123 ms
forward previous block to here: 37.9760627746582 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 4.635647773742676 ms
		kv cache update (if evict): 0.19353599846363068 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.051839828491211 ms
		out projection: 0.3614720106124878 ms
	self-attn: 27.419647216796875 ms
	cross-attn: 1.427456021308899 ms
	ffn: 3.2368640899658203 ms
forward previous block to here: 36.0898551940918 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 8.092672348022461 ms
		kv cache update (if evict): 4.0663042068481445 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.022143840789795 ms
		out projection: 0.2826240062713623 ms
	self-attn: 27.75654411315918 ms
	cross-attn: 5.101568222045898 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 36.34892654418945 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.19763199985027313 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.10483169555664 ms
		out projection: 0.30720001459121704 ms
	self-attn: 21.135360717773438 ms
	cross-attn: 8.476672172546387 ms
	ffn: 4.6233601570129395 ms
forward previous block to here: 34.739200592041016 ms
---------------- attention block ----------------
		qkv norm: 3.4949119091033936 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.2396160066127777 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.053887844085693 ms
		out projection: 0.3420160114765167 ms
	self-attn: 15.014911651611328 ms
	cross-attn: 5.297152042388916 ms
	ffn: 2.8190720081329346 ms
forward previous block to here: 23.623680114746094 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.19251200556755066 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.784064292907715 ms
		out projection: 2.540544033050537 ms
	self-attn: 19.628032684326172 ms
	cross-attn: 1.462272047996521 ms
	ffn: 1.5237120389938354 ms
forward previous block to here: 30.361600875854492 ms
---------------- attention block ----------------
		qkv norm: 1.031167984008789 ms
		rope: 3.7027840614318848 ms
		kv cache update (if evict): 0.20377600193023682 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.039552211761475 ms
		out projection: 0.25600001215934753 ms
	self-attn: 15.805439949035645 ms
	cross-attn: 1.1694079637527466 ms
	ffn: 1.4940160512924194 ms
forward previous block to here: 18.999296188354492 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.945343971252441 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.250816345214844 ms
	cross-attn: 1.082368016242981 ms
	ffn: 1.6517119407653809 ms
forward previous block to here: 12.464127540588379 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.18943999707698822 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.225215911865234 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.26035213470459 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.233407974243164 ms
	cross-attn: 1.2513279914855957 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.569600105285645 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.19046400487422943 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.201663970947266 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.188672065734863 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.18835163116455 ms
	cross-attn: 1.2257280349731445 ms
	ffn: 1.5882240533828735 ms
forward previous block to here: 12.53990364074707 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.186304092407227 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.157952308654785 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.148415565490723 ms
	cross-attn: 1.2031999826431274 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.543999671936035 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.149439811706543 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.120063781738281 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.945343971252441 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.156607627868652 ms
	cross-attn: 1.1929600238800049 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 12.298239707946777 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.2017280012369156 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.18835163116455 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.16921615600586 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.3031040132045746 ms
	self-attn: 9.279487609863281 ms
	cross-attn: 1.2175359725952148 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.46617603302002 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.18534399569034576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.943295955657959 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.29587173461914 ms
	cross-attn: 1.1212799549102783 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.353535652160645 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0383360385894775 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.933055877685547 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.21190357208252 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.215295791625977 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.208831787109375 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.174336433410645 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.18227200210094452 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2662400007247925 ms
	self-attn: 9.19654369354248 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.15078353881836 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.17919999361038208 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.158656120300293 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.104703903198242 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.156607627868652 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.094464302062988 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.187328338623047 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.125184059143066 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.148415565490723 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.082176208496094 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.18227200210094452 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2826240062713623 ms
	self-attn: 9.360383987426758 ms
	cross-attn: 1.2912640571594238 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.680191993713379 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.1802240014076233 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.954559803009033 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.247743606567383 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.338175773620605 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.156607627868652 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.113920211791992 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.20684799551963806 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.192447662353516 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.45305597782135 ms
forward last block: 12.166144371032715 ms
##### model time: 505.40032958984375 #####
	 Transformer denoising step 1/4 completed in 505.94 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.131071999669075 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9607038497924805 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.200639724731445 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.167167663574219 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.22316837310791 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4950400590896606 ms
forward previous block to here: 12.262399673461914 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.2590720057487488 ms
	self-attn: 9.169919967651367 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.16102409362793 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.155584335327148 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 12.15283203125 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.165823936462402 ms
	cross-attn: 1.1048959493637085 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.188672065734863 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.136128425598145 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 12.100607872009277 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.181183815002441 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.180480003356934 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.20473575592041 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.220416069030762 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.2662400007247925 ms
	self-attn: 9.208831787109375 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.204031944274902 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.134079933166504 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.066816329956055 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.168895721435547 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.101632118225098 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.185279846191406 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.121088027954102 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.163776397705078 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 12.113920211791992 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.116671562194824 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.094464302062988 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.13203239440918 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.072959899902344 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.104384422302246 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.03609561920166 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.12281608581543 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.066816329956055 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.118720054626465 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.083200454711914 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.92793607711792 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.144319534301758 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.116991996765137 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.964799880981445 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.175040245056152 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.132351875305176 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.153535842895508 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.095487594604492 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.18432000279426575 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.152511596679688 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 12.130304336547852 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.160703659057617 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.125184059143066 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.119744300842285 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.084223747253418 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.1146240234375 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.105728149414062 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.12281608581543 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.104703903198242 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.117695808410645 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 12.102656364440918 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.154560089111328 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.124159812927246 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.163776397705078 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.114944458007812 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.125887870788574 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4520319700241089 ms
forward last block: 12.073984146118164 ms
##### model time: 368.29901123046875 #####
	 Transformer denoising step 2/4 completed in 368.88 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.1228799968957901 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.92793607711792 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.210880279541016 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.2357759475708 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.171968460083008 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.157952308654785 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.161727905273438 ms
	cross-attn: 1.1018240451812744 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.175359725952148 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.959680080413818 ms
		out projection: 0.28467199206352234 ms
	self-attn: 9.324543952941895 ms
	cross-attn: 1.2144639492034912 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.47539234161377 ms
---------------- attention block ----------------
		qkv norm: 0.886784017086029 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9402241706848145 ms
		out projection: 0.32870399951934814 ms
	self-attn: 9.464832305908203 ms
	cross-attn: 1.7192959785461426 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 13.15225601196289 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.26009601354599 ms
	self-attn: 8.993791580200195 ms
	cross-attn: 1.1253759860992432 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.006400108337402 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.4116480052471161 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.206463813781738 ms
		out projection: 0.2734079957008362 ms
	self-attn: 9.646080017089844 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4878720045089722 ms
forward previous block to here: 12.58291244506836 ms
---------------- attention block ----------------
		qkv norm: 0.9267200231552124 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.365504264831543 ms
	cross-attn: 1.1376639604568481 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.423168182373047 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.372352123260498 ms
		out projection: 0.7475200295448303 ms
	self-attn: 10.490880012512207 ms
	cross-attn: 1.5482879877090454 ms
	ffn: 1.5534080266952515 ms
forward previous block to here: 14.054400444030762 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0751999616622925 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.253888130187988 ms
	cross-attn: 1.1397119760513306 ms
	ffn: 1.556480050086975 ms
forward previous block to here: 12.45900821685791 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0794878005981445 ms
		out projection: 0.35225600004196167 ms
	self-attn: 9.539584159851074 ms
	cross-attn: 1.2584960460662842 ms
	ffn: 1.5144959688186646 ms
forward previous block to here: 12.787712097167969 ms
---------------- attention block ----------------
		qkv norm: 1.026047945022583 ms
		rope: 1.3547519445419312 ms
		kv cache update (if evict): 0.2027519941329956 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.148096084594727 ms
		out projection: 0.25702399015426636 ms
	self-attn: 10.043392181396484 ms
	cross-attn: 1.1479040384292603 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 13.117440223693848 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.3557759523391724 ms
		kv cache update (if evict): 0.18636800348758698 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.089727878570557 ms
		out projection: 0.31334400177001953 ms
	self-attn: 9.83142375946045 ms
	cross-attn: 1.1284480094909668 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.872703552246094 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.941247940063477 ms
		out projection: 0.2590720057487488 ms
	self-attn: 9.285632133483887 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 12.27673625946045 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.17919999361038208 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.20678424835205 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 12.233728408813477 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.155584335327148 ms
	cross-attn: 1.1315200328826904 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.190719604492188 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.116671562194824 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 12.064767837524414 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.1146240234375 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.088319778442383 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.17100800573825836 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.12179183959961 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.076031684875488 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.156607627868652 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.111871719360352 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.20070399343967438 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.170944213867188 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.123135566711426 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.1146240234375 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.078080177307129 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.134079933166504 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 12.04531192779541 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.117695808410645 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.081151962280273 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.704319953918457 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.864448070526123 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.805376052856445 ms
	cross-attn: 1.0004479885101318 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.560959815979004 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13209599256515503 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.943295955657959 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.1274240016937256 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9707520008087158 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.957632064819336 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 11.622400283813477 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.868544101715088 ms
		out projection: 0.22732800245285034 ms
	self-attn: 8.79923152923584 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.54150390625 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.868544101715088 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.82483196258545 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4079999923706055 ms
forward last block: 11.569151878356934 ms
##### model time: 371.989501953125 #####
	 Transformer denoising step 3/4 completed in 372.73 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09625600278377533 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.851455688476562 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.627519607543945 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.82688045501709 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4079999923706055 ms
forward previous block to here: 11.589632034301758 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9707520008087158 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.580415725708008 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.847359657287598 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.5073280334472656 ms
forward previous block to here: 11.888640403747559 ms
---------------- attention block ----------------
		qkv norm: 0.8785920143127441 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.309184074401855 ms
	cross-attn: 1.2615679502487183 ms
	ffn: 1.4663679599761963 ms
forward previous block to here: 12.447744369506836 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.956607818603516 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.22316837310791 ms
	cross-attn: 1.2503039836883545 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 12.338175773620605 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.1740799993276596 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.015999794006348 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.279487609863281 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 12.120063781738281 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.743231773376465 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.837120056152344 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.659263610839844 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.655167579650879 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.64083194732666 ms
---------------- attention block ----------------
		qkv norm: 0.9666560292243958 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.208831787109375 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.974656105041504 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.3041279911994934 ms
	self-attn: 9.027584075927734 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.852800369262695 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.944640159606934 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.744256019592285 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.760640144348145 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.056256294250488 ms
	cross-attn: 1.1110399961471558 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 11.996159553527832 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.057279586791992 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 11.987968444824219 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.086976051330566 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 11.988991737365723 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.12281608581543 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.015616416931152 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.27750399708747864 ms
	self-attn: 9.103360176086426 ms
	cross-attn: 1.1100159883499146 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.058624267578125 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.944320201873779 ms
		out projection: 0.2908160090446472 ms
	self-attn: 9.151488304138184 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 12.092415809631348 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.12179183959961 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.041215896606445 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.27750399708747864 ms
	self-attn: 9.11359977722168 ms
	cross-attn: 1.077247977256775 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.051456451416016 ms
---------------- attention block ----------------
		qkv norm: 0.8806399703025818 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.120767593383789 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.098560333251953 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.18534399569034576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02726411819458 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.430015563964844 ms
	cross-attn: 1.1079679727554321 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.45900821685791 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0844160318374634 ms
		kv cache update (if evict): 0.2099200040102005 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.056960105895996 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.401344299316406 ms
	cross-attn: 1.2544000148773193 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.497920036315918 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.00710391998291 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 11.907072067260742 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.1028480529785156 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.067200183868408 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.361408233642578 ms
	cross-attn: 1.1601920127868652 ms
	ffn: 1.5278079509735107 ms
forward last block: 12.44876766204834 ms
##### model time: 363.72174072265625 #####
	 Transformer final denoising step 4/4 completed in 364.30 ms
‚ö° Block 7 denoising completed in 1616.43 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.130048006772995 ms
---------------- attention block ----------------
		qkv norm: 0.92876797914505 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.941247940063477 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.323519706726074 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.168191909790039 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.710463523864746 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.912896156311035 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8796160221099854 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.773951530456543 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.942591667175293 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.752448081970215 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.683839797973633 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.861696243286133 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.681792259216309 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.910847663879395 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.718655586242676 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.709440231323242 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.676671981811523 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.680768013000488 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.926207542419434 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 11.727871894836426 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.661312103271484 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.659263610839844 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.672575950622559 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.8985595703125 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.706368446350098 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.737088203430176 ms
---------------- attention block ----------------
		qkv norm: 0.8837119936943054 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.964096069335938 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.905728340148926 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.697152137756348 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.903679847717285 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.719679832458496 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.667455673217773 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.706368446350098 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.697152137756348 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.694080352783203 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4284800291061401 ms
forward last block: 11.717632293701172 ms
##### model time: 355.4314270019531 #####
KV cache update for next block completed in 356.00 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 7 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 7 VAE decoding completed in 55.61 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 7 for sending...
‚úÖ Block 7 completed in 2047.63 ms (12 frames queued in 0.019 ms)
In loop: idx 7, current_num_frames 3, current_start_frame 21
üîÑ Processing block 8/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 12.092415809631348 ms
		kv cache update (if evict): 0.6717439889907837 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.042623996734619 ms
		out projection: 0.29183998703956604 ms
	self-attn: 20.938751220703125 ms
	cross-attn: 13.323264122009277 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 38.27916717529297 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 8.39475154876709 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.656319618225098 ms
		out projection: 0.29388800263404846 ms
	self-attn: 21.09440040588379 ms
	cross-attn: 1.351680040359497 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 31.0251522064209 ms
---------------- attention block ----------------
		qkv norm: 4.280320167541504 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 2.6716160774230957 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.3850240111351013 ms
	self-attn: 20.390911102294922 ms
	cross-attn: 1.3527040481567383 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 28.535808563232422 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.026047945022583 ms
		kv cache update (if evict): 8.187904357910156 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.071296215057373 ms
		out projection: 0.2754560112953186 ms
	self-attn: 17.316864013671875 ms
	cross-attn: 3.782655954360962 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 22.8853759765625 ms
---------------- attention block ----------------
		qkv norm: 7.906303882598877 ms
		rope: 5.161983966827393 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.002687931060791 ms
		out projection: 8.794112205505371 ms
	self-attn: 29.453311920166016 ms
	cross-attn: 4.972544193267822 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 37.577728271484375 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.719871997833252 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.261823654174805 ms
		out projection: 0.2764799892902374 ms
	self-attn: 17.167360305786133 ms
	cross-attn: 3.4949119091033936 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 22.50547218322754 ms
---------------- attention block ----------------
		qkv norm: 8.366080284118652 ms
		rope: 4.933631896972656 ms
		kv cache update (if evict): 0.6696959733963013 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.003712177276611 ms
		out projection: 0.2693119943141937 ms
	self-attn: 21.104639053344727 ms
	cross-attn: 5.286911964416504 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 38.40409469604492 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0455039739608765 ms
		kv cache update (if evict): 0.6635519862174988 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.979135990142822 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.650176048278809 ms
	cross-attn: 1.141759991645813 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.567551612854004 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.17574405670166 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.703424453735352 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.534784317016602 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.477120399475098 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 12.305407524108887 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.826367974281311 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.942272186279297 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.773056030273438 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.581888198852539 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.475071907043457 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.25830364227295 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.7884799838066101 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.6430082321167 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.464127540588379 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.425919532775879 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.229632377624512 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.2718080282211304 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.672703742980957 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.498944282531738 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.6635519862174988 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.435135841369629 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.233728408813477 ms
---------------- attention block ----------------
		qkv norm: 0.9523199796676636 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.6440960168838501 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.513983726501465 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.294143676757812 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.398271560668945 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.182527542114258 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.6451200246810913 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.375743865966797 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.155903816223145 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.424896240234375 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.210176467895508 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.430015563964844 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 12.25113582611084 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.366527557373047 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.179455757141113 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.367551803588867 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 12.15385627746582 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.433088302612305 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 12.225536346435547 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.4136323928833 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.191743850708008 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.6615039706230164 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.385984420776367 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.171263694763184 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.385984420776367 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.199935913085938 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.6686720252037048 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.43718433380127 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 12.229632377624512 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.407487869262695 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.228608131408691 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.367551803588867 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4315520524978638 ms
forward last block: 12.164095878601074 ms
##### model time: 508.2511291503906 #####
	 Transformer denoising step 1/4 completed in 508.74 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.673600196838379 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.877056121826172 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.659263610839844 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.857600212097168 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.636735916137695 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.949760437011719 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.713536262512207 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.662336349487305 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.636735916137695 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.913920402526855 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.696127891540527 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.672575950622559 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.720704078674316 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.922112464904785 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.719679832458496 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.879103660583496 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.851455688476562 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.64185619354248 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.857600212097168 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.637760162353516 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.637760162353516 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.837120056152344 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.643903732299805 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.706368446350098 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.653120040893555 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.850432395935059 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.610112190246582 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 11.685888290405273 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4192639589309692 ms
forward last block: 11.671551704406738 ms
##### model time: 353.6609191894531 #####
	 Transformer denoising step 2/4 completed in 354.13 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.6561918258667 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.910847663879395 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.695103645324707 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.884223937988281 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.680768013000488 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.736063957214355 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.669504165649414 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.64185619354248 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.877056121826172 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 11.696127891540527 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.913920402526855 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.879103660583496 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.667455673217773 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.63366413116455 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.63366413116455 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.887295722961426 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.681792259216309 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.887295722961426 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.861696243286133 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.629568099975586 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.672575950622559 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.952832221984863 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.811840057373047 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.693056106567383 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.862719535827637 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.6244478225708 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.681792259216309 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.967872142791748 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.039872169494629 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.852800369262695 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.691007614135742 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.857600212097168 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.636735916137695 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4161920547485352 ms
forward last block: 11.63366413116455 ms
##### model time: 353.9527587890625 #####
	 Transformer denoising step 3/4 completed in 354.44 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.694080352783203 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.903679847717285 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.723775863647461 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.3901439905166626 ms
	self-attn: 9.068544387817383 ms
	cross-attn: 1.3137919902801514 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.218367576599121 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.979135990142822 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.239551544189453 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.05452823638916 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.685888290405273 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.90982437133789 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.74732780456543 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.696127891540527 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.64799976348877 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.683839797973633 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.694080352783203 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.850432395935059 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.6428804397583 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.618304252624512 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.630592346191406 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.627519607543945 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.28672000765800476 ms
	self-attn: 9.042943954467773 ms
	cross-attn: 1.240064024925232 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.106752395629883 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.970943927764893 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.219072341918945 ms
	cross-attn: 1.2339199781417847 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.284928321838379 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9699201583862305 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.235456466674805 ms
	cross-attn: 1.2267520427703857 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 12.333056449890137 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.17203199863433838 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.23852825164795 ms
	cross-attn: 1.2124160528182983 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 12.293120384216309 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9699201583862305 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.19654369354248 ms
	cross-attn: 1.2349439859390259 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 12.26854419708252 ms
---------------- attention block ----------------
		qkv norm: 0.8908799886703491 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.979455947875977 ms
	cross-attn: 1.2369920015335083 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.990015983581543 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.646976470947266 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.879103660583496 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.613183975219727 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.622400283813477 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.847359657287598 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.589632034301758 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.590656280517578 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4243839979171753 ms
forward last block: 11.612159729003906 ms
##### model time: 357.6156311035156 #####
	 Transformer final denoising step 4/4 completed in 358.08 ms
‚ö° Block 8 denoising completed in 1577.26 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09728000313043594 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.626496315002441 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.82483196258545 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.593728065490723 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.593728065490723 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.859647750854492 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.596799850463867 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.847359657287598 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.588607788085938 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.607040405273438 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.844287872314453 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.630592346191406 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.827903747558594 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.595775604248047 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.622400283813477 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.847359657287598 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.594752311706543 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.827903747558594 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.608063697814941 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.854528427124023 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.64083194732666 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.822784423828125 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.553791999816895 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.858624458312988 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.606016159057617 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.842240333557129 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.587583541870117 ms
---------------- attention block ----------------
		qkv norm: 0.8826879858970642 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.27955201268196106 ms
	self-attn: 9.241600036621094 ms
	cross-attn: 1.1796480417251587 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.277759552001953 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.983551979064941 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.763711929321289 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.74118423461914 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.30105599761009216 ms
	self-attn: 9.102335929870605 ms
	cross-attn: 1.4417920112609863 ms
	ffn: 1.5257600545883179 ms
forward previous block to here: 12.44262409210205 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.952832221984863 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2764799892902374 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.712512016296387 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.677696228027344 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.673600196838379 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.64799976348877 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.601920127868652 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.090559959411621 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.713536262512207 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4254080057144165 ms
forward last block: 11.677696228027344 ms
##### model time: 354.1678161621094 #####
KV cache update for next block completed in 354.68 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 8 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 8 VAE decoding completed in 55.05 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 8 for sending...
‚úÖ Block 8 completed in 2005.39 ms (12 frames queued in 0.018 ms)
In loop: idx 8, current_num_frames 3, current_start_frame 24
üîÑ Processing block 9/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10956799983978271 ms
---------------- attention block ----------------
		qkv norm: 8.018943786621094 ms
		rope: 4.588543891906738 ms
		kv cache update (if evict): 0.6656000018119812 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.037504196166992 ms
		out projection: 3.9976959228515625 ms
	self-attn: 31.509504318237305 ms
	cross-attn: 1.346560001373291 ms
	ffn: 2.6163198947906494 ms
forward previous block to here: 35.83692932128906 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 7.698431968688965 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.555968284606934 ms
		out projection: 0.2969599962234497 ms
	self-attn: 20.310016632080078 ms
	cross-attn: 1.2861440181732178 ms
	ffn: 8.71833610534668 ms
forward previous block to here: 33.60153579711914 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 3.140608072280884 ms
		kv cache update (if evict): 0.7864320278167725 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.1675519943237305 ms
		out projection: 0.29286399483680725 ms
	self-attn: 14.781439781188965 ms
	cross-attn: 1.4295040369033813 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 23.052288055419922 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 7.701504230499268 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.506815910339355 ms
		out projection: 0.2949120104312897 ms
	self-attn: 20.318208694458008 ms
	cross-attn: 8.440832138061523 ms
	ffn: 4.793344020843506 ms
forward previous block to here: 34.28351974487305 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 3.220479965209961 ms
		kv cache update (if evict): 0.6676480174064636 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97811222076416 ms
		out projection: 0.32972800731658936 ms
	self-attn: 14.338047981262207 ms
	cross-attn: 4.690944194793701 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 22.717439651489258 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 4.330495834350586 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.022143840789795 ms
		out projection: 0.2887679934501648 ms
	self-attn: 20.83123207092285 ms
	cross-attn: 12.47539234161377 ms
	ffn: 1.474560022354126 ms
forward previous block to here: 37.363712310791016 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 7.643136024475098 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.500672340393066 ms
		out projection: 0.2836480140686035 ms
	self-attn: 20.205568313598633 ms
	cross-attn: 1.146880030632019 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 23.12601661682129 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.52185583114624 ms
		out projection: 0.2385919988155365 ms
	self-attn: 10.14681625366211 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.97100830078125 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.360383987426758 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.147711753845215 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9402241706848145 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.591808319091797 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 12.35968017578125 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.23244799673557281 ms
	self-attn: 9.380864143371582 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.155903816223145 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0590081214904785 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.779199600219727 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.55833625793457 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.391103744506836 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.172287940979004 ms
---------------- attention block ----------------
		qkv norm: 0.8376320004463196 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.7147520184516907 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.039552211761475 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.784319877624512 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 12.587008476257324 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.434111595153809 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 12.201984405517578 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.7936000227928162 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.689087867736816 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.493824005126953 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.346048355102539 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 12.100607872009277 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.749567985534668 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.541631698608398 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 12.277759552001953 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.6451200246810913 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.382911682128906 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 12.14463996887207 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.360383987426758 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.12723159790039 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.32966423034668 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 12.116991996765137 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23244799673557281 ms
	self-attn: 9.361408233642578 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 12.123135566711426 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.379839897155762 ms
	cross-attn: 1.0024960041046143 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 12.114944458007812 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.6440960168838501 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.377792358398438 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 12.133376121520996 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.6440960168838501 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.364480018615723 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 12.15180778503418 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.6635519862174988 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.373696327209473 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.182527542114258 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23244799673557281 ms
	self-attn: 9.343999862670898 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.123135566711426 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.22937600314617157 ms
	self-attn: 9.359359741210938 ms
	cross-attn: 0.9994239807128906 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 12.113920211791992 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23244799673557281 ms
	self-attn: 9.385984420776367 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 12.177408218383789 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.353216171264648 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.418239951133728 ms
forward last block: 12.132351875305176 ms
##### model time: 495.67742919921875 #####
	 Transformer denoising step 1/4 completed in 496.16 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09728000313043594 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.634688377380371 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.858624458312988 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.620351791381836 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.858624458312988 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.619327545166016 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.830975532531738 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 11.559935569763184 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.591679573059082 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.857600212097168 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.411072015762329 ms
forward previous block to here: 11.620351791381836 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.613183975219727 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.62342357635498 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.837120056152344 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.568127632141113 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.646976470947266 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.847359657287598 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.598848342895508 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.942272186279297 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.828927993774414 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.592703819274902 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.831999778747559 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.611136436462402 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.81868839263916 ms
	cross-attn: 0.9983999729156494 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.539456367492676 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.595775604248047 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.592703819274902 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.82585620880127 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.609087944030762 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.81868839263916 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.616255760192871 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.82688045501709 ms
	cross-attn: 0.9953280091285706 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.584511756896973 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.63161563873291 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.844287872314453 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.581439971923828 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.81049633026123 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.560959815979004 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.588607788085938 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.829952239990234 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.572223663330078 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.859647750854492 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.681792259216309 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4120960235595703 ms
forward last block: 11.593728065490723 ms
##### model time: 351.83001708984375 #####
	 Transformer denoising step 2/4 completed in 352.30 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09625600278377533 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.663359642028809 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.6561918258667 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.851455688476562 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.634688377380371 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.001471996307373 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.616255760192871 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.64799976348877 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.62342357635498 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.609087944030762 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.636735916137695 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.851455688476562 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.644927978515625 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.840191841125488 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.604991912841797 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.833024024963379 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.576319694519043 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.635711669921875 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.609087944030762 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.577343940734863 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.831999778747559 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.599871635437012 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.823807716369629 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.571200370788574 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 0.9953280091285706 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.576319694519043 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.593728065490723 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.596799850463867 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.82585620880127 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.599871635437012 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.8340482711792 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.601920127868652 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.935103893280029 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.821760177612305 ms
	cross-attn: 0.9983999729156494 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.564031600952148 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.833024024963379 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.587583541870117 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.423359990119934 ms
forward last block: 11.617280006408691 ms
##### model time: 352.1023864746094 #####
	 Transformer denoising step 3/4 completed in 352.59 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10035199671983719 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.63161563873291 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.66540813446045 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.830975532531738 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.612159729003906 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.608063697814941 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.838144302368164 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.616255760192871 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.846336364746094 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.611136436462402 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.634688377380371 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.827903747558594 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.588607788085938 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.82585620880127 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.577343940734863 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.575296401977539 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.894463539123535 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.680768013000488 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.946687698364258 ms
	cross-attn: 1.4264320135116577 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.206080436706543 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.0854400396347046 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.203712463378906 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.118016242980957 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.101311683654785 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.95315170288086 ms
---------------- attention block ----------------
		qkv norm: 0.9134079813957214 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.235456466674805 ms
	cross-attn: 1.1141120195388794 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.196864128112793 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.0936319828033447 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.08902359008789 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.93779182434082 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.1960320472717285 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 11.973631858825684 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.688960075378418 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.91596794128418 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.710463523864746 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.934399604797363 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.711487770080566 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.712512016296387 ms
---------------- attention block ----------------
		qkv norm: 0.8806399703025818 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.948736190795898 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.953856468200684 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.725824356079102 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.879103660583496 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.697152137756348 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.655167579650879 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.6746244430542 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.667455673217773 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4202879667282104 ms
forward last block: 11.684864044189453 ms
##### model time: 355.9208984375 #####
	 Transformer final denoising step 4/4 completed in 356.41 ms
‚ö° Block 9 denoising completed in 1561.73 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09728000313043594 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.938495635986328 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.691007614135742 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.673600196838379 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.694080352783203 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.74937629699707 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 11.704319953918457 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.887295722961426 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.861696243286133 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.619327545166016 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.600895881652832 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.603967666625977 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.667455673217773 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.610112190246582 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.850432395935059 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.626496315002441 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.663359642028809 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.652095794677734 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.65004825592041 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.905728340148926 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.66643238067627 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.671551704406738 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.617280006408691 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.672575950622559 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.628543853759766 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.62342357635498 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.659263610839844 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4192639589309692 ms
forward last block: 11.65721607208252 ms
##### model time: 353.3096923828125 #####
KV cache update for next block completed in 353.83 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 9 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 9 VAE decoding completed in 55.06 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 9 for sending...
‚úÖ Block 9 completed in 1988.90 ms (12 frames queued in 0.018 ms)
In loop: idx 9, current_num_frames 3, current_start_frame 27
üîÑ Processing block 10/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10035199671983719 ms
---------------- attention block ----------------
		qkv norm: 7.994368076324463 ms
		rope: 4.737023830413818 ms
		kv cache update (if evict): 0.6830080151557922 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.196096420288086 ms
		out projection: 0.3307519853115082 ms
	self-attn: 28.016639709472656 ms
	cross-attn: 7.606272220611572 ms
	ffn: 3.408895969390869 ms
forward previous block to here: 39.49875259399414 ms
---------------- attention block ----------------
		qkv norm: 0.9594879746437073 ms
		rope: 8.540160179138184 ms
		kv cache update (if evict): 4.339712142944336 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.17574405670166 ms
		out projection: 7.864319801330566 ms
	self-attn: 29.520896911621094 ms
	cross-attn: 2.1043200492858887 ms
	ffn: 1.5022079944610596 ms
forward previous block to here: 40.73984146118164 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.6717439889907837 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.989376068115234 ms
		out projection: 0.2826240062713623 ms
	self-attn: 16.528383255004883 ms
	cross-attn: 7.1249918937683105 ms
	ffn: 1.5052800178527832 ms
forward previous block to here: 25.589759826660156 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0629119873046875 ms
		kv cache update (if evict): 0.7608320116996765 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.811712265014648 ms
		out projection: 0.2734079957008362 ms
	self-attn: 22.955007553100586 ms
	cross-attn: 1.3158400058746338 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 33.9947509765625 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.6901760101318359 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.008831977844238 ms
		out projection: 0.28569599986076355 ms
	self-attn: 23.425024032592773 ms
	cross-attn: 1.6343040466308594 ms
	ffn: 3.8010880947113037 ms
forward previous block to here: 29.466623306274414 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 8.61081600189209 ms
		kv cache update (if evict): 0.78438401222229 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.2969599962234497 ms
	self-attn: 28.869632720947266 ms
	cross-attn: 5.3585920333862305 ms
	ffn: 3.4600958824157715 ms
forward previous block to here: 38.390785217285156 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 8.549375534057617 ms
		kv cache update (if evict): 4.297728061676025 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97811222076416 ms
		out projection: 0.26316800713539124 ms
	self-attn: 21.01759910583496 ms
	cross-attn: 1.4346239566802979 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 24.370176315307617 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.108160018920898 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.706496238708496 ms
	cross-attn: 1.2144639492034912 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.7457275390625 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.1857919692993164 ms
		kv cache update (if evict): 0.6737920045852661 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.957632064819336 ms
		out projection: 0.3246079981327057 ms
	self-attn: 9.999360084533691 ms
	cross-attn: 1.3803520202636719 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 13.255680084228516 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.951488018035889 ms
		out projection: 0.29286399483680725 ms
	self-attn: 9.680895805358887 ms
	cross-attn: 1.1479040384292603 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.75494384765625 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.637887954711914 ms
	cross-attn: 1.3322240114212036 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 12.919808387756348 ms
---------------- attention block ----------------
		qkv norm: 0.8970239758491516 ms
		rope: 1.0936319828033447 ms
		kv cache update (if evict): 0.6440960168838501 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.62662410736084 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.5943679809570312 ms
forward previous block to here: 12.828672409057617 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.51091194152832 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.33407974243164 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.7485439777374268 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0733442306518555 ms
		out projection: 0.40140798687934875 ms
	self-attn: 10.22156810760498 ms
	cross-attn: 1.2165119647979736 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 13.262847900390625 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.7034879922866821 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.532416343688965 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.361727714538574 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.415679931640625 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.248064041137695 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.42899227142334 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.207103729248047 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.6440960168838501 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.409536361694336 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 12.211199760437012 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.377792358398438 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 12.174336433410645 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.41260814666748 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.188672065734863 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.40236759185791 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.177408218383789 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.392127990722656 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.167167663574219 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.375743865966797 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 12.17843246459961 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.396224021911621 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.17024040222168 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.41158390045166 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.187647819519043 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.42080020904541 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 12.222463607788086 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.400320053100586 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.194815635681152 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.430015563964844 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 12.219391822814941 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.956607818603516 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.629695892333984 ms
	cross-attn: 1.2769279479980469 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.748800277709961 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.8171520233154297 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.976064205169678 ms
		out projection: 0.2826240062713623 ms
	self-attn: 10.008576393127441 ms
	cross-attn: 1.2451839447021484 ms
	ffn: 1.4684159755706787 ms
forward last block: 13.093888282775879 ms
##### model time: 524.0729370117188 #####
	 Transformer denoising step 1/4 completed in 524.68 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11161600053310394 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.210368037223816 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.26316800713539124 ms
	self-attn: 9.560064315795898 ms
	cross-attn: 1.551360011100769 ms
	ffn: 1.4704639911651611 ms
forward previous block to here: 12.993535995483398 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0475519895553589 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.196224212646484 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.479167938232422 ms
	cross-attn: 1.5493119955062866 ms
	ffn: 1.5144959688186646 ms
forward previous block to here: 12.95257568359375 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.201151967048645 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.965824127197266 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.460736274719238 ms
	cross-attn: 1.3004800081253052 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.770303726196289 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 1.0680320262908936 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.052864074707031 ms
		out projection: 0.3563520014286041 ms
	self-attn: 9.587712287902832 ms
	cross-attn: 1.248255968093872 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 12.713983535766602 ms
---------------- attention block ----------------
		qkv norm: 1.0782719850540161 ms
		rope: 1.0516480207443237 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.984255790710449 ms
		out projection: 0.27136000990867615 ms
	self-attn: 9.52627182006836 ms
	cross-attn: 1.2257280349731445 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.591103553771973 ms
---------------- attention block ----------------
		qkv norm: 0.9236479997634888 ms
		rope: 1.0536960363388062 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.96889591217041 ms
		out projection: 0.32767999172210693 ms
	self-attn: 9.693183898925781 ms
	cross-attn: 1.2441600561141968 ms
	ffn: 1.6168960332870483 ms
forward previous block to here: 12.934144020080566 ms
---------------- attention block ----------------
		qkv norm: 0.9379839897155762 ms
		rope: 1.1315200328826904 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9258880615234375 ms
		out projection: 0.28569599986076355 ms
	self-attn: 9.377792358398438 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.5124479532241821 ms
forward previous block to here: 12.46003246307373 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.997568130493164 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.083904266357422 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 12.022784233093262 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0301439762115479 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.066495895385742 ms
	cross-attn: 1.1857919692993164 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.04428768157959 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0813440084457397 ms
		kv cache update (if evict): 0.20787200331687927 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.261055946350098 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 12.073984146118164 ms
---------------- attention block ----------------
		qkv norm: 0.8642560243606567 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.993791580200195 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.84768009185791 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.975359916687012 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.828224182128906 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.685888290405273 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.608063697814941 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.851455688476562 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.599871635437012 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.614208221435547 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.629568099975586 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.681792259216309 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90982437133789 ms
	cross-attn: 1.0782719850540161 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.728896141052246 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.663359642028809 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.63161563873291 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.661312103271484 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.884223937988281 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.881152153015137 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.427456021308899 ms
forward last block: 11.64902400970459 ms
##### model time: 363.3223571777344 #####
	 Transformer denoising step 2/4 completed in 363.81 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.661312103271484 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.858624458312988 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.644927978515625 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.715583801269531 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.644927978515625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 11.708415985107422 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.751423835754395 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.988672256469727 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.780096054077148 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.836095809936523 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.879103660583496 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.698176383972168 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.607040405273438 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.64902400970459 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.669504165649414 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.64799976348877 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.847359657287598 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.630592346191406 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.673600196838379 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.857600212097168 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.669504165649414 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.679743766784668 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.63366413116455 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.862719535827637 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.644927978515625 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.841216087341309 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.595775604248047 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.64185619354248 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.1458560228347778 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.786239624023438 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.418239951133728 ms
forward last block: 11.63980770111084 ms
##### model time: 353.6343078613281 #####
	 Transformer denoising step 3/4 completed in 354.14 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.63366413116455 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 11.715583801269531 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.714559555053711 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.626496315002441 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.695103645324707 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.89139175415039 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.673600196838379 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.858624458312988 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.630592346191406 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 1.087488055229187 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.989376068115234 ms
		out projection: 0.27750399708747864 ms
	self-attn: 9.40236759185791 ms
	cross-attn: 1.2718080282211304 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.523520469665527 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.979135990142822 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.23033618927002 ms
	cross-attn: 1.235967993736267 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.306431770324707 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.012928009033203 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.234432220458984 ms
	cross-attn: 1.2441600561141968 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.322815895080566 ms
---------------- attention block ----------------
		qkv norm: 0.8898559808731079 ms
		rope: 1.0680320262908936 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.983232021331787 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.265151977539062 ms
	cross-attn: 1.2267520427703857 ms
	ffn: 1.491968035697937 ms
forward previous block to here: 12.35148811340332 ms
---------------- attention block ----------------
		qkv norm: 0.8939520120620728 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97708797454834 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.257984161376953 ms
	cross-attn: 1.2380160093307495 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 12.33407974243164 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.219072341918945 ms
	cross-attn: 1.2441600561141968 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.314623832702637 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0659840106964111 ms
		kv cache update (if evict): 0.1669120043516159 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9555840492248535 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.2293119430542 ms
	cross-attn: 1.2451839447021484 ms
	ffn: 1.4581760168075562 ms
forward previous block to here: 12.307456016540527 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.974016189575195 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.22111988067627 ms
	cross-attn: 1.2247040271759033 ms
	ffn: 1.5656960010528564 ms
forward previous block to here: 12.402688026428223 ms
---------------- attention block ----------------
		qkv norm: 0.9646080136299133 ms
		rope: 1.0711040496826172 ms
		kv cache update (if evict): 0.16486400365829468 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.290752410888672 ms
	cross-attn: 1.2288000583648682 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 12.337151527404785 ms
---------------- attention block ----------------
		qkv norm: 0.8775680065155029 ms
		rope: 1.0444799661636353 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.993472099304199 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.243647575378418 ms
	cross-attn: 1.2113920450210571 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.34124755859375 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0618879795074463 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.963776111602783 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.23852825164795 ms
	cross-attn: 1.2257280349731445 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.34329605102539 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0690560340881348 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.971968173980713 ms
		out projection: 0.26521599292755127 ms
	self-attn: 9.255935668945312 ms
	cross-attn: 1.2154879570007324 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 12.316672325134277 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.971968173980713 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.185279846191406 ms
	cross-attn: 1.2615679502487183 ms
	ffn: 1.4673919677734375 ms
forward previous block to here: 12.303359985351562 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.16793599724769592 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.983232021331787 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.225215911865234 ms
	cross-attn: 1.2042239904403687 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 12.278783798217773 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0393600463867188 ms
		kv cache update (if evict): 0.18943999707698822 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.97811222076416 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.2293119430542 ms
	cross-attn: 1.222656011581421 ms
	ffn: 1.486847996711731 ms
forward previous block to here: 12.409855842590332 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.942591667175293 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.737088203430176 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.912896156311035 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.708415985107422 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.672575950622559 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.678720474243164 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0926079750061035 ms
	ffn: 1.4315520524978638 ms
forward last block: 11.73196792602539 ms
##### model time: 364.0944519042969 #####
	 Transformer final denoising step 4/4 completed in 364.57 ms
‚ö° Block 10 denoising completed in 1609.41 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.08908800035715103 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.699199676513672 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.862719535827637 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.700223922729492 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.652095794677734 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.700223922729492 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.699199676513672 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.697152137756348 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.854528427124023 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.675647735595703 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.887295722961426 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.903679847717285 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.881152153015137 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.679743766784668 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.663359642028809 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.9512959718704224 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.984576225280762 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.797504425048828 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.693056106567383 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.652095794677734 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.614208221435547 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.884223937988281 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.643903732299805 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.612159729003906 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.679743766784668 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.691007614135742 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4254080057144165 ms
forward last block: 11.729920387268066 ms
##### model time: 353.76434326171875 #####
KV cache update for next block completed in 354.28 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 10 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 10 VAE decoding completed in 54.98 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 10 for sending...
‚úÖ Block 10 completed in 2037.11 ms (12 frames queued in 0.018 ms)
In loop: idx 10, current_num_frames 3, current_start_frame 30
üîÑ Processing block 11/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.1085439994931221 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 11.799551963806152 ms
		kv cache update (if evict): 0.6830080151557922 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.032383918762207 ms
		out projection: 7.522304058074951 ms
	self-attn: 28.16819190979004 ms
	cross-attn: 4.946944236755371 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 36.61209487915039 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 8.013824462890625 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.768959999084473 ms
		out projection: 0.2908160090446472 ms
	self-attn: 20.901887893676758 ms
	cross-attn: 1.3608959913253784 ms
	ffn: 7.9134721755981445 ms
forward previous block to here: 30.57254409790039 ms
---------------- attention block ----------------
		qkv norm: 0.936959981918335 ms
		rope: 1.052672028541565 ms
		kv cache update (if evict): 0.6748160123825073 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.012928009033203 ms
		out projection: 3.763200044631958 ms
	self-attn: 23.816192626953125 ms
	cross-attn: 1.640447974205017 ms
	ffn: 3.892224073410034 ms
forward previous block to here: 29.722623825073242 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 8.651776313781738 ms
		kv cache update (if evict): 0.7598080039024353 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.030335903167725 ms
		out projection: 7.565311908721924 ms
	self-attn: 28.476415634155273 ms
	cross-attn: 4.963327884674072 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 37.164031982421875 ms
---------------- attention block ----------------
		qkv norm: 0.8878080248832703 ms
		rope: 1.0762239694595337 ms
		kv cache update (if evict): 8.093695640563965 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.759743690490723 ms
		out projection: 0.2949120104312897 ms
	self-attn: 21.364736557006836 ms
	cross-attn: 1.2451839447021484 ms
	ffn: 9.252863883972168 ms
forward previous block to here: 35.4600944519043 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 3.402751922607422 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.040575981140137 ms
		out projection: 0.3246079981327057 ms
	self-attn: 15.110143661499023 ms
	cross-attn: 4.8455681800842285 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 23.558143615722656 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 1.082368016242981 ms
		kv cache update (if evict): 0.6666240096092224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 10.02291202545166 ms
		out projection: 0.30617600679397583 ms
	self-attn: 21.593088150024414 ms
	cross-attn: 1.2677119970321655 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 24.67737579345703 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 0.6615039706230164 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.946368217468262 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.558015823364258 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.7868800163269043 ms
forward previous block to here: 13.091839790344238 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.934080123901367 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.538559913635254 ms
	cross-attn: 1.112064003944397 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 12.422143936157227 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.6430082321167 ms
	cross-attn: 1.2799999713897705 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.709888458251953 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.480192184448242 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.310527801513672 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.932032108306885 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.481216430664062 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 12.34124755859375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.470975875854492 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.328960418701172 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.2764799892902374 ms
	self-attn: 9.60921573638916 ms
	cross-attn: 1.1479040384292603 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.562432289123535 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.707584023475647 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.971968173980713 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.649151802062988 ms
	cross-attn: 1.136639952659607 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 12.593152046203613 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.6952959895133972 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.951488018035889 ms
		out projection: 0.2457599937915802 ms
	self-attn: 9.572352409362793 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.45798397064209 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.6676480174064636 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.497599601745605 ms
	cross-attn: 1.1048959493637085 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.36787223815918 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.442303657531738 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.319744110107422 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.25600001215934753 ms
	self-attn: 9.480192184448242 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.348416328430176 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.452544212341309 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.280832290649414 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.4269437789917 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 12.246015548706055 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.49350357055664 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.320768356323242 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.477120399475098 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 12.364800453186035 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.2611199915409088 ms
	self-attn: 9.5098876953125 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 12.397567749023438 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.24371199309825897 ms
	self-attn: 9.462783813476562 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 12.302335739135742 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.41977596282959 ms
	cross-attn: 1.1079679727554321 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 12.312576293945312 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.6635519862174988 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.491456031799316 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.329983711242676 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.446399688720703 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 12.316672325134277 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.6850559711456299 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.484288215637207 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 12.328960418701172 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.593855857849121 ms
	cross-attn: 1.1407359838485718 ms
	ffn: 1.4428160190582275 ms
forward last block: 12.517375946044922 ms
##### model time: 509.8997802734375 #####
	 Transformer denoising step 1/4 completed in 510.44 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09830400347709656 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.942591667175293 ms
	cross-attn: 1.1141120195388794 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 11.834367752075195 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.975359916687012 ms
	cross-attn: 1.1038719415664673 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.841535568237305 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2611199915409088 ms
	self-attn: 8.983551979064941 ms
	cross-attn: 1.134592056274414 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.890687942504883 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.970239639282227 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.804672241210938 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.75654411315918 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.19251200556755066 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.00812816619873 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 11.866111755371094 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.958975791931152 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.776000022888184 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.92793607711792 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.947711944580078 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.843584060668945 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.961024284362793 ms
	cross-attn: 1.0844160318374634 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.828224182128906 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.958975791931152 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.800576210021973 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.974335670471191 ms
	cross-attn: 1.1048959493637085 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.85587215423584 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.2682879865169525 ms
	self-attn: 8.974335670471191 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.876352310180664 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.953856468200684 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.795455932617188 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.74118423461914 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.715583801269531 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.718655586242676 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.962047576904297 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.800576210021973 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.012224197387695 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.815936088562012 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.930303573608398 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.758591651916504 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.976384162902832 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.828224182128906 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.01529598236084 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.883520126342773 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.981504440307617 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.823103904724121 ms
---------------- attention block ----------------
		qkv norm: 0.9277439713478088 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.182208061218262 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.995136260986328 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.938495635986328 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.752448081970215 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.896512031555176 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.76576042175293 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.90982437133789 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.744256019592285 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.793408393859863 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.966143608093262 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.800576210021973 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.1212799549102783 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 11.849727630615234 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.00710391998291 ms
	cross-attn: 1.1427839994430542 ms
	ffn: 1.4458880424499512 ms
forward last block: 11.942912101745605 ms
##### model time: 358.1788024902344 #####
	 Transformer denoising step 2/4 completed in 358.68 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10239999741315842 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.934080123901367 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.01632022857666 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4755840301513672 ms
forward previous block to here: 11.907072067260742 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91871976852417 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.964096069335938 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.809791564941406 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.967167854309082 ms
	cross-attn: 1.124351978302002 ms
	ffn: 1.4807039499282837 ms
forward previous block to here: 11.934720039367676 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.070080041885376 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.945343971252441 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.067520141601562 ms
	cross-attn: 1.0741759538650513 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.914239883422852 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.948736190795898 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.762687683105469 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.26419198513031006 ms
	self-attn: 8.958975791931152 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.789312362670898 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.26009601354599 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.806719779968262 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.916672229766846 ms
		out projection: 0.25600001215934753 ms
	self-attn: 8.951807975769043 ms
	cross-attn: 1.1028480529785156 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.835391998291016 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.26009601354599 ms
	self-attn: 8.994815826416016 ms
	cross-attn: 1.112064003944397 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.877375602722168 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.903679847717285 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.705344200134277 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.74937629699707 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.706368446350098 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.952832221984863 ms
	cross-attn: 1.1786240339279175 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 11.941887855529785 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.02246379852295 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.85484790802002 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.24985599517822266 ms
	self-attn: 9.027584075927734 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.878399848937988 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.995840072631836 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.799551963806152 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.921088218688965 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.720704078674316 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.25804799795150757 ms
	self-attn: 8.942591667175293 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.742207527160645 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.928256034851074 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.719679832458496 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2815999984741211 ms
	self-attn: 9.01427173614502 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.843584060668945 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2611199915409088 ms
	self-attn: 8.991744041442871 ms
	cross-attn: 1.1161600351333618 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.873279571533203 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.962047576904297 ms
	cross-attn: 1.1110399961471558 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.83846378326416 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.25088000297546387 ms
	self-attn: 9.005056381225586 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.876352310180664 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.928256034851074 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.772928237915039 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.948736190795898 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 11.804672241210938 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.929280281066895 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.767807960510254 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.707391738891602 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 11.750399589538574 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.938495635986328 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.74015998840332 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.25804799795150757 ms
	self-attn: 8.951807975769043 ms
	cross-attn: 1.0987520217895508 ms
	ffn: 1.439743995666504 ms
forward last block: 11.829248428344727 ms
##### model time: 357.9893798828125 #####
	 Transformer denoising step 3/4 completed in 358.47 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09932799637317657 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.91974401473999 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.976384162902832 ms
	cross-attn: 1.1397119760513306 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.890687942504883 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.921792030334473 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.786239624023438 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.919039726257324 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.926207542419434 ms
	cross-attn: 1.0813440084457397 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.91801643371582 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.714559555053711 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.940544128417969 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.763711929321289 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.995840072631836 ms
	cross-attn: 1.117184042930603 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.871232032775879 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.945664405822754 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.960000038146973 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.785216331481934 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.932032108306885 ms
		out projection: 0.2744320034980774 ms
	self-attn: 9.084927558898926 ms
	cross-attn: 1.1325440406799316 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.003328323364258 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.011199951171875 ms
	cross-attn: 1.0956799983978271 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.876352310180664 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.920767784118652 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.951807975769043 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.813887596130371 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.74630355834961 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0117119550704956 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.728896141052246 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.963776111602783 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.979455947875977 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.811840057373047 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.887295722961426 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.671551704406738 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.729920387268066 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2662400007247925 ms
	self-attn: 8.955904006958008 ms
	cross-attn: 1.112064003944397 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 11.860992431640625 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.936448097229004 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.74835205078125 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.943615913391113 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.792384147644043 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.714559555053711 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.1079679727554321 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.814911842346191 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.065152168273926 ms
		out projection: 0.2672640085220337 ms
	self-attn: 9.22111988067627 ms
	cross-attn: 1.1110399961471558 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.137472152709961 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.2590720057487488 ms
	self-attn: 8.977408409118652 ms
	cross-attn: 1.1038719415664673 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.86406421661377 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.929280281066895 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.793408393859863 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.725824356079102 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.702272415161133 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.929280281066895 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.16076800227165222 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.926207542419434 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4243839979171753 ms
forward last block: 11.729920387268066 ms
##### model time: 357.4435729980469 #####
	 Transformer final denoising step 4/4 completed in 357.95 ms
‚ö° Block 11 denoising completed in 1587.45 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.724800109863281 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.726847648620605 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.944640159606934 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.801600456237793 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.732992172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.949760437011719 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.781120300292969 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.948736190795898 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.743231773376465 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.652095794677734 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2611199915409088 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0977280139923096 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.842559814453125 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2529279887676239 ms
	self-attn: 9.01632022857666 ms
	cross-attn: 1.1151360273361206 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.928576469421387 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.915647983551025 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.992768287658691 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.829248428344727 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.912896156311035 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.724800109863281 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.944640159606934 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.91494369506836 ms
	cross-attn: 1.3352960348129272 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.009471893310547 ms
---------------- attention block ----------------
		qkv norm: 0.8714240193367004 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.93337631225586 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.727871894836426 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.92518424987793 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.732992172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.91494369506836 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.709440231323242 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.075712203979492 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.879424095153809 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.736063957214355 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.26316800713539124 ms
	self-attn: 8.982527732849121 ms
	cross-attn: 1.1223039627075195 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.8722562789917 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.991744041442871 ms
	cross-attn: 1.1335680484771729 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.912192344665527 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0362880229949951 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.929984092712402 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.028608322143555 ms
	cross-attn: 1.099776029586792 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.895808219909668 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.018880009651184 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.800576210021973 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.919039726257324 ms
	cross-attn: 1.0864640474319458 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.969216346740723 ms
	cross-attn: 1.316864013671875 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 12.078080177307129 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.974335670471191 ms
	cross-attn: 1.1233279705047607 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.875328063964844 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.962047576904297 ms
	cross-attn: 1.1653120517730713 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.888640403747559 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.926207542419434 ms
	cross-attn: 1.1038719415664673 ms
	ffn: 1.4387199878692627 ms
forward last block: 11.796480178833008 ms
##### model time: 357.6381530761719 #####
KV cache update for next block completed in 358.18 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 11 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 11 VAE decoding completed in 55.22 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 11 for sending...
‚úÖ Block 11 completed in 2019.51 ms (12 frames queued in 0.018 ms)
In loop: idx 11, current_num_frames 3, current_start_frame 33
üîÑ Processing block 12/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.11776000261306763 ms
---------------- attention block ----------------
		qkv norm: 0.9164800047874451 ms
		rope: 8.545280456542969 ms
		kv cache update (if evict): 0.8058879971504211 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.086656093597412 ms
		out projection: 0.3256320059299469 ms
	self-attn: 21.350400924682617 ms
	cross-attn: 13.234175682067871 ms
	ffn: 1.605631947517395 ms
forward previous block to here: 36.74726486206055 ms
---------------- attention block ----------------
		qkv norm: 3.1057920455932617 ms
		rope: 1.0895359516143799 ms
		kv cache update (if evict): 0.6768640279769897 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.086656093597412 ms
		out projection: 0.2959359884262085 ms
	self-attn: 14.329855918884277 ms
	cross-attn: 5.517312049865723 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 23.550975799560547 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.117184042930603 ms
		kv cache update (if evict): 0.6922240257263184 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 13.58028793334961 ms
		out projection: 0.2979840040206909 ms
	self-attn: 17.42950439453125 ms
	cross-attn: 3.7611520290374756 ms
	ffn: 1.5073280334472656 ms
forward previous block to here: 23.09836769104004 ms
---------------- attention block ----------------
		qkv norm: 9.018367767333984 ms
		rope: 5.025792121887207 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.211456298828125 ms
		out projection: 0.35737600922584534 ms
	self-attn: 29.14201545715332 ms
	cross-attn: 1.7192959785461426 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 37.78355026245117 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0414079427719116 ms
		kv cache update (if evict): 8.014847755432129 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.231040000915527 ms
		out projection: 0.32767999172210693 ms
	self-attn: 19.780607223510742 ms
	cross-attn: 1.5052800178527832 ms
	ffn: 1.5104000568389893 ms
forward previous block to here: 23.199743270874023 ms
---------------- attention block ----------------
		qkv norm: 5.700607776641846 ms
		rope: 1.21343994140625 ms
		kv cache update (if evict): 0.6789119839668274 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.456192016601562 ms
		out projection: 0.3461120128631592 ms
	self-attn: 30.114816665649414 ms
	cross-attn: 1.9128320217132568 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 39.11782455444336 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0885119438171387 ms
		kv cache update (if evict): 0.6758400201797485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.161407947540283 ms
		out projection: 0.30105599761009216 ms
	self-attn: 17.466367721557617 ms
	cross-attn: 3.7498879432678223 ms
	ffn: 1.533951997756958 ms
forward previous block to here: 23.1147518157959 ms
---------------- attention block ----------------
		qkv norm: 12.685312271118164 ms
		rope: 1.1653120517730713 ms
		kv cache update (if evict): 0.6748160123825073 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.616959571838379 ms
		out projection: 0.2805759906768799 ms
	self-attn: 24.399871826171875 ms
	cross-attn: 1.2799999713897705 ms
	ffn: 1.4786560535430908 ms
forward previous block to here: 27.504640579223633 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0373120307922363 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.012928009033203 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.657343864440918 ms
	cross-attn: 1.1683839559555054 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 12.610560417175293 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0168319940567017 ms
		kv cache update (if evict): 0.6615039706230164 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.951488018035889 ms
		out projection: 0.26419198513031006 ms
	self-attn: 9.54470443725586 ms
	cross-attn: 1.141759991645813 ms
	ffn: 1.4510079622268677 ms
forward previous block to here: 12.473343849182129 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.6615039706230164 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.937151908874512 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.530367851257324 ms
	cross-attn: 1.1335680484771729 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.447744369506836 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.52627182006836 ms
	cross-attn: 1.1018240451812744 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 12.396544456481934 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0178560018539429 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.485312461853027 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4551039934158325 ms
forward previous block to here: 12.321791648864746 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.401344299316406 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 12.186623573303223 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2303999960422516 ms
	self-attn: 9.373696327209473 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 12.14361572265625 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23449599742889404 ms
	self-attn: 9.358336448669434 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 12.112895965576172 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.6420480012893677 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.39417552947998 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 12.17024040222168 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2519040107727051 ms
	self-attn: 9.474047660827637 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 12.315648078918457 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.415679931640625 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.262399673461914 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9258880615234375 ms
		out projection: 0.24268800020217896 ms
	self-attn: 9.466879844665527 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.275712013244629 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.42182445526123 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.23475170135498 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.371647834777832 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 12.163071632385254 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.387007713317871 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4561280012130737 ms
forward previous block to here: 12.200960159301758 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.6676480174064636 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.433088302612305 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.233728408813477 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6584320068359375 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.391103744506836 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.206080436706543 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.367551803588867 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.185600280761719 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.383935928344727 ms
	cross-attn: 1.0792959928512573 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 12.222463607788086 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0158079862594604 ms
		kv cache update (if evict): 0.6727679967880249 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.496576309204102 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 12.322815895080566 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.40236759185791 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.211199760437012 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.366527557373047 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4284800291061401 ms
forward last block: 12.146688461303711 ms
##### model time: 510.36773681640625 #####
	 Transformer denoising step 1/4 completed in 510.85 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.963071823120117 ms
	cross-attn: 1.1089919805526733 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.8405122756958 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.719679832458496 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.0670080184936523 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.712512016296387 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.894463539123535 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.678720474243164 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.697152137756348 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.63366413116455 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.913599967956543 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.669504165649414 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.735039710998535 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.0915839672088623 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.769856452941895 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.8985595703125 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.858624458312988 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.62342357635498 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.911871910095215 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.6746244430542 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.881152153015137 ms
	cross-attn: 1.0639359951019287 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.708415985107422 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.935423851013184 ms
	cross-attn: 1.1868159770965576 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.880448341369629 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.031167984008789 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.946687698364258 ms
	cross-attn: 1.094655990600586 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.810815811157227 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.877056121826172 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.685888290405273 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.887295722961426 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.24883200228214264 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4489599466323853 ms
forward previous block to here: 11.673600196838379 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.894463539123535 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.6561918258667 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.917695999145508 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.951807975769043 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.710463523864746 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.64185619354248 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.663359642028809 ms
---------------- attention block ----------------
		qkv norm: 0.8386560082435608 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.25702399015426636 ms
	self-attn: 9.02246379852295 ms
	cross-attn: 1.1008000373840332 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.888640403747559 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.962047576904297 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.791359901428223 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9269118309021 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.979455947875977 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4479360580444336 ms
forward last block: 11.799551963806152 ms
##### model time: 355.00238037109375 #####
	 Transformer denoising step 2/4 completed in 355.49 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.954879760742188 ms
	cross-attn: 1.0803200006484985 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.807744026184082 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.732992172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.070080041885376 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.724800109863281 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 11.73196792602539 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.16383999586105347 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.26316800713539124 ms
	self-attn: 8.991744041442871 ms
	cross-attn: 1.117184042930603 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 11.910143852233887 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.0219520330429077 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.98969554901123 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 11.83027172088623 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9115519523620605 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.667455673217773 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.877056121826172 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.65004825592041 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.653120040893555 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.667455673217773 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.677696228027344 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.25702399015426636 ms
	self-attn: 8.92416000366211 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.726847648620605 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.26419198513031006 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0833920240402222 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.774975776672363 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.726847648620605 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.913920402526855 ms
	cross-attn: 1.07315194606781 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.751423835754395 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0321919918060303 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.90982437133789 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 11.720704078674316 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.841216087341309 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.616255760192871 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.830975532531738 ms
	cross-attn: 0.9983999729156494 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.619327545166016 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.646976470947266 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.618304252624512 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.828927993774414 ms
	cross-attn: 1.0004479885101318 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.572223663330078 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.842240333557129 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.615232467651367 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.727871894836426 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.27852800488471985 ms
	self-attn: 9.086976051330566 ms
	cross-attn: 1.1438080072402954 ms
	ffn: 1.4643199443817139 ms
forward previous block to here: 12.051456451416016 ms
---------------- attention block ----------------
		qkv norm: 0.8601599931716919 ms
		rope: 1.0250240564346313 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.923840045928955 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.050111770629883 ms
	cross-attn: 1.1028480529785156 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 11.960320472717285 ms
---------------- attention block ----------------
		qkv norm: 0.8581119775772095 ms
		rope: 1.0209280252456665 ms
		kv cache update (if evict): 0.15462400019168854 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.975359916687012 ms
	cross-attn: 1.0967040061950684 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.907072067260742 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.005568027496338 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.992768287658691 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4458880424499512 ms
forward last block: 11.897855758666992 ms
##### model time: 355.7068786621094 #####
	 Transformer denoising step 3/4 completed in 356.19 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10035199671983719 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.969216346740723 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.807744026184082 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.15564799308776855 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.2949120104312897 ms
	self-attn: 9.029631614685059 ms
	cross-attn: 1.4479360580444336 ms
	ffn: 1.5206400156021118 ms
forward previous block to here: 12.416000366210938 ms
---------------- attention block ----------------
		qkv norm: 0.8663039803504944 ms
		rope: 1.04038405418396 ms
		kv cache update (if evict): 0.1812479943037033 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.01907205581665 ms
		out projection: 0.24883200228214264 ms
	self-attn: 9.31123161315918 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 12.128255844116211 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.2826240062713623 ms
	self-attn: 9.080831527709961 ms
	cross-attn: 1.1458560228347778 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 12.051456451416016 ms
---------------- attention block ----------------
		qkv norm: 0.8857600092887878 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 0.16998399794101715 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.24473600089550018 ms
	self-attn: 9.120767593383789 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 12.073984146118164 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.941568374633789 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.762687683105469 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.950783729553223 ms
	cross-attn: 1.2257280349731445 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.96236801147461 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.939519882202148 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.958975791931152 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.760640144348145 ms
---------------- attention block ----------------
		qkv norm: 0.8796160221099854 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.970239639282227 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.757568359375 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.932352066040039 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.721728324890137 ms
---------------- attention block ----------------
		qkv norm: 0.8632320165634155 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0106879472732544 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.098944187164307 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.143296241760254 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.941887855529785 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.903359889984131 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.904704093933105 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.689984321594238 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.006592035293579 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.679743766784668 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.64185619354248 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9738240242004395 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.831999778747559 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.581439971923828 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.289792001247406 ms
	self-attn: 9.036800384521484 ms
	cross-attn: 1.2902400493621826 ms
	ffn: 1.4725120067596436 ms
forward previous block to here: 12.175359725952148 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0424319505691528 ms
		kv cache update (if evict): 0.17510400712490082 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.976064205169678 ms
		out projection: 0.2590720057487488 ms
	self-attn: 9.21292781829834 ms
	cross-attn: 1.2677119970321655 ms
	ffn: 1.4714879989624023 ms
forward previous block to here: 12.335103988647461 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 1.0721280574798584 ms
		kv cache update (if evict): 0.17612800002098083 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.975039958953857 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.286656379699707 ms
	cross-attn: 1.2584960460662842 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.400639533996582 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0465279817581177 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.982207775115967 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.255935668945312 ms
	cross-attn: 1.2206079959869385 ms
	ffn: 1.4735360145568848 ms
forward previous block to here: 12.320768356323242 ms
---------------- attention block ----------------
		qkv norm: 0.8673279881477356 ms
		rope: 1.0567679405212402 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.962751865386963 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.249792098999023 ms
	cross-attn: 1.2349439859390259 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.316672325134277 ms
---------------- attention block ----------------
		qkv norm: 0.8837119936943054 ms
		rope: 1.043455958366394 ms
		kv cache update (if evict): 0.16281600296497345 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.972991943359375 ms
		out projection: 0.2662400007247925 ms
	self-attn: 9.248767852783203 ms
	cross-attn: 1.2247040271759033 ms
	ffn: 1.4602240324020386 ms
forward previous block to here: 12.314623832702637 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0496000051498413 ms
		kv cache update (if evict): 0.1730560064315796 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.984255790710449 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.2293119430542 ms
	cross-attn: 1.0485759973526 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.07091236114502 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2764799892902374 ms
	self-attn: 9.030655860900879 ms
	cross-attn: 1.252351999282837 ms
	ffn: 1.4776320457458496 ms
forward previous block to here: 12.123135566711426 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 1.1438080072402954 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.154560089111328 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.965439796447754 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.653120040893555 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4346239566802979 ms
forward last block: 11.65824031829834 ms
##### model time: 361.9123229980469 #####
	 Transformer final denoising step 4/4 completed in 362.42 ms
‚ö° Block 12 denoising completed in 1586.87 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09216000139713287 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.706368446350098 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.90880012512207 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.654144287109375 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.622400283813477 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.630592346191406 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.857600212097168 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.611136436462402 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.625472068786621 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.644927978515625 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2734079957008362 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0629119873046875 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.727871894836426 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.669504165649414 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.851455688476562 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.612159729003906 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.661312103271484 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2672640085220337 ms
	self-attn: 8.960000038146973 ms
	cross-attn: 1.1059199571609497 ms
	ffn: 1.4632960557937622 ms
forward previous block to here: 11.858943939208984 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0076160430908203 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.964096069335938 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.805695533752441 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0086400508880615 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.922815799713135 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.962047576904297 ms
	cross-attn: 1.0895359516143799 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.8220796585083 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.912576198577881 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.694080352783203 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.846336364746094 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.6428804397583 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.0332159996032715 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.65004825592041 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.64083194732666 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.661312103271484 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.997376024723053 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.857600212097168 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.65721607208252 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.8985595703125 ms
	cross-attn: 1.0690560340881348 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.711487770080566 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.896512031555176 ms
	cross-attn: 1.0618879795074463 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 11.778047561645508 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9943040013313293 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.90982437133789 ms
	cross-attn: 1.0762239694595337 ms
	ffn: 1.4284800291061401 ms
forward last block: 11.73196792602539 ms
##### model time: 354.0531311035156 #####
KV cache update for next block completed in 354.58 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 12 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 12 VAE decoding completed in 55.02 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 12 for sending...
‚úÖ Block 12 completed in 2015.29 ms (12 frames queued in 0.019 ms)
In loop: idx 12, current_num_frames 3, current_start_frame 36
üîÑ Processing block 13/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09523200243711472 ms
---------------- attention block ----------------
		qkv norm: 7.996416091918945 ms
		rope: 4.746240139007568 ms
		kv cache update (if evict): 0.6625279784202576 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.071296215057373 ms
		out projection: 0.5222399830818176 ms
	self-attn: 32.339969635009766 ms
	cross-attn: 1.3312000036239624 ms
	ffn: 1.4899200201034546 ms
forward previous block to here: 36.88140869140625 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 1.0240000486373901 ms
		kv cache update (if evict): 7.895040035247803 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.0733442306518555 ms
		out projection: 2.6316800117492676 ms
	self-attn: 19.557376861572266 ms
	cross-attn: 1.3158400058746338 ms
	ffn: 1.481727957725525 ms
forward previous block to here: 30.033920288085938 ms
---------------- attention block ----------------
		qkv norm: 4.484096050262451 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 2.6255359649658203 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 4.580351829528809 ms
	self-attn: 23.87353515625 ms
	cross-attn: 1.257472038269043 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 28.420095443725586 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0291199684143066 ms
		kv cache update (if evict): 8.0066556930542 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.139904022216797 ms
		out projection: 0.289792001247406 ms
	self-attn: 17.289215087890625 ms
	cross-attn: 3.6167678833007812 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 22.739967346191406 ms
---------------- attention block ----------------
		qkv norm: 8.424448013305664 ms
		rope: 4.868095874786377 ms
		kv cache update (if evict): 0.6594560146331787 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.326144218444824 ms
		out projection: 0.6912000179290771 ms
	self-attn: 32.884735107421875 ms
	cross-attn: 1.3035520315170288 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 37.7077751159668 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.1018240451812744 ms
		kv cache update (if evict): 0.6912000179290771 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.066175937652588 ms
		out projection: 0.27033600211143494 ms
	self-attn: 17.351680755615234 ms
	cross-attn: 3.551232099533081 ms
	ffn: 1.45305597782135 ms
forward previous block to here: 22.731775283813477 ms
---------------- attention block ----------------
		qkv norm: 8.469504356384277 ms
		rope: 1.1550719738006592 ms
		kv cache update (if evict): 0.6615039706230164 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 8.439807891845703 ms
		out projection: 4.296703815460205 ms
	self-attn: 32.760833740234375 ms
	cross-attn: 1.3199360370635986 ms
	ffn: 2.787328004837036 ms
forward previous block to here: 37.25107192993164 ms
---------------- attention block ----------------
		qkv norm: 0.8622080087661743 ms
		rope: 1.035264015197754 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.954559803009033 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.537535667419434 ms
	cross-attn: 1.0854400396347046 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.395520210266113 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.478143692016602 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.241920471191406 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.365504264831543 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 12.126208305358887 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.6533120274543762 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.106112003326416 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.578495979309082 ms
	cross-attn: 1.0024960041046143 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 12.317695617675781 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.6451200246810913 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.4269437789917 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.188672065734863 ms
---------------- attention block ----------------
		qkv norm: 0.8611840009689331 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.7618560194969177 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.620479583740234 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 12.377087593078613 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.6430720090866089 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.2303999960422516 ms
	self-attn: 9.315327644348145 ms
	cross-attn: 1.0178560018539429 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 12.085247993469238 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.7802879810333252 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23244799673557281 ms
	self-attn: 9.529343605041504 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 12.273664474487305 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.354240417480469 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 12.108799934387207 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.6420480012893677 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2303999960422516 ms
	self-attn: 9.347071647644043 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.113920211791992 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.22937600314617157 ms
	self-attn: 9.334783554077148 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 12.083200454711914 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.6451200246810913 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.340928077697754 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 12.122112274169922 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2303999960422516 ms
	self-attn: 9.379839897155762 ms
	cross-attn: 0.9994239807128906 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 12.119039535522461 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0280959606170654 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.4136323928833 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 12.176383972167969 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.364480018615723 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 12.115967750549316 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.6543359756469727 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.345024108886719 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.123135566711426 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.340928077697754 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.141568183898926 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.22937600314617157 ms
	self-attn: 9.315327644348145 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.083200454711914 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.22835199534893036 ms
	self-attn: 9.368576049804688 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 12.12723159790039 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.372672080993652 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 12.139519691467285 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 1.001471996307373 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.433088302612305 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 12.205056190490723 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.6420480012893677 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.25804799795150757 ms
	self-attn: 9.397248268127441 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.142592430114746 ms
---------------- attention block ----------------
		qkv norm: 0.8386560082435608 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.867519855499268 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.366527557373047 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4264320135116577 ms
forward last block: 12.107775688171387 ms
##### model time: 499.6444091796875 #####
	 Transformer denoising step 1/4 completed in 500.17 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09011200070381165 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.839167594909668 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.596799850463867 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.819711685180664 ms
	cross-attn: 1.0045440196990967 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.579392433166504 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.580415725708008 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 0.997376024723053 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 11.574272155761719 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.22835199534893036 ms
	self-attn: 8.798208236694336 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.55788803100586 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.823807716369629 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4090240001678467 ms
forward previous block to here: 11.552767753601074 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.582464218139648 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.82483196258545 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.554816246032715 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.847359657287598 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.582464218139648 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13209599256515503 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.830975532531738 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.595775604248047 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.821760177612305 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.610112190246582 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.82688045501709 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.563008308410645 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.819711685180664 ms
	cross-attn: 0.9912319779396057 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.545599937438965 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.828927993774414 ms
	cross-attn: 1.001471996307373 ms
	ffn: 1.4120960235595703 ms
forward previous block to here: 11.560959815979004 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9707520008087158 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.22835199534893036 ms
	self-attn: 8.804351806640625 ms
	cross-attn: 1.0004479885101318 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 11.53331184387207 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.820735931396484 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.583488464355469 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13209599256515503 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.26009601354599 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.63366413116455 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.844287872314453 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.410048007965088 ms
forward previous block to here: 11.565055847167969 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8664960861206055 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.823807716369629 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.569151878356934 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.82585620880127 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.580415725708008 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.821760177612305 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.589632034301758 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.851455688476562 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9697279930114746 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.821760177612305 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.579392433166504 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.159743994474411 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9852800369262695 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.01529598236084 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4827519655227661 ms
forward previous block to here: 11.829248428344727 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.850432395935059 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.637760162353516 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.590656280517578 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.81663990020752 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.571200370788574 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.868544101715088 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.814592361450195 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.555839538574219 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.844287872314453 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4120960235595703 ms
forward last block: 11.566080093383789 ms
##### model time: 351.21356201171875 #####
	 Transformer denoising step 2/4 completed in 351.69 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09011200070381165 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.843263626098633 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.850432395935059 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.601920127868652 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1515520066022873 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.898240089416504 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.913920402526855 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4479360580444336 ms
forward previous block to here: 11.725824356079102 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.881152153015137 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.662336349487305 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.587583541870117 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.617280006408691 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.811519622802734 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.578368186950684 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.15052799880504608 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0004479885101318 ms
	ffn: 1.4131200313568115 ms
forward previous block to here: 11.580415725708008 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13312000036239624 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.842240333557129 ms
	cross-attn: 0.9963520169258118 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.571200370788574 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.850432395935059 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.595775604248047 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.822784423828125 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.600895881652832 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.868544101715088 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.819711685180664 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.586560249328613 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.828927993774414 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.586560249328613 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.82688045501709 ms
	cross-attn: 0.9994239807128906 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.565055847167969 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.158720001578331 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.838144302368164 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.585536003112793 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.840191841125488 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.593728065490723 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.971776008605957 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.81049633026123 ms
	cross-attn: 1.001471996307373 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.54047966003418 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.813568115234375 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.574272155761719 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.814592361450195 ms
	cross-attn: 1.0024960041046143 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.54867172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.846336364746094 ms
	cross-attn: 0.9932799935340881 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.572223663330078 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.865471839904785 ms
		out projection: 0.22732800245285034 ms
	self-attn: 8.822784423828125 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.559935569763184 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.894144058227539 ms
		out projection: 0.22835199534893036 ms
	self-attn: 8.820735931396484 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.580415725708008 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8664960861206055 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.821760177612305 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.586560249328613 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.822784423828125 ms
	cross-attn: 1.0024960041046143 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.54867172241211 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.829952239990234 ms
	cross-attn: 0.9994239807128906 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.577343940734863 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.8340482711792 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.586560249328613 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.8340482711792 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.4141440391540527 ms
forward previous block to here: 11.566080093383789 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9748479723930359 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8664960861206055 ms
		out projection: 0.22937600314617157 ms
	self-attn: 8.79206371307373 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.54969596862793 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.844287872314453 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.584511756896973 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.2764799892902374 ms
	self-attn: 8.90675163269043 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.411072015762329 ms
forward last block: 11.663359642028809 ms
##### model time: 351.2668151855469 #####
	 Transformer denoising step 3/4 completed in 351.73 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09113600105047226 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.854528427124023 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.621376037597656 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2303999960422516 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.662336349487305 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.28672000765800476 ms
	self-attn: 9.071616172790527 ms
	cross-attn: 1.3107199668884277 ms
	ffn: 1.4929920434951782 ms
forward previous block to here: 12.264448165893555 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 1.1008000373840332 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.988351821899414 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.336832046508789 ms
	cross-attn: 1.2718080282211304 ms
	ffn: 1.4837759733200073 ms
forward previous block to here: 12.486656188964844 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 0.16896000504493713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.975039958953857 ms
		out projection: 0.26316800713539124 ms
	self-attn: 9.253888130187988 ms
	cross-attn: 1.2288000583648682 ms
	ffn: 1.5964159965515137 ms
forward previous block to here: 12.44159984588623 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2754560112953186 ms
	self-attn: 9.071616172790527 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4428160190582275 ms
forward previous block to here: 11.886591911315918 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.2539519965648651 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.439743995666504 ms
forward previous block to here: 11.721728324890137 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.889344215393066 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.680768013000488 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.887295722961426 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.701248168945312 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.0096640586853027 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.693056106567383 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.884223937988281 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.785216331481934 ms
---------------- attention block ----------------
		qkv norm: 0.869376003742218 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.1525759994983673 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.972288131713867 ms
	cross-attn: 1.0680320262908936 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.803647994995117 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.90060806274414 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 11.716608047485352 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 11.686911582946777 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.909503936767578 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.89958381652832 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.629568099975586 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.654144287109375 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.629568099975586 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.888319969177246 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.643903732299805 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.643903732299805 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.8985595703125 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.859647750854492 ms
	cross-attn: 1.0557440519332886 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.6561918258667 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0024960041046143 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.859647750854492 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.677696228027344 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.671551704406738 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.850432395935059 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.879103660583496 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.022976040840149 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.64902400970459 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.856575965881348 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4161920547485352 ms
forward last block: 11.638784408569336 ms
##### model time: 356.1850891113281 #####
	 Transformer final denoising step 4/4 completed in 356.67 ms
‚ö° Block 13 denoising completed in 1564.66 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.65004825592041 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.0373120307922363 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.669504165649414 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.875007629394531 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.683839797973633 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 1.013759970664978 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.899263858795166 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.8985595703125 ms
	cross-attn: 1.0588159561157227 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.723775863647461 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.1130880117416382 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.792384147644043 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.9169921875 ms
	cross-attn: 1.0936319828033447 ms
	ffn: 1.4520319700241089 ms
forward previous block to here: 11.785216331481934 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.15667200088500977 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4417920112609863 ms
forward previous block to here: 11.637760162353516 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.16179199516773224 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.90982437133789 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.691007614135742 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.2396160066127777 ms
	self-attn: 8.90777587890625 ms
	cross-attn: 1.0659840106964111 ms
	ffn: 1.4469120502471924 ms
forward previous block to here: 11.745280265808105 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.902336120605469 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.64902400970459 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.27136000990867615 ms
	self-attn: 8.927231788635254 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.720704078674316 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.619327545166016 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.26521599292755127 ms
	self-attn: 8.90982437133789 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.693056106567383 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.859647750854492 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.64185619354248 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.885248184204102 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.65004825592041 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.625472068786621 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9994239807128906 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.938495635986328 ms
	cross-attn: 1.060863971710205 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.737088203430176 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0199040174484253 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.89139175415039 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.880127906799316 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.684864044189453 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.24780799448490143 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0209280252456665 ms
	ffn: 1.4387199878692627 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.900288105010986 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.907455921173096 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.643903732299805 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13414399325847626 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.831999778747559 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.589632034301758 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.0342400074005127 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.6428804397583 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.610112190246582 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.89139175415039 ms
	cross-attn: 1.0475519895553589 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.680768013000488 ms
---------------- attention block ----------------
		qkv norm: 0.8652799725532532 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.861696243286133 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4202879667282104 ms
forward last block: 11.62342357635498 ms
##### model time: 353.9312744140625 #####
KV cache update for next block completed in 354.46 .s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
üé® Decoding block 13 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 13 VAE decoding completed in 55.76 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 13 for sending...
‚úÖ Block 13 completed in 1993.74 ms (12 frames queued in 0.019 ms)
In loop: idx 13, current_num_frames 3, current_start_frame 39
üîÑ Processing block 14/14
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09318400174379349 ms
---------------- attention block ----------------
		qkv norm: 8.574975967407227 ms
		rope: 4.782080173492432 ms
		kv cache update (if evict): 0.6574079990386963 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.008831977844238 ms
		out projection: 0.2959359884262085 ms
	self-attn: 28.16102409362793 ms
	cross-attn: 1.7346559762954712 ms
	ffn: 1.465343952178955 ms
forward previous block to here: 36.724735260009766 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0342400074005127 ms
		kv cache update (if evict): 8.032256126403809 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.541631698608398 ms
		out projection: 0.2590720057487488 ms
	self-attn: 20.539392471313477 ms
	cross-attn: 13.235199928283691 ms
	ffn: 1.4612480401992798 ms
forward previous block to here: 35.57683181762695 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 1.06496000289917 ms
		kv cache update (if evict): 7.825407981872559 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 9.591808319091797 ms
		out projection: 0.26214399933815 ms
	self-attn: 23.219200134277344 ms
	cross-attn: 1.2441600561141968 ms
	ffn: 7.9482879638671875 ms
forward previous block to here: 32.833534240722656 ms
---------------- attention block ----------------
		qkv norm: 0.9215999841690063 ms
		rope: 1.0147839784622192 ms
		kv cache update (if evict): 0.66457599401474 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.010879993438721 ms
		out projection: 0.5662720203399658 ms
	self-attn: 23.3492488861084 ms
	cross-attn: 1.1755520105361938 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 27.94700813293457 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.022976040840149 ms
		kv cache update (if evict): 7.584767818450928 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.998591899871826 ms
		out projection: 2.5026559829711914 ms
	self-attn: 19.050495147705078 ms
	cross-attn: 1.2165119647979736 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 22.054912567138672 ms
---------------- attention block ----------------
		qkv norm: 4.436992168426514 ms
		rope: 1.0270719528198242 ms
		kv cache update (if evict): 2.7832319736480713 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.02623987197876 ms
		out projection: 0.5304319858551025 ms
	self-attn: 21.221376419067383 ms
	cross-attn: 1.4387199878692627 ms
	ffn: 1.462272047996521 ms
forward previous block to here: 29.76460838317871 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 1.0332159996032715 ms
		kv cache update (if evict): 7.931903839111328 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 6.089727878570557 ms
		out projection: 0.5662720203399658 ms
	self-attn: 19.915775299072266 ms
	cross-attn: 1.1919360160827637 ms
	ffn: 1.457152009010315 ms
forward previous block to here: 22.905855178833008 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 1.0035200119018555 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.908480167388916 ms
		out projection: 0.23244799673557281 ms
	self-attn: 9.409536361694336 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.189696311950684 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.6440960168838501 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.388031959533691 ms
	cross-attn: 1.0086400508880615 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.137472152709961 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.6492159962654114 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.370623588562012 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 12.119039535522461 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.6430720090866089 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.349120140075684 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 12.120063781738281 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23244799673557281 ms
	self-attn: 9.33785629272461 ms
	cross-attn: 1.0158079862594604 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.091391563415527 ms
---------------- attention block ----------------
		qkv norm: 0.8407040238380432 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23654399812221527 ms
	self-attn: 9.363455772399902 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 12.123135566711426 ms
---------------- attention block ----------------
		qkv norm: 0.839680016040802 ms
		rope: 0.9861119985580444 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.327615737915039 ms
	cross-attn: 1.0035200119018555 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.075008392333984 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.2303999960422516 ms
	self-attn: 9.393152236938477 ms
	cross-attn: 1.006592035293579 ms
	ffn: 1.4438400268554688 ms
forward previous block to here: 12.154879570007324 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.24063999950885773 ms
	self-attn: 9.367551803588867 ms
	cross-attn: 1.0147839784622192 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 12.13542366027832 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.6604800224304199 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.340928077697754 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 12.081151962280273 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.6502400040626526 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87059211730957 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.32147216796875 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 12.085247993469238 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2303999960422516 ms
	self-attn: 9.325568199157715 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 12.103679656982422 ms
---------------- attention block ----------------
		qkv norm: 0.8560640215873718 ms
		rope: 0.9728000164031982 ms
		kv cache update (if evict): 0.6481919884681702 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.22937600314617157 ms
	self-attn: 9.32249641418457 ms
	cross-attn: 1.0383360385894775 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.104703903198242 ms
---------------- attention block ----------------
		qkv norm: 0.8529919981956482 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.6512640118598938 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.22835199534893036 ms
	self-attn: 9.364480018615723 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.116991996765137 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.2314240038394928 ms
	self-attn: 9.347071647644043 ms
	cross-attn: 1.0711040496826172 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 12.232704162597656 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9953280091285706 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23347200453281403 ms
	self-attn: 9.400320053100586 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 12.149760246276855 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2549760043621063 ms
	self-attn: 9.380864143371582 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 12.134400367736816 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.6420480012893677 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2539519965648651 ms
	self-attn: 9.439231872558594 ms
	cross-attn: 1.0506240129470825 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.230655670166016 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.647167980670929 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24780799448490143 ms
	self-attn: 9.41055965423584 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 12.190719604492188 ms
---------------- attention block ----------------
		qkv norm: 0.8806399703025818 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.6563839912414551 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.885951995849609 ms
		out projection: 0.23552000522613525 ms
	self-attn: 9.42899227142334 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 12.195839881896973 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.6553599834442139 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.24166400730609894 ms
	self-attn: 9.415679931640625 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.4376959800720215 ms
forward previous block to here: 12.231679916381836 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.6461439728736877 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23756800591945648 ms
	self-attn: 9.375743865966797 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 12.176383972167969 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.652288019657135 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.2467840015888214 ms
	self-attn: 9.42080020904541 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4213119745254517 ms
forward last block: 12.206080436706543 ms
##### model time: 498.9992980957031 #####
	 Transformer denoising step 1/4 completed in 499.48 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10035199671983719 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.929280281066895 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.685888290405273 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.66540813446045 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9932799935340881 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.896512031555176 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.660287857055664 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2529279887676239 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.06496000289917 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.710463523864746 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 1.0127359628677368 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.672575950622559 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.0598399639129639 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.675647735595703 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.24985599517822266 ms
	self-attn: 8.901632308959961 ms
	cross-attn: 1.0751999616622925 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.736063957214355 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.912896156311035 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.726847648620605 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.884223937988281 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.785216331481934 ms
---------------- attention block ----------------
		qkv norm: 0.8734719753265381 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.17715199291706085 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.9903998374938965 ms
		out projection: 0.2682879865169525 ms
	self-attn: 9.356287956237793 ms
	cross-attn: 1.2554240226745605 ms
	ffn: 1.4766080379486084 ms
forward previous block to here: 12.491776466369629 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0547200441360474 ms
		kv cache update (if evict): 0.19660800695419312 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.965824127197266 ms
		out projection: 0.26009601354599 ms
	self-attn: 9.271295547485352 ms
	cross-attn: 1.2472319602966309 ms
	ffn: 1.46943998336792 ms
forward previous block to here: 12.35763168334961 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0844160318374634 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.901311874389648 ms
		out projection: 0.2385919988155365 ms
	self-attn: 9.078783988952637 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.880448341369629 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.668479919433594 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1454080045223236 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.866815567016602 ms
	cross-attn: 1.0444799661636353 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.646976470947266 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.841216087341309 ms
	cross-attn: 1.0117119550704956 ms
	ffn: 1.4407680034637451 ms
forward previous block to here: 11.614208221435547 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.644927978515625 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.906432151794434 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.708415985107422 ms
---------------- attention block ----------------
		qkv norm: 0.8683519959449768 ms
		rope: 1.117184042930603 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.939199924468994 ms
		out projection: 0.26214399933815 ms
	self-attn: 9.235456466674805 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 12.0698881149292 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.924863815307617 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.9169921875 ms
	cross-attn: 1.0516480207443237 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.727871894836426 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.861696243286133 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.858624458312988 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4254080057144165 ms
forward previous block to here: 11.653120040893555 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.895487785339355 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4295040369033813 ms
forward previous block to here: 11.662336349487305 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.15360000729560852 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.905728340148926 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.700223922729492 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.91494369506836 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.683839797973633 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.923135757446289 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.692031860351562 ms
---------------- attention block ----------------
		qkv norm: 0.8509439826011658 ms
		rope: 0.9912319779396057 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.627519607543945 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.043455958366394 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.655167579650879 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 1.0045440196990967 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.855551719665527 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.645952224731445 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9871360063552856 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.864768028259277 ms
	cross-attn: 1.052672028541565 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.676671981811523 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.870911598205566 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4243839979171753 ms
forward last block: 11.65004825592041 ms
##### model time: 356.09906005859375 #####
	 Transformer denoising step 2/4 completed in 356.58 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.09932799637317657 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9830399751663208 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.904384136199951 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.671551704406738 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0414079427719116 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.662336349487305 ms
---------------- attention block ----------------
		qkv norm: 0.857088029384613 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.879103660583496 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.920063972473145 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.696127891540527 ms
---------------- attention block ----------------
		qkv norm: 0.851967990398407 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.14336000382900238 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.2519040107727051 ms
	self-attn: 8.890368461608887 ms
	cross-attn: 1.0250240564346313 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.852479934692383 ms
	cross-attn: 1.0240000486373901 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.63161563873291 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13516800105571747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0547200441360474 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.677696228027344 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 1.0926079750061035 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.981504440307617 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.435647964477539 ms
forward previous block to here: 11.792384147644043 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.861696243286133 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.65824031829834 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.854528427124023 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.423359990119934 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.872960090637207 ms
	cross-attn: 1.04038405418396 ms
	ffn: 1.4540799856185913 ms
forward previous block to here: 11.685888290405273 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14643199741840363 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.880832195281982 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.919039726257324 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.685888290405273 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.914624214172363 ms
		out projection: 0.24371199309825897 ms
	self-attn: 8.886272430419922 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.653120040893555 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.841216087341309 ms
	cross-attn: 1.0076160430908203 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.597824096679688 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.873984336853027 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.65004825592041 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.841216087341309 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.591679573059082 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.876736164093018 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.860671997070312 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.615232467651367 ms
---------------- attention block ----------------
		qkv norm: 0.8704000115394592 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.883199691772461 ms
	cross-attn: 1.0280959606170654 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.653120040893555 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.888000011444092 ms
		out projection: 0.2549760043621063 ms
	self-attn: 8.894463539123535 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.655167579650879 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.859647750854492 ms
	cross-attn: 1.031167984008789 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.63263988494873 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9963520169258118 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.878080368041992 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.677696228027344 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.1443839967250824 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.63980770111084 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9789440035820007 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.882880210876465 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.835071563720703 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.4264320135116577 ms
forward previous block to here: 11.595775604248047 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.874688148498535 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0127359628677368 ms
	ffn: 1.4499839544296265 ms
forward previous block to here: 11.646976470947266 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.869888305664062 ms
	cross-attn: 1.0199040174484253 ms
	ffn: 1.4161920547485352 ms
forward previous block to here: 11.627519607543945 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9983999729156494 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.910528182983398 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.91494369506836 ms
	cross-attn: 1.0168319940567017 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.671551704406738 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.845312118530273 ms
	cross-attn: 1.0393600463867188 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.6428804397583 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.890048027038574 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.4172159433364868 ms
forward previous block to here: 11.620351791381836 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24166400730609894 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0455039739608765 ms
	ffn: 1.4161920547485352 ms
forward last block: 11.63980770111084 ms
##### model time: 353.2462158203125 #####
	 Transformer denoising step 3/4 completed in 353.74 ms
wulawula causal model inference func!
is_grad_enabled: False
gradient_checkpointing: False
forward previous block to here: 0.10751999914646149 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.893119812011719 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.676671981811523 ms
---------------- attention block ----------------
		qkv norm: 0.8499199748039246 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.87775993347168 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0577919483184814 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.676671981811523 ms
---------------- attention block ----------------
		qkv norm: 0.8540160059928894 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.877056121826172 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.662336349487305 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.892096042633057 ms
		out projection: 0.24268800020217896 ms
	self-attn: 8.863743782043457 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.64799976348877 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14950400590896606 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.897215843200684 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.871935844421387 ms
	cross-attn: 1.0096640586853027 ms
	ffn: 1.4592000246047974 ms
forward previous block to here: 11.664383888244629 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.1658879965543747 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.27033600211143494 ms
	self-attn: 9.061375617980957 ms
	cross-attn: 1.2451839447021484 ms
	ffn: 1.4684159755706787 ms
forward previous block to here: 12.140543937683105 ms
---------------- attention block ----------------
		qkv norm: 1.2912640571594238 ms
		rope: 1.0506240129470825 ms
		kv cache update (if evict): 0.17817600071430206 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.991424083709717 ms
		out projection: 0.2396160066127777 ms
	self-attn: 9.645055770874023 ms
	cross-attn: 1.0721280574798584 ms
	ffn: 1.4284800291061401 ms
forward previous block to here: 12.471296310424805 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.883903980255127 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.881152153015137 ms
	cross-attn: 1.0536960363388062 ms
	ffn: 1.4346239566802979 ms
forward previous block to here: 11.702272415161133 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.868864059448242 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4243839979171753 ms
forward previous block to here: 11.643903732299805 ms
---------------- attention block ----------------
		qkv norm: 0.8478720188140869 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.2467840015888214 ms
	self-attn: 8.945664405822754 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.44486403465271 ms
forward previous block to here: 11.750399589538574 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.15769599378108978 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.877056121826172 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4202879667282104 ms
forward previous block to here: 11.6561918258667 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8961920738220215 ms
		out projection: 0.24473600089550018 ms
	self-attn: 8.897536277770996 ms
	cross-attn: 1.0465279817581177 ms
	ffn: 1.4366719722747803 ms
forward previous block to here: 11.704319953918457 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9881600141525269 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8869757652282715 ms
		out projection: 0.23552000522613525 ms
	self-attn: 8.854528427124023 ms
	cross-attn: 1.035264015197754 ms
	ffn: 1.4192639589309692 ms
forward previous block to here: 11.644927978515625 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.984063982963562 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.889023780822754 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.849408149719238 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.598848342895508 ms
---------------- attention block ----------------
		qkv norm: 0.8468480110168457 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.872640132904053 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.867839813232422 ms
	cross-attn: 1.0219520330429077 ms
	ffn: 1.4315520524978638 ms
forward previous block to here: 11.662336349487305 ms
---------------- attention block ----------------
		qkv norm: 0.8427519798278809 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.875711917877197 ms
		out projection: 0.23347200453281403 ms
	self-attn: 8.829952239990234 ms
	cross-attn: 1.018880009651184 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.592703819274902 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.2314240038394928 ms
	self-attn: 8.892416000366211 ms
	cross-attn: 1.005568027496338 ms
	ffn: 1.415168046951294 ms
forward previous block to here: 11.62342357635498 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9779199957847595 ms
		kv cache update (if evict): 0.1372160017490387 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.86956787109375 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.0106879472732544 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.606016159057617 ms
---------------- attention block ----------------
		qkv norm: 0.8437759876251221 ms
		rope: 0.9768959879875183 ms
		kv cache update (if evict): 0.13619199395179749 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.873663902282715 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.844287872314453 ms
	cross-attn: 1.026047945022583 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.629568099975586 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8818559646606445 ms
		out projection: 0.23244799673557281 ms
	self-attn: 8.848383903503418 ms
	cross-attn: 1.0291199684143066 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.611136436462402 ms
---------------- attention block ----------------
		qkv norm: 0.8417279720306396 ms
		rope: 0.9758719801902771 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.871615886688232 ms
		out projection: 0.25088000297546387 ms
	self-attn: 8.938495635986328 ms
	cross-attn: 1.0885119438171387 ms
	ffn: 1.4305280447006226 ms
forward previous block to here: 11.792384147644043 ms
---------------- attention block ----------------
		qkv norm: 0.8591359853744507 ms
		rope: 0.9922559857368469 ms
		kv cache update (if evict): 0.14233599603176117 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.902655601501465 ms
	cross-attn: 1.0567679405212402 ms
	ffn: 1.4335999488830566 ms
forward previous block to here: 11.713536262512207 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9799680113792419 ms
		kv cache update (if evict): 0.14028799533843994 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.895167827606201 ms
		out projection: 0.24063999950885773 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0321919918060303 ms
	ffn: 1.4223359823226929 ms
forward previous block to here: 11.654144287109375 ms
---------------- attention block ----------------
		qkv norm: 0.8488960266113281 ms
		rope: 0.9902080297470093 ms
		kv cache update (if evict): 0.14131200313568115 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.893440246582031 ms
	cross-attn: 1.013759970664978 ms
	ffn: 1.4458880424499512 ms
forward previous block to here: 11.670528411865234 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.1382399946451187 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.879807949066162 ms
		out projection: 0.2457599937915802 ms
	self-attn: 8.884223937988281 ms
	cross-attn: 1.0496000051498413 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.682815551757812 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9891840219497681 ms
		kv cache update (if evict): 0.14745600521564484 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.905407905578613 ms
		out projection: 0.23449599742889404 ms
	self-attn: 8.882176399230957 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4213119745254517 ms
forward previous block to here: 11.651071548461914 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9809920191764832 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.865792274475098 ms
	cross-attn: 1.0301439762115479 ms
	ffn: 1.418239951133728 ms
forward previous block to here: 11.638784408569336 ms
---------------- attention block ----------------
		qkv norm: 0.8458240032196045 ms
		rope: 0.9820160269737244 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.8787841796875 ms
		out projection: 0.23756800591945648 ms
	self-attn: 8.853504180908203 ms
	cross-attn: 1.0424319505691528 ms
	ffn: 1.427456021308899 ms
forward previous block to here: 11.661312103271484 ms
---------------- attention block ----------------
		qkv norm: 0.8550400137901306 ms
		rope: 1.0004479885101318 ms
		kv cache update (if evict): 0.14847999811172485 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.891071796417236 ms
		out projection: 0.2385919988155365 ms
	self-attn: 8.937472343444824 ms
	cross-attn: 1.0362880229949951 ms
	ffn: 1.4325759410858154 ms
forward previous block to here: 11.738112449645996 ms
---------------- attention block ----------------
		qkv norm: 0.8447999954223633 ms
		rope: 0.9850879907608032 ms
		kv cache update (if evict): 0.13926400244235992 ms
		attention input size: 
			 rope: torch.Size([1, 4680, 12, 128]) 
			 k: torch.Size([1, 32760, 12, 128]) 
			 v: torch.Size([1, 32760, 12, 128])
		attention: 5.884928226470947 ms
		out projection: 0.23654399812221527 ms
	self-attn: 8.876031875610352 ms
	cross-attn: 1.0270719528198242 ms
	ffn: 1.4223359823226929 ms
forward last block: 11.653120040893555 ms
##### model time: 354.850830078125 #####
	 Transformer final denoising step 4/4 completed in 355.35 ms
‚ö° Block 14 denoising completed in 1567.20 ms
üé® Decoding block 14 to pixels...
denoised_pred shape: torch.Size([1, 6, 16, 60, 104])
pixels shape: torch.Size([1, 24, 3, 480, 832])
##### [taehv vae] ##### torch.Size([1, 12, 3, 480, 832])
üé® Block 14 VAE decoding completed in 54.99 ms
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
üì° Queueing 12 frames from block 14 for sending...
‚úÖ Block 14 completed in 1642.74 ms (12 frames queued in 0.018 ms)
üéâ Generation completed in 25.16 ms! 165 frames queued for sending
üéâ Generation completed in 25162.89 ms! (recorded with cuda event!)
‚è≥ Waiting for all frames to be sent...
‚úÖ All frames sent successfully!
Video saved to ./videos/A stylish woman walk_1402690515_5665df7398.mp4
üì° Frame sender thread stopped
