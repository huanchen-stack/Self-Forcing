Free VRAM 39.08099365234375 GB
âœ… VAE decoder initialized with default VAE
KV inference with 3 frames per block
ðŸš€ Starting demo on http://0.0.0.0:5001
 * Serving Flask app 'demo'
 * Debug mode: off
Client connected
Client disconnected
Client connected
ðŸ“¡ Frame sender thread started
Prompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.
Conditional dict keys: ['prompt_embeds']
	 prompt_embeds shape: torch.Size([1, 512, 4096]), dtype: torch.bfloat16
Moving DynamicSwap_WanTextEncoder to cuda:0 with preserved memory: 41.193517208099365 GB
Seed: 1918263992
rnd: <torch._C.Generator object at 0x78b829198a90> (based on gpu cuda:0)
Frames status: total 21, per block [3, 3, 3, 3, 3, 3, 3]
In loop: idx 0, current_num_frames 3, current_start_frame 0
ðŸ”„ Processing block 1/7
noisy_input shape: torch.Size([1, 3, 16, 60, 104]), sharding: 0 to 3
	 current_start_frame: 0, num_input_frames: 0, current_num_frames: 3
pipeline denoising steps: tensor([1000.0000,  960.0000,  888.8889,  727.2728])
	 timestep shape: torch.Size([1, 3]), value: tensor([[1000., 1000., 1000.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.49710965156555176 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 0
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0010318756103515625 0.0 43130881 7188479 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0028781890869140625 0.0 43130880 7188480 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0052716732025146484 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 1/4 completed in 0.54s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[960., 960., 960.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.1619563102722168 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 0
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0011463165283203125 0.0 43130883 7188477 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.00292205810546875 0.0 43130880 7188480 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002105236053466797 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 2/4 completed in 0.18s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[888.8889, 888.8889, 888.8889]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.14791274070739746 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 0
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0013074874877929688 0.0 43130880 7188480 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0029296875 0.0 43130880 7188480 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002079010009765625 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 3/4 completed in 0.16s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[727.2728, 727.2728, 727.2728]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.15348529815673828 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 0
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0015192031860351562 0.0 43130884 7188476 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0028095245361328125 0.0 43130880 7188480 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00020599365234375 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer final denoising step 4/4 completed in 0.17s
âš¡ Block 1 denoising completed in 1.10s
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.14748477935791016 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 0
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.00202178955078125 0.0 43130883 7188477 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0005846023559570312 0.0 43130882 7188478 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 4680 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00020194053649902344 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
KV cache update for next block completed in 0.16s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
decoding args: args.trt False, current_use_taehv False, vae_cache is None False
ðŸŽ¨ Decoding block 1 to pixels...
Length of feat_cache:  32
ðŸŽ¨ Block 1 VAE decoding completed in 0.26s
pixels shape after possible cropping: torch.Size([1, 9, 3, 480, 832]), block_frames: 9
ðŸ“¡ Queueing 9 frames from block 1 for sending...
	 Queueing frame 1/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 2/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 3/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 4/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 5/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 6/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 7/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 8/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 9/9 of block 1, shape: torch.Size([3, 480, 832]), dtype: torch.float32
âœ… Block 1 completed in 1.90s (9 frames queued in 0.379s)
In loop: idx 1, current_num_frames 3, current_start_frame 3
ðŸ”„ Processing block 2/7
noisy_input shape: torch.Size([1, 3, 16, 60, 104]), sharding: 3 to 6
	 current_start_frame: 3, num_input_frames: 0, current_num_frames: 3
pipeline denoising steps: tensor([1000.0000,  960.0000,  888.8889,  727.2728])
	 timestep shape: torch.Size([1, 3]), value: tensor([[1000., 1000., 1000.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2385406494140625 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 4680
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0027484893798828125 0.0 35942405 14376955 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0020427703857421875 0.0 35942402 14376958 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00027179718017578125 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 1/4 completed in 0.25s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[960., 960., 960.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.17333650588989258 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 4680
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0029659271240234375 0.0 35942405 14376955 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0021457672119140625 0.0 35942402 14376958 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0003414154052734375 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 2/4 completed in 0.19s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[888.8889, 888.8889, 888.8889]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.17151808738708496 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 4680
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.003200531005859375 0.0 35942403 14376957 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.00263214111328125 0.0 35942403 14376957 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00024962425231933594 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 3/4 completed in 0.19s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[727.2728, 727.2728, 727.2728]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.17073774337768555 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 4680
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.003482818603515625 0.0 35942404 14376956 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0024700164794921875 0.0 35942402 14376958 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.000217437744140625 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer final denoising step 4/4 completed in 0.19s
âš¡ Block 2 denoising completed in 0.83s
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.17094135284423828 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 4680
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.004241943359375 0.0 35942404 14376956 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 9.524822235107422e-05 0.0 35942402 14376958 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 9360 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002129077911376953 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
KV cache update for next block completed in 0.19s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
decoding args: args.trt False, current_use_taehv False, vae_cache is None False
ðŸŽ¨ Decoding block 2 to pixels...
Length of feat_cache:  32
ðŸŽ¨ Block 2 VAE decoding completed in 0.19s
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
ðŸ“¡ Queueing 12 frames from block 2 for sending...
	 Queueing frame 1/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 2/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 3/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 4/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 5/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 6/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 7/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 8/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 9/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 10/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 11/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 12/12 of block 2, shape: torch.Size([3, 480, 832]), dtype: torch.float32
âœ… Block 2 completed in 1.59s (12 frames queued in 0.386s)
In loop: idx 2, current_num_frames 3, current_start_frame 6
ðŸ”„ Processing block 3/7
noisy_input shape: torch.Size([1, 3, 16, 60, 104]), sharding: 6 to 9
	 current_start_frame: 6, num_input_frames: 0, current_num_frames: 3
pipeline denoising steps: tensor([1000.0000,  960.0000,  888.8889,  727.2728])
	 timestep shape: torch.Size([1, 3]), value: tensor([[1000., 1000., 1000.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2698688507080078 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 9360
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0037384033203125 0.0 28753928 21565432 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0027713775634765625 0.0 28753922 21565438 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002810955047607422 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 1/4 completed in 0.29s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[960., 960., 960.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.19372153282165527 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 9360
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0039215087890625 0.0 28753926 21565434 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0030536651611328125 0.0 28753922 21565438 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00020885467529296875 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 2/4 completed in 0.21s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[888.8889, 888.8889, 888.8889]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.19362688064575195 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 9360
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.00408935546875 0.0 28753926 21565434 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0031795501708984375 0.0 28753922 21565438 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002238750457763672 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 3/4 completed in 0.21s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[727.2728, 727.2728, 727.2728]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.19825172424316406 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 9360
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.00433349609375 0.0 28753924 21565436 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0031585693359375 0.0 28753923 21565437 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00021767616271972656 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer final denoising step 4/4 completed in 0.21s
âš¡ Block 3 denoising completed in 0.93s
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.19858908653259277 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 9360
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.00457000732421875 0.0 28753924 21565436 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0007548332214355469 0.0 28753922 21565438 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 14040 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00022172927856445312 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
KV cache update for next block completed in 0.21s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
decoding args: args.trt False, current_use_taehv False, vae_cache is None False
ðŸŽ¨ Decoding block 3 to pixels...
Length of feat_cache:  32
ðŸŽ¨ Block 3 VAE decoding completed in 0.19s
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
ðŸ“¡ Queueing 12 frames from block 3 for sending...
	 Queueing frame 1/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 2/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 3/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 4/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 5/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 6/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 7/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 8/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 9/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 10/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 11/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 12/12 of block 3, shape: torch.Size([3, 480, 832]), dtype: torch.float32
âœ… Block 3 completed in 1.75s (12 frames queued in 0.411s)
In loop: idx 3, current_num_frames 3, current_start_frame 9
ðŸ”„ Processing block 4/7
noisy_input shape: torch.Size([1, 3, 16, 60, 104]), sharding: 9 to 12
	 current_start_frame: 9, num_input_frames: 0, current_num_frames: 3
pipeline denoising steps: tensor([1000.0000,  960.0000,  888.8889,  727.2728])
	 timestep shape: torch.Size([1, 3]), value: tensor([[1000., 1000., 1000.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2889101505279541 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 14040
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0094451904296875 0.0 21565444 28753916 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.003566741943359375 0.0 21565442 28753918 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002765655517578125 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 1/4 completed in 0.31s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[960., 960., 960.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.22287321090698242 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 14040
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0095367431640625 0.0 21565446 28753914 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0037631988525390625 0.0 21565442 28753918 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00022411346435546875 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 2/4 completed in 0.24s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[888.8889, 888.8889, 888.8889]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.21619129180908203 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 14040
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0096435546875 0.0 21565445 28753915 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0038280487060546875 0.0 21565442 28753918 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00022459030151367188 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 3/4 completed in 0.23s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[727.2728, 727.2728, 727.2728]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.22095131874084473 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 14040
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0097503662109375 0.0 21565447 28753913 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0038166046142578125 0.0 21565442 28753918 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00022220611572265625 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer final denoising step 4/4 completed in 0.24s
âš¡ Block 4 denoising completed in 1.03s
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.22191739082336426 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 14040
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.00974273681640625 0.0 21565447 28753913 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0012044906616210938 0.0 21565443 28753917 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 18720 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00023102760314941406 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
KV cache update for next block completed in 0.24s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
decoding args: args.trt False, current_use_taehv False, vae_cache is None False
ðŸŽ¨ Decoding block 4 to pixels...
Length of feat_cache:  32
ðŸŽ¨ Block 4 VAE decoding completed in 0.19s
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
ðŸ“¡ Queueing 12 frames from block 4 for sending...
	 Queueing frame 1/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 2/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 3/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 4/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 5/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 6/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 7/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 8/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 9/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 10/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 11/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 12/12 of block 4, shape: torch.Size([3, 480, 832]), dtype: torch.float32
âœ… Block 4 completed in 1.85s (12 frames queued in 0.389s)
In loop: idx 4, current_num_frames 3, current_start_frame 12
ðŸ”„ Processing block 5/7
noisy_input shape: torch.Size([1, 3, 16, 60, 104]), sharding: 12 to 15
	 current_start_frame: 12, num_input_frames: 0, current_num_frames: 3
pipeline denoising steps: tensor([1000.0000,  960.0000,  888.8889,  727.2728])
	 timestep shape: torch.Size([1, 3]), value: tensor([[1000., 1000., 1000.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.30823326110839844 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 18720
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.01374053955078125 0.0 14376969 35942391 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.00411224365234375 0.0 14376963 35942397 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002601146697998047 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 1/4 completed in 0.33s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[960., 960., 960.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.23952627182006836 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 18720
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.013763427734375 0.0 14376968 35942392 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.004131317138671875 0.0 14376963 35942397 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00020885467529296875 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 2/4 completed in 0.26s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[888.8889, 888.8889, 888.8889]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.239518404006958 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 18720
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.01378631591796875 0.0 14376969 35942391 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0041656494140625 0.0 14376963 35942397 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002193450927734375 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 3/4 completed in 0.26s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[727.2728, 727.2728, 727.2728]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.24098587036132812 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 18720
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0137481689453125 0.0 14376967 35942393 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.004150390625 0.0 14376963 35942397 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002124309539794922 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer final denoising step 4/4 completed in 0.26s
âš¡ Block 5 denoising completed in 1.11s
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.24161434173583984 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 18720
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.01338958740234375 0.0 14376967 35942393 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.0018310546875 0.0 14376964 35942396 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 23400 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002193450927734375 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
KV cache update for next block completed in 0.26s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
decoding args: args.trt False, current_use_taehv False, vae_cache is None False
ðŸŽ¨ Decoding block 5 to pixels...
Length of feat_cache:  32
ðŸŽ¨ Block 5 VAE decoding completed in 0.19s
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
ðŸ“¡ Queueing 12 frames from block 5 for sending...
	 Queueing frame 1/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 2/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 3/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 4/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 5/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 6/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 7/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 8/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 9/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 10/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 11/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 12/12 of block 5, shape: torch.Size([3, 480, 832]), dtype: torch.float32
âœ… Block 5 completed in 1.93s (12 frames queued in 0.374s)
In loop: idx 5, current_num_frames 3, current_start_frame 15
ðŸ”„ Processing block 6/7
noisy_input shape: torch.Size([1, 3, 16, 60, 104]), sharding: 15 to 18
	 current_start_frame: 15, num_input_frames: 0, current_num_frames: 3
pipeline denoising steps: tensor([1000.0000,  960.0000,  888.8889,  727.2728])
	 timestep shape: torch.Size([1, 3]), value: tensor([[1000., 1000., 1000.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.3463294506072998 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 23400
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0139312744140625 0.0 7188492 43130868 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.00466156005859375 0.0 7188484 43130876 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002732276916503906 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 1/4 completed in 0.36s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[960., 960., 960.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.26239538192749023 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 23400
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.014007568359375 0.0 7188488 43130872 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.004772186279296875 0.0 7188484 43130876 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00021386146545410156 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 2/4 completed in 0.28s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[888.8889, 888.8889, 888.8889]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.26518893241882324 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 23400
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0140838623046875 0.0 7188488 43130872 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.004581451416015625 0.0 7188484 43130876 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002281665802001953 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 3/4 completed in 0.28s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[727.2728, 727.2728, 727.2728]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2644314765930176 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 23400
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.014129638671875 0.0 7188488 43130872 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.004833221435546875 0.0 7188484 43130876 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00020647048950195312 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer final denoising step 4/4 completed in 0.28s
âš¡ Block 6 denoising completed in 1.23s
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2621903419494629 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 23400
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.01457977294921875 0.0 7188488 43130872 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.00214385986328125 0.0 7188484 43130876 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 28080 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002377033233642578 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
KV cache update for next block completed in 0.28s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
decoding args: args.trt False, current_use_taehv False, vae_cache is None False
ðŸŽ¨ Decoding block 6 to pixels...
Length of feat_cache:  32
ðŸŽ¨ Block 6 VAE decoding completed in 0.19s
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
ðŸ“¡ Queueing 12 frames from block 6 for sending...
	 Queueing frame 1/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 2/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 3/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 4/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 5/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 6/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 7/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 8/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 9/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 10/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 11/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 12/12 of block 6, shape: torch.Size([3, 480, 832]), dtype: torch.float32
âœ… Block 6 completed in 2.08s (12 frames queued in 0.384s)
In loop: idx 6, current_num_frames 3, current_start_frame 18
ðŸ”„ Processing block 7/7
noisy_input shape: torch.Size([1, 3, 16, 60, 104]), sharding: 18 to 21
	 current_start_frame: 18, num_input_frames: 0, current_num_frames: 3
pipeline denoising steps: tensor([1000.0000,  960.0000,  888.8889,  727.2728])
	 timestep shape: torch.Size([1, 3]), value: tensor([[1000., 1000., 1000.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.34159231185913086 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 28080
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0157318115234375 -0.001445770263671875 9 50319351 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.004940032958984375 0.0038394927978515625 4 50319356 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00023865699768066406 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 1/4 completed in 0.37s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[960., 960., 960.]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2906453609466553 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 28080
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.0158538818359375 -0.0015039443969726562 9 50319351 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.005084991455078125 0.003971099853515625 4 50319356 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00023627281188964844 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 2/4 completed in 0.31s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[888.8889, 888.8889, 888.8889]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2904181480407715 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 28080
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.015960693359375 -0.0015687942504882812 9 50319351 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.00516510009765625 0.00414276123046875 4 50319356 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.00020766258239746094 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer denoising step 3/4 completed in 0.31s
	   âž• Add noise for next step completed in 0.00s
	 timestep shape: torch.Size([1, 3]), value: tensor([[727.2728, 727.2728, 727.2728]], device='cuda:0')
### wan_wrapper.py ###
#### forward ####
##### using kv cache #####
##### model time: 0.2874910831451416 #####
========= shapes =========
noisy_image_or_video: torch.Size([1, 3, 16, 60, 104])
noisy_image_or_video permute: torch.Size([1, 16, 3, 60, 104])
t input_timestep: torch.Size([1, 3])
context prompt_embeds: torch.Size([1, 512, 4096])
seq_len: 32760
kv_cache: 30
kv_cache_dtype: <class 'dict'>
cache_start: None
current_start: 28080
@@@ k @@@
@@@@ torch.Size([1, 32760, 12, 128]) -0.01611328125 -0.0016717910766601562 9 50319351 @@@@
@@@ v @@@
@@@@ torch.Size([1, 32760, 12, 128]) 0.00518035888671875 0.004352569580078125 4 50319356 @@@@
@@@ global_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
@@@ local_end_index @@@
@@@@ torch.Size([1]) 32760 @@@@
<class 'list'>
30
<class 'dict'>
$$$ k $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.0302276611328125 0.05511474609375 0 786432 $$$$
$$$ v $$$
$$$$ torch.Size([1, 512, 12, 128]) 0.00014972686767578125 -4.762411117553711e-05 422 786010 $$$$
$$$ is_init $$$
$$$$ <class 'bool'> $$$$
-------------------------
flow_pred: torch.Size([1, 3, 16, 60, 104])
=========================
##### convert time: 0.0002186298370361328 #####
pred_x0: torch.Size([1, 3, 16, 60, 104])
#########################
	 Transformer final denoising step 4/4 completed in 0.31s
âš¡ Block 7 denoising completed in 1.30s
KV cache update for next block completed in 0.00s
denoised_pred shape: torch.Size([1, 3, 16, 60, 104]), dtype: torch.float16
decoding args: args.trt False, current_use_taehv False, vae_cache is None False
ðŸŽ¨ Decoding block 7 to pixels...
Length of feat_cache:  32
ðŸŽ¨ Block 7 VAE decoding completed in 0.20s
pixels shape after possible cropping: torch.Size([1, 12, 3, 480, 832]), block_frames: 12
ðŸ“¡ Queueing 12 frames from block 7 for sending...
	 Queueing frame 1/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 2/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 3/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 4/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 5/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 6/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 7/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 8/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 9/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 10/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 11/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
	 Queueing frame 12/12 of block 7, shape: torch.Size([3, 480, 832]), dtype: torch.float32
âœ… Block 7 completed in 1.87s (12 frames queued in 0.371s)
ðŸŽ‰ Generation completed in 12.97s! 81 frames queued for sending
â³ Waiting for all frames to be sent...
âœ… All frames sent successfully!
Video saved to ./videos/A stylish woman walk_1918263992_5665df7398.mp4
ðŸ“¡ Frame sender thread stopped
